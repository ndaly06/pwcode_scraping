{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports the required packages\n",
    "from bs4 import BeautifulSoup\n",
    "from html.parser import HTMLParser\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import pickle\n",
    "import re\n",
    "import requests\n",
    "# from selenium import webdriver\n",
    "import time\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "return_headings method\n",
    "\n",
    "\"\"\"\n",
    "def return_headings(url, heading_tag):\n",
    "    \n",
    "    html_doc = requests.get(url).content\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    \n",
    "    # searches for all h4 headings\n",
    "    search_headings = soup.findAll(heading_tag)\n",
    "    \n",
    "    # empty area headings list\n",
    "    headings = []\n",
    "    \n",
    "    # appends each area heading to the area_headings list\n",
    "    for div in search_headings:\n",
    "        headings.append(div.text)\n",
    "\n",
    "    # removes the white space from the headings list\n",
    "    headings = list(map(str.strip, headings))\n",
    "\n",
    "    # list comphrehension for lower casing each string in the area_headings list\n",
    "    headings = [x.lower() for x in headings]\n",
    "\n",
    "    # list comprehension that replaces the white space with an dash in the area_headings list\n",
    "    headings = [x.replace(\" \", \"-\") for x in headings]\n",
    "    \n",
    "    # returns the headings array\n",
    "    return headings\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "return_dataset method\n",
    "\n",
    "\"\"\"\n",
    "def return_dataset(url, dataset_tag):\n",
    "    \n",
    "    html_doc = requests.get(url).content\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    \n",
    "    #\n",
    "    search_datasets = soup.findAll('div', attrs = {'class': dataset_tag})\n",
    "    \n",
    "    # empty area headings list\n",
    "    task_datasets = []\n",
    "    \n",
    "    # appends each task dataset to the task_datasets list\n",
    "    for div in search_datasets:\n",
    "        task_datasets.append(div.text)\n",
    "\n",
    "    # removes the white space from the task_datasets list\n",
    "    task_datasets = list(map(str.strip, task_datasets))\n",
    "\n",
    "    # list comphrension for lower casing each string in the task_datasets list\n",
    "    task_datasets = [x.lower() for x in task_datasets]\n",
    "\n",
    "    # list comphrension for replacing the white space with an dash in the task_datasets list\n",
    "    task_datasets = [x.replace(\" \", \"-\") for x in task_datasets]\n",
    "    \n",
    "    # removes the brace in the task_datasets list\n",
    "    task_datasets = [x.replace(\"(\", \"\") for x in task_datasets]\n",
    "    task_datasets = [x.replace(\")\", \"\") for x in task_datasets]\n",
    "    \n",
    "    # returns the task_datasets list\n",
    "    return task_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['computer-vision', 'natural-language-processing', 'medical', 'methodology', 'miscellaneous', 'speech', 'playing-games', 'graphs', 'time-series', 'audio', 'robots', 'music', 'computer-code', 'reasoning', 'knowledge-base', 'adversarial']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "url = 'https://paperswithcode.com/sota'\n",
    "heading_tag = 'h4'\n",
    "\n",
    "# invokes the return_headings function to return each of the area headings\n",
    "area_headings = return_headings(url, heading_tag)\n",
    "\n",
    "print(area_headings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantic-segmentation\n"
     ]
    }
   ],
   "source": [
    "# initialises an empty task headings list\n",
    "task_headings = []\n",
    "    \n",
    "# iterates through each of the area headings\n",
    "for i in range(len(area_headings)):\n",
    "    \n",
    "    url = 'https://paperswithcode.com/area/' + area_headings[i]\n",
    "    heading_tag = 'h4'\n",
    "    \n",
    "    # invokes the return_headings function to return \n",
    "    # and append each of the task headings to the task_headings list\n",
    "    task_headings.append(return_headings(url, heading_tag))\n",
    "    \n",
    "# converts the resulting 2d array into a 1d array using list comprehension\n",
    "# task_headings = [s for S in task_headings for s in S]\n",
    "\n",
    "# prints the first 10 elements in the task_headings list\n",
    "print(task_headings[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracts the sub-tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialises an empty subtask headings list\n",
    "subtask_headings = []\n",
    "    \n",
    "# iterates through each of the area headings\n",
    "for i in range(len(area_headings)):\n",
    "    \n",
    "    # iterates through each of the corresponding subtask headings\n",
    "    for j in range(len(task_headings[i])):\n",
    "        \n",
    "        url = 'https://paperswithcode.com/area/' + area_headings[i] + '/' + task_headings[i][j] \n",
    "        heading_tag = 'h1'\n",
    "        \n",
    "        # invokes the return_headings function to return \n",
    "        # and append each of the subtask headings to the subtask_headings list\n",
    "        subtask_headings.append(return_headings(url, heading_tag))\n",
    "        \n",
    "# converts the resulting 2d list into a 1d list using list comprehension\n",
    "subtask_headings = [s for S in subtask_headings for s in S]\n",
    "\n",
    "# list comprehension for removing duplicate subtask headings\n",
    "subtask_headings = [ x for x in subtask_headings if \"-subtasks\" not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['semantic-segmentation',\n",
       " 'real-time-semantic-segmentation',\n",
       " 'scene-segmentation',\n",
       " '3d-part-segmentation',\n",
       " 'weakly-supervised-semantic-segmentation']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# displays the first 5 elements in the subtask_headings list\n",
    "subtask_headings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1260"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subtask_headings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pwcode_scraping",
   "language": "python",
   "name": "pwcode_scraping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
