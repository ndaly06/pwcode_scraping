{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "import heapq\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "from tika import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>extradata</th>\n",
       "      <th>global_rank</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>metric_value</th>\n",
       "      <th>model</th>\n",
       "      <th>remove</th>\n",
       "      <th>task</th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_path</th>\n",
       "      <th>paper_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IC15</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 10</td>\n",
       "      <td>F-Measure</td>\n",
       "      <td>75.61%</td>\n",
       "      <td>SegLink</td>\n",
       "      <td>-</td>\n",
       "      <td>Scene Text Detection</td>\n",
       "      <td>Detecting Oriented Text in Natural Images by L...</td>\n",
       "      <td>/paper/detecting-oriented-text-in-natural-imag...</td>\n",
       "      <td>https://arxiv.org/pdf/1703.06520v3.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SCUT-CTW1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 5</td>\n",
       "      <td>F-Measure</td>\n",
       "      <td>40.8%</td>\n",
       "      <td>SegLink</td>\n",
       "      <td>-</td>\n",
       "      <td>Curved Text Detection</td>\n",
       "      <td>Detecting Oriented Text in Natural Images by L...</td>\n",
       "      <td>/paper/detecting-oriented-text-in-natural-imag...</td>\n",
       "      <td>https://arxiv.org/pdf/1703.06520v3.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 36</td>\n",
       "      <td>% Test Accuracy</td>\n",
       "      <td>84.6</td>\n",
       "      <td>300D NSE encoders</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 44</td>\n",
       "      <td>% Train Accuracy</td>\n",
       "      <td>86.2</td>\n",
       "      <td>300D NSE encoders</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 1</td>\n",
       "      <td>Parameters</td>\n",
       "      <td>3.0m</td>\n",
       "      <td>300D NSE encoders</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dataset  extradata global_rank       metric_name metric_value  \\\n",
       "0          IC15        NaN        # 10         F-Measure       75.61%   \n",
       "1  SCUT-CTW1500        NaN         # 5         F-Measure        40.8%   \n",
       "2          SNLI        NaN        # 36   % Test Accuracy         84.6   \n",
       "3          SNLI        NaN        # 44  % Train Accuracy         86.2   \n",
       "4          SNLI        NaN         # 1        Parameters         3.0m   \n",
       "\n",
       "               model remove                        task  \\\n",
       "0            SegLink      -        Scene Text Detection   \n",
       "1            SegLink      -       Curved Text Detection   \n",
       "2  300D NSE encoders      -  Natural Language Inference   \n",
       "3  300D NSE encoders      -  Natural Language Inference   \n",
       "4  300D NSE encoders      -  Natural Language Inference   \n",
       "\n",
       "                                         paper_title  \\\n",
       "0  Detecting Oriented Text in Natural Images by L...   \n",
       "1  Detecting Oriented Text in Natural Images by L...   \n",
       "2                           Neural Semantic Encoders   \n",
       "3                           Neural Semantic Encoders   \n",
       "4                           Neural Semantic Encoders   \n",
       "\n",
       "                                          paper_path  \\\n",
       "0  /paper/detecting-oriented-text-in-natural-imag...   \n",
       "1  /paper/detecting-oriented-text-in-natural-imag...   \n",
       "2                    /paper/neural-semantic-encoders   \n",
       "3                    /paper/neural-semantic-encoders   \n",
       "4                    /paper/neural-semantic-encoders   \n",
       "\n",
       "                                paper_url  \n",
       "0  https://arxiv.org/pdf/1703.06520v3.pdf  \n",
       "1  https://arxiv.org/pdf/1703.06520v3.pdf  \n",
       "2  https://arxiv.org/pdf/1607.04315v3.pdf  \n",
       "3  https://arxiv.org/pdf/1607.04315v3.pdf  \n",
       "4  https://arxiv.org/pdf/1607.04315v3.pdf  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/pw_code_model_data.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>extradata</th>\n",
       "      <th>global_rank</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>metric_value</th>\n",
       "      <th>model</th>\n",
       "      <th>remove</th>\n",
       "      <th>task</th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_path</th>\n",
       "      <th>paper_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IC15</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 10</td>\n",
       "      <td>F-Measure</td>\n",
       "      <td>75.61%</td>\n",
       "      <td>SegLink</td>\n",
       "      <td>-</td>\n",
       "      <td>Scene Text Detection</td>\n",
       "      <td>Detecting Oriented Text in Natural Images by L...</td>\n",
       "      <td>/paper/detecting-oriented-text-in-natural-imag...</td>\n",
       "      <td>https://arxiv.org/pdf/1703.06520v3.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SCUT-CTW1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 5</td>\n",
       "      <td>F-Measure</td>\n",
       "      <td>40.8%</td>\n",
       "      <td>SegLink</td>\n",
       "      <td>-</td>\n",
       "      <td>Curved Text Detection</td>\n",
       "      <td>Detecting Oriented Text in Natural Images by L...</td>\n",
       "      <td>/paper/detecting-oriented-text-in-natural-imag...</td>\n",
       "      <td>https://arxiv.org/pdf/1703.06520v3.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 36</td>\n",
       "      <td>% Test Accuracy</td>\n",
       "      <td>84.6</td>\n",
       "      <td>300D NSE encoders</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 44</td>\n",
       "      <td>% Train Accuracy</td>\n",
       "      <td>86.2</td>\n",
       "      <td>300D NSE encoders</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 1</td>\n",
       "      <td>Parameters</td>\n",
       "      <td>3.0m</td>\n",
       "      <td>300D NSE encoders</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dataset  extradata global_rank       metric_name metric_value  \\\n",
       "0          IC15        NaN        # 10         F-Measure       75.61%   \n",
       "1  SCUT-CTW1500        NaN         # 5         F-Measure        40.8%   \n",
       "2          SNLI        NaN        # 36   % Test Accuracy         84.6   \n",
       "3          SNLI        NaN        # 44  % Train Accuracy         86.2   \n",
       "4          SNLI        NaN         # 1        Parameters         3.0m   \n",
       "\n",
       "               model remove                        task  \\\n",
       "0            SegLink      -        Scene Text Detection   \n",
       "1            SegLink      -       Curved Text Detection   \n",
       "2  300D NSE encoders      -  Natural Language Inference   \n",
       "3  300D NSE encoders      -  Natural Language Inference   \n",
       "4  300D NSE encoders      -  Natural Language Inference   \n",
       "\n",
       "                                         paper_title  \\\n",
       "0  Detecting Oriented Text in Natural Images by L...   \n",
       "1  Detecting Oriented Text in Natural Images by L...   \n",
       "2                           Neural Semantic Encoders   \n",
       "3                           Neural Semantic Encoders   \n",
       "4                           Neural Semantic Encoders   \n",
       "\n",
       "                                          paper_path  \\\n",
       "0  /paper/detecting-oriented-text-in-natural-imag...   \n",
       "1  /paper/detecting-oriented-text-in-natural-imag...   \n",
       "2                    /paper/neural-semantic-encoders   \n",
       "3                    /paper/neural-semantic-encoders   \n",
       "4                    /paper/neural-semantic-encoders   \n",
       "\n",
       "                                paper_url  \n",
       "0  https://arxiv.org/pdf/1703.06520v3.pdf  \n",
       "1  https://arxiv.org/pdf/1703.06520v3.pdf  \n",
       "2  https://arxiv.org/pdf/1607.04315v3.pdf  \n",
       "3  https://arxiv.org/pdf/1607.04315v3.pdf  \n",
       "4  https://arxiv.org/pdf/1607.04315v3.pdf  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[:30]\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>extradata</th>\n",
       "      <th>global_rank</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>metric_value</th>\n",
       "      <th>model</th>\n",
       "      <th>remove</th>\n",
       "      <th>task</th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_path</th>\n",
       "      <th>paper_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IC15</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 10</td>\n",
       "      <td>F-Measure</td>\n",
       "      <td>75.61%</td>\n",
       "      <td>SegLink</td>\n",
       "      <td>-</td>\n",
       "      <td>Scene Text Detection</td>\n",
       "      <td>Detecting Oriented Text in Natural Images by L...</td>\n",
       "      <td>/paper/detecting-oriented-text-in-natural-imag...</td>\n",
       "      <td>https://arxiv.org/pdf/1703.06520v3.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SCUT-CTW1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 5</td>\n",
       "      <td>F-Measure</td>\n",
       "      <td>40.8%</td>\n",
       "      <td>SegLink</td>\n",
       "      <td>-</td>\n",
       "      <td>Curved Text Detection</td>\n",
       "      <td>Detecting Oriented Text in Natural Images by L...</td>\n",
       "      <td>/paper/detecting-oriented-text-in-natural-imag...</td>\n",
       "      <td>https://arxiv.org/pdf/1703.06520v3.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 36</td>\n",
       "      <td>% Test Accuracy</td>\n",
       "      <td>84.6</td>\n",
       "      <td>300D NSE encoders</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 44</td>\n",
       "      <td>% Train Accuracy</td>\n",
       "      <td>86.2</td>\n",
       "      <td>300D NSE encoders</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 1</td>\n",
       "      <td>Parameters</td>\n",
       "      <td>3.0m</td>\n",
       "      <td>300D NSE encoders</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 33</td>\n",
       "      <td>% Test Accuracy</td>\n",
       "      <td>85.4</td>\n",
       "      <td>300D MMA-NSE encoders with attention</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 41</td>\n",
       "      <td>% Train Accuracy</td>\n",
       "      <td>86.9</td>\n",
       "      <td>300D MMA-NSE encoders with attention</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 1</td>\n",
       "      <td>Parameters</td>\n",
       "      <td>3.2m</td>\n",
       "      <td>300D MMA-NSE encoders with attention</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SST-2 Binary classification</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 10</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>89.7</td>\n",
       "      <td>Neural Semantic Encoder</td>\n",
       "      <td>-</td>\n",
       "      <td>Sentiment Analysis</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WikiQA</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 8</td>\n",
       "      <td>MAP</td>\n",
       "      <td>0.6811</td>\n",
       "      <td>MMA-NSE attention</td>\n",
       "      <td>-</td>\n",
       "      <td>Question Answering</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>WikiQA</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 7</td>\n",
       "      <td>MRR</td>\n",
       "      <td>0.6993</td>\n",
       "      <td>MMA-NSE attention</td>\n",
       "      <td>-</td>\n",
       "      <td>Question Answering</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>WMT2014 English-German</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 21</td>\n",
       "      <td>BLEU score</td>\n",
       "      <td>17.93</td>\n",
       "      <td>NSE-NSE</td>\n",
       "      <td>-</td>\n",
       "      <td>Machine Translation</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>VQA v2</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 2</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>70.24%</td>\n",
       "      <td>Pythia v0.1</td>\n",
       "      <td>-</td>\n",
       "      <td>Visual Question Answering</td>\n",
       "      <td>Pythia v0.1: the Winning Entry to the VQA Chal...</td>\n",
       "      <td>/paper/pythia-v01-the-winning-entry-to-the-vqa</td>\n",
       "      <td>https://arxiv.org/pdf/1807.09956v2.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ImageNet 128x128</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 3</td>\n",
       "      <td>FID</td>\n",
       "      <td>9.6</td>\n",
       "      <td>BigGAN</td>\n",
       "      <td>-</td>\n",
       "      <td>Conditional Image Generation</td>\n",
       "      <td>Large Scale GAN Training for High Fidelity Nat...</td>\n",
       "      <td>/paper/large-scale-gan-training-for-high-fidelity</td>\n",
       "      <td>https://arxiv.org/pdf/1809.11096v2.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ImageNet 128x128</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 2</td>\n",
       "      <td>Inception score</td>\n",
       "      <td>166.3</td>\n",
       "      <td>BigGAN</td>\n",
       "      <td>-</td>\n",
       "      <td>Conditional Image Generation</td>\n",
       "      <td>Large Scale GAN Training for High Fidelity Nat...</td>\n",
       "      <td>/paper/large-scale-gan-training-for-high-fidelity</td>\n",
       "      <td>https://arxiv.org/pdf/1809.11096v2.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ImageNet 128x128</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 1</td>\n",
       "      <td>FID</td>\n",
       "      <td>7.4</td>\n",
       "      <td>BigGAN-deep</td>\n",
       "      <td>-</td>\n",
       "      <td>Conditional Image Generation</td>\n",
       "      <td>Large Scale GAN Training for High Fidelity Nat...</td>\n",
       "      <td>/paper/large-scale-gan-training-for-high-fidelity</td>\n",
       "      <td>https://arxiv.org/pdf/1809.11096v2.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ImageNet 128x128</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 1</td>\n",
       "      <td>Inception score</td>\n",
       "      <td>166.5</td>\n",
       "      <td>BigGAN-deep</td>\n",
       "      <td>-</td>\n",
       "      <td>Conditional Image Generation</td>\n",
       "      <td>Large Scale GAN Training for High Fidelity Nat...</td>\n",
       "      <td>/paper/large-scale-gan-training-for-high-fidelity</td>\n",
       "      <td>https://arxiv.org/pdf/1809.11096v2.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BUCC French-to-English</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 2</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>92.89</td>\n",
       "      <td>Multilingual Sentence Embeddings</td>\n",
       "      <td>-</td>\n",
       "      <td>Cross-Lingual Bitext Mining</td>\n",
       "      <td>Margin-based Parallel Corpus Mining with Multi...</td>\n",
       "      <td>/paper/margin-based-parallel-corpus-mining-with</td>\n",
       "      <td>https://arxiv.org/pdf/1811.01136v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BUCC German-to-English</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 2</td>\n",
       "      <td>F1 score</td>\n",
       "      <td>95.58</td>\n",
       "      <td>Multilingual Sentence Embeddings</td>\n",
       "      <td>-</td>\n",
       "      <td>Cross-Lingual Bitext Mining</td>\n",
       "      <td>Margin-based Parallel Corpus Mining with Multi...</td>\n",
       "      <td>/paper/margin-based-parallel-corpus-mining-with</td>\n",
       "      <td>https://arxiv.org/pdf/1811.01136v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Mini-ImageNet - 1-Shot Learning</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 5</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>50.13%</td>\n",
       "      <td>PLATIPUS</td>\n",
       "      <td>-</td>\n",
       "      <td>Few-Shot Image Classification</td>\n",
       "      <td>Probabilistic Model-Agnostic Meta-Learning</td>\n",
       "      <td>/paper/probabilistic-model-agnostic-meta-learning</td>\n",
       "      <td>https://arxiv.org/pdf/1806.02817v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Multi-Domain Sentiment Dataset</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 2</td>\n",
       "      <td>DVD</td>\n",
       "      <td>78.14</td>\n",
       "      <td>Multi-task tri-training</td>\n",
       "      <td>-</td>\n",
       "      <td>Sentiment Analysis</td>\n",
       "      <td>Strong Baselines for Neural Semi-supervised Le...</td>\n",
       "      <td>/paper/strong-baselines-for-neural-semi-superv...</td>\n",
       "      <td>https://arxiv.org/pdf/1804.09530v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Multi-Domain Sentiment Dataset</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 2</td>\n",
       "      <td>Books</td>\n",
       "      <td>74.86</td>\n",
       "      <td>Multi-task tri-training</td>\n",
       "      <td>-</td>\n",
       "      <td>Sentiment Analysis</td>\n",
       "      <td>Strong Baselines for Neural Semi-supervised Le...</td>\n",
       "      <td>/paper/strong-baselines-for-neural-semi-superv...</td>\n",
       "      <td>https://arxiv.org/pdf/1804.09530v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Multi-Domain Sentiment Dataset</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 1</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>81.45</td>\n",
       "      <td>Multi-task tri-training</td>\n",
       "      <td>-</td>\n",
       "      <td>Sentiment Analysis</td>\n",
       "      <td>Strong Baselines for Neural Semi-supervised Le...</td>\n",
       "      <td>/paper/strong-baselines-for-neural-semi-superv...</td>\n",
       "      <td>https://arxiv.org/pdf/1804.09530v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Multi-Domain Sentiment Dataset</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 4</td>\n",
       "      <td>Kitchen</td>\n",
       "      <td>82.14</td>\n",
       "      <td>Multi-task tri-training</td>\n",
       "      <td>-</td>\n",
       "      <td>Sentiment Analysis</td>\n",
       "      <td>Strong Baselines for Neural Semi-supervised Le...</td>\n",
       "      <td>/paper/strong-baselines-for-neural-semi-superv...</td>\n",
       "      <td>https://arxiv.org/pdf/1804.09530v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Multi-Domain Sentiment Dataset</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 2</td>\n",
       "      <td>Average</td>\n",
       "      <td>79.15</td>\n",
       "      <td>Multi-task tri-training</td>\n",
       "      <td>-</td>\n",
       "      <td>Sentiment Analysis</td>\n",
       "      <td>Strong Baselines for Neural Semi-supervised Le...</td>\n",
       "      <td>/paper/strong-baselines-for-neural-semi-superv...</td>\n",
       "      <td>https://arxiv.org/pdf/1804.09530v1.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>BSD100 - 4x upscaling</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 1</td>\n",
       "      <td>PSNR</td>\n",
       "      <td>27.85</td>\n",
       "      <td>SRGAN + Residual-in-Residual Dense Block</td>\n",
       "      <td>-</td>\n",
       "      <td>Image Super-Resolution</td>\n",
       "      <td>ESRGAN: Enhanced Super-Resolution Generative A...</td>\n",
       "      <td>/paper/esrgan-enhanced-super-resolution-genera...</td>\n",
       "      <td>https://arxiv.org/pdf/1809.00219v2.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>BSD100 - 4x upscaling</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 3</td>\n",
       "      <td>SSIM</td>\n",
       "      <td>0.7455</td>\n",
       "      <td>SRGAN + Residual-in-Residual Dense Block</td>\n",
       "      <td>-</td>\n",
       "      <td>Image Super-Resolution</td>\n",
       "      <td>ESRGAN: Enhanced Super-Resolution Generative A...</td>\n",
       "      <td>/paper/esrgan-enhanced-super-resolution-genera...</td>\n",
       "      <td>https://arxiv.org/pdf/1809.00219v2.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Manga109 - 4x upscaling</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 8</td>\n",
       "      <td>PSNR</td>\n",
       "      <td>24.89</td>\n",
       "      <td>bicubic</td>\n",
       "      <td>-</td>\n",
       "      <td>Image Super-Resolution</td>\n",
       "      <td>ESRGAN: Enhanced Super-Resolution Generative A...</td>\n",
       "      <td>/paper/esrgan-enhanced-super-resolution-genera...</td>\n",
       "      <td>https://arxiv.org/pdf/1809.00219v2.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Manga109 - 4x upscaling</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 9</td>\n",
       "      <td>SSIM</td>\n",
       "      <td>0.7866</td>\n",
       "      <td>bicubic</td>\n",
       "      <td>-</td>\n",
       "      <td>Image Super-Resolution</td>\n",
       "      <td>ESRGAN: Enhanced Super-Resolution Generative A...</td>\n",
       "      <td>/paper/esrgan-enhanced-super-resolution-genera...</td>\n",
       "      <td>https://arxiv.org/pdf/1809.00219v2.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Manga109 - 4x upscaling</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 1</td>\n",
       "      <td>PSNR</td>\n",
       "      <td>31.66</td>\n",
       "      <td>SRGAN + Residual-in-Residual Dense Block</td>\n",
       "      <td>-</td>\n",
       "      <td>Image Super-Resolution</td>\n",
       "      <td>ESRGAN: Enhanced Super-Resolution Generative A...</td>\n",
       "      <td>/paper/esrgan-enhanced-super-resolution-genera...</td>\n",
       "      <td>https://arxiv.org/pdf/1809.00219v2.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            dataset  extradata global_rank       metric_name  \\\n",
       "0                              IC15        NaN        # 10         F-Measure   \n",
       "1                      SCUT-CTW1500        NaN         # 5         F-Measure   \n",
       "2                              SNLI        NaN        # 36   % Test Accuracy   \n",
       "3                              SNLI        NaN        # 44  % Train Accuracy   \n",
       "4                              SNLI        NaN         # 1        Parameters   \n",
       "5                              SNLI        NaN        # 33   % Test Accuracy   \n",
       "6                              SNLI        NaN        # 41  % Train Accuracy   \n",
       "7                              SNLI        NaN         # 1        Parameters   \n",
       "8       SST-2 Binary classification        NaN        # 10          Accuracy   \n",
       "9                            WikiQA        NaN         # 8               MAP   \n",
       "10                           WikiQA        NaN         # 7               MRR   \n",
       "11           WMT2014 English-German        NaN        # 21        BLEU score   \n",
       "12                           VQA v2        NaN         # 2          Accuracy   \n",
       "13                 ImageNet 128x128        NaN         # 3               FID   \n",
       "14                 ImageNet 128x128        NaN         # 2   Inception score   \n",
       "15                 ImageNet 128x128        NaN         # 1               FID   \n",
       "16                 ImageNet 128x128        NaN         # 1   Inception score   \n",
       "17           BUCC French-to-English        NaN         # 2          F1 score   \n",
       "18           BUCC German-to-English        NaN         # 2          F1 score   \n",
       "19  Mini-ImageNet - 1-Shot Learning        NaN         # 5          Accuracy   \n",
       "20   Multi-Domain Sentiment Dataset        NaN         # 2               DVD   \n",
       "21   Multi-Domain Sentiment Dataset        NaN         # 2             Books   \n",
       "22   Multi-Domain Sentiment Dataset        NaN         # 1       Electronics   \n",
       "23   Multi-Domain Sentiment Dataset        NaN         # 4           Kitchen   \n",
       "24   Multi-Domain Sentiment Dataset        NaN         # 2           Average   \n",
       "25            BSD100 - 4x upscaling        NaN         # 1              PSNR   \n",
       "26            BSD100 - 4x upscaling        NaN         # 3              SSIM   \n",
       "27          Manga109 - 4x upscaling        NaN         # 8              PSNR   \n",
       "28          Manga109 - 4x upscaling        NaN         # 9              SSIM   \n",
       "29          Manga109 - 4x upscaling        NaN         # 1              PSNR   \n",
       "\n",
       "   metric_value                                     model remove  \\\n",
       "0        75.61%                                   SegLink      -   \n",
       "1         40.8%                                   SegLink      -   \n",
       "2          84.6                         300D NSE encoders      -   \n",
       "3          86.2                         300D NSE encoders      -   \n",
       "4          3.0m                         300D NSE encoders      -   \n",
       "5          85.4      300D MMA-NSE encoders with attention      -   \n",
       "6          86.9      300D MMA-NSE encoders with attention      -   \n",
       "7          3.2m      300D MMA-NSE encoders with attention      -   \n",
       "8          89.7                   Neural Semantic Encoder      -   \n",
       "9        0.6811                         MMA-NSE attention      -   \n",
       "10       0.6993                         MMA-NSE attention      -   \n",
       "11        17.93                                   NSE-NSE      -   \n",
       "12       70.24%                               Pythia v0.1      -   \n",
       "13          9.6                                    BigGAN      -   \n",
       "14        166.3                                    BigGAN      -   \n",
       "15          7.4                               BigGAN-deep      -   \n",
       "16        166.5                               BigGAN-deep      -   \n",
       "17        92.89          Multilingual Sentence Embeddings      -   \n",
       "18        95.58          Multilingual Sentence Embeddings      -   \n",
       "19       50.13%                                  PLATIPUS      -   \n",
       "20        78.14                   Multi-task tri-training      -   \n",
       "21        74.86                   Multi-task tri-training      -   \n",
       "22        81.45                   Multi-task tri-training      -   \n",
       "23        82.14                   Multi-task tri-training      -   \n",
       "24        79.15                   Multi-task tri-training      -   \n",
       "25        27.85  SRGAN + Residual-in-Residual Dense Block      -   \n",
       "26       0.7455  SRGAN + Residual-in-Residual Dense Block      -   \n",
       "27        24.89                                   bicubic      -   \n",
       "28       0.7866                                   bicubic      -   \n",
       "29        31.66  SRGAN + Residual-in-Residual Dense Block      -   \n",
       "\n",
       "                             task  \\\n",
       "0            Scene Text Detection   \n",
       "1           Curved Text Detection   \n",
       "2      Natural Language Inference   \n",
       "3      Natural Language Inference   \n",
       "4      Natural Language Inference   \n",
       "5      Natural Language Inference   \n",
       "6      Natural Language Inference   \n",
       "7      Natural Language Inference   \n",
       "8              Sentiment Analysis   \n",
       "9              Question Answering   \n",
       "10             Question Answering   \n",
       "11            Machine Translation   \n",
       "12      Visual Question Answering   \n",
       "13   Conditional Image Generation   \n",
       "14   Conditional Image Generation   \n",
       "15   Conditional Image Generation   \n",
       "16   Conditional Image Generation   \n",
       "17    Cross-Lingual Bitext Mining   \n",
       "18    Cross-Lingual Bitext Mining   \n",
       "19  Few-Shot Image Classification   \n",
       "20             Sentiment Analysis   \n",
       "21             Sentiment Analysis   \n",
       "22             Sentiment Analysis   \n",
       "23             Sentiment Analysis   \n",
       "24             Sentiment Analysis   \n",
       "25         Image Super-Resolution   \n",
       "26         Image Super-Resolution   \n",
       "27         Image Super-Resolution   \n",
       "28         Image Super-Resolution   \n",
       "29         Image Super-Resolution   \n",
       "\n",
       "                                          paper_title  \\\n",
       "0   Detecting Oriented Text in Natural Images by L...   \n",
       "1   Detecting Oriented Text in Natural Images by L...   \n",
       "2                            Neural Semantic Encoders   \n",
       "3                            Neural Semantic Encoders   \n",
       "4                            Neural Semantic Encoders   \n",
       "5                            Neural Semantic Encoders   \n",
       "6                            Neural Semantic Encoders   \n",
       "7                            Neural Semantic Encoders   \n",
       "8                            Neural Semantic Encoders   \n",
       "9                            Neural Semantic Encoders   \n",
       "10                           Neural Semantic Encoders   \n",
       "11                           Neural Semantic Encoders   \n",
       "12  Pythia v0.1: the Winning Entry to the VQA Chal...   \n",
       "13  Large Scale GAN Training for High Fidelity Nat...   \n",
       "14  Large Scale GAN Training for High Fidelity Nat...   \n",
       "15  Large Scale GAN Training for High Fidelity Nat...   \n",
       "16  Large Scale GAN Training for High Fidelity Nat...   \n",
       "17  Margin-based Parallel Corpus Mining with Multi...   \n",
       "18  Margin-based Parallel Corpus Mining with Multi...   \n",
       "19         Probabilistic Model-Agnostic Meta-Learning   \n",
       "20  Strong Baselines for Neural Semi-supervised Le...   \n",
       "21  Strong Baselines for Neural Semi-supervised Le...   \n",
       "22  Strong Baselines for Neural Semi-supervised Le...   \n",
       "23  Strong Baselines for Neural Semi-supervised Le...   \n",
       "24  Strong Baselines for Neural Semi-supervised Le...   \n",
       "25  ESRGAN: Enhanced Super-Resolution Generative A...   \n",
       "26  ESRGAN: Enhanced Super-Resolution Generative A...   \n",
       "27  ESRGAN: Enhanced Super-Resolution Generative A...   \n",
       "28  ESRGAN: Enhanced Super-Resolution Generative A...   \n",
       "29  ESRGAN: Enhanced Super-Resolution Generative A...   \n",
       "\n",
       "                                           paper_path  \\\n",
       "0   /paper/detecting-oriented-text-in-natural-imag...   \n",
       "1   /paper/detecting-oriented-text-in-natural-imag...   \n",
       "2                     /paper/neural-semantic-encoders   \n",
       "3                     /paper/neural-semantic-encoders   \n",
       "4                     /paper/neural-semantic-encoders   \n",
       "5                     /paper/neural-semantic-encoders   \n",
       "6                     /paper/neural-semantic-encoders   \n",
       "7                     /paper/neural-semantic-encoders   \n",
       "8                     /paper/neural-semantic-encoders   \n",
       "9                     /paper/neural-semantic-encoders   \n",
       "10                    /paper/neural-semantic-encoders   \n",
       "11                    /paper/neural-semantic-encoders   \n",
       "12     /paper/pythia-v01-the-winning-entry-to-the-vqa   \n",
       "13  /paper/large-scale-gan-training-for-high-fidelity   \n",
       "14  /paper/large-scale-gan-training-for-high-fidelity   \n",
       "15  /paper/large-scale-gan-training-for-high-fidelity   \n",
       "16  /paper/large-scale-gan-training-for-high-fidelity   \n",
       "17    /paper/margin-based-parallel-corpus-mining-with   \n",
       "18    /paper/margin-based-parallel-corpus-mining-with   \n",
       "19  /paper/probabilistic-model-agnostic-meta-learning   \n",
       "20  /paper/strong-baselines-for-neural-semi-superv...   \n",
       "21  /paper/strong-baselines-for-neural-semi-superv...   \n",
       "22  /paper/strong-baselines-for-neural-semi-superv...   \n",
       "23  /paper/strong-baselines-for-neural-semi-superv...   \n",
       "24  /paper/strong-baselines-for-neural-semi-superv...   \n",
       "25  /paper/esrgan-enhanced-super-resolution-genera...   \n",
       "26  /paper/esrgan-enhanced-super-resolution-genera...   \n",
       "27  /paper/esrgan-enhanced-super-resolution-genera...   \n",
       "28  /paper/esrgan-enhanced-super-resolution-genera...   \n",
       "29  /paper/esrgan-enhanced-super-resolution-genera...   \n",
       "\n",
       "                                 paper_url  \n",
       "0   https://arxiv.org/pdf/1703.06520v3.pdf  \n",
       "1   https://arxiv.org/pdf/1703.06520v3.pdf  \n",
       "2   https://arxiv.org/pdf/1607.04315v3.pdf  \n",
       "3   https://arxiv.org/pdf/1607.04315v3.pdf  \n",
       "4   https://arxiv.org/pdf/1607.04315v3.pdf  \n",
       "5   https://arxiv.org/pdf/1607.04315v3.pdf  \n",
       "6   https://arxiv.org/pdf/1607.04315v3.pdf  \n",
       "7   https://arxiv.org/pdf/1607.04315v3.pdf  \n",
       "8   https://arxiv.org/pdf/1607.04315v3.pdf  \n",
       "9   https://arxiv.org/pdf/1607.04315v3.pdf  \n",
       "10  https://arxiv.org/pdf/1607.04315v3.pdf  \n",
       "11  https://arxiv.org/pdf/1607.04315v3.pdf  \n",
       "12  https://arxiv.org/pdf/1807.09956v2.pdf  \n",
       "13  https://arxiv.org/pdf/1809.11096v2.pdf  \n",
       "14  https://arxiv.org/pdf/1809.11096v2.pdf  \n",
       "15  https://arxiv.org/pdf/1809.11096v2.pdf  \n",
       "16  https://arxiv.org/pdf/1809.11096v2.pdf  \n",
       "17  https://arxiv.org/pdf/1811.01136v1.pdf  \n",
       "18  https://arxiv.org/pdf/1811.01136v1.pdf  \n",
       "19  https://arxiv.org/pdf/1806.02817v1.pdf  \n",
       "20  https://arxiv.org/pdf/1804.09530v1.pdf  \n",
       "21  https://arxiv.org/pdf/1804.09530v1.pdf  \n",
       "22  https://arxiv.org/pdf/1804.09530v1.pdf  \n",
       "23  https://arxiv.org/pdf/1804.09530v1.pdf  \n",
       "24  https://arxiv.org/pdf/1804.09530v1.pdf  \n",
       "25  https://arxiv.org/pdf/1809.00219v2.pdf  \n",
       "26  https://arxiv.org/pdf/1809.00219v2.pdf  \n",
       "27  https://arxiv.org/pdf/1809.00219v2.pdf  \n",
       "28  https://arxiv.org/pdf/1809.00219v2.pdf  \n",
       "29  https://arxiv.org/pdf/1809.00219v2.pdf  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>extradata</th>\n",
       "      <th>global_rank</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>metric_value</th>\n",
       "      <th>model</th>\n",
       "      <th>remove</th>\n",
       "      <th>task</th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_path</th>\n",
       "      <th>paper_url</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IC15</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 10</td>\n",
       "      <td>F-Measure</td>\n",
       "      <td>75.61%</td>\n",
       "      <td>SegLink</td>\n",
       "      <td>-</td>\n",
       "      <td>Scene Text Detection</td>\n",
       "      <td>Detecting Oriented Text in Natural Images by L...</td>\n",
       "      <td>/paper/detecting-oriented-text-in-natural-imag...</td>\n",
       "      <td>https://arxiv.org/pdf/1703.06520v3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SCUT-CTW1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 5</td>\n",
       "      <td>F-Measure</td>\n",
       "      <td>40.8%</td>\n",
       "      <td>SegLink</td>\n",
       "      <td>-</td>\n",
       "      <td>Curved Text Detection</td>\n",
       "      <td>Detecting Oriented Text in Natural Images by L...</td>\n",
       "      <td>/paper/detecting-oriented-text-in-natural-imag...</td>\n",
       "      <td>https://arxiv.org/pdf/1703.06520v3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 36</td>\n",
       "      <td>% Test Accuracy</td>\n",
       "      <td>84.6</td>\n",
       "      <td>300D NSE encoders</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 44</td>\n",
       "      <td>% Train Accuracy</td>\n",
       "      <td>86.2</td>\n",
       "      <td>300D NSE encoders</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 1</td>\n",
       "      <td>Parameters</td>\n",
       "      <td>3.0m</td>\n",
       "      <td>300D NSE encoders</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dataset  extradata global_rank       metric_name metric_value  \\\n",
       "0          IC15        NaN        # 10         F-Measure       75.61%   \n",
       "1  SCUT-CTW1500        NaN         # 5         F-Measure        40.8%   \n",
       "2          SNLI        NaN        # 36   % Test Accuracy         84.6   \n",
       "3          SNLI        NaN        # 44  % Train Accuracy         86.2   \n",
       "4          SNLI        NaN         # 1        Parameters         3.0m   \n",
       "\n",
       "               model remove                        task  \\\n",
       "0            SegLink      -        Scene Text Detection   \n",
       "1            SegLink      -       Curved Text Detection   \n",
       "2  300D NSE encoders      -  Natural Language Inference   \n",
       "3  300D NSE encoders      -  Natural Language Inference   \n",
       "4  300D NSE encoders      -  Natural Language Inference   \n",
       "\n",
       "                                         paper_title  \\\n",
       "0  Detecting Oriented Text in Natural Images by L...   \n",
       "1  Detecting Oriented Text in Natural Images by L...   \n",
       "2                           Neural Semantic Encoders   \n",
       "3                           Neural Semantic Encoders   \n",
       "4                           Neural Semantic Encoders   \n",
       "\n",
       "                                          paper_path  \\\n",
       "0  /paper/detecting-oriented-text-in-natural-imag...   \n",
       "1  /paper/detecting-oriented-text-in-natural-imag...   \n",
       "2                    /paper/neural-semantic-encoders   \n",
       "3                    /paper/neural-semantic-encoders   \n",
       "4                    /paper/neural-semantic-encoders   \n",
       "\n",
       "                                paper_url  paper_text  \n",
       "0  https://arxiv.org/pdf/1703.06520v3.pdf         NaN  \n",
       "1  https://arxiv.org/pdf/1703.06520v3.pdf         NaN  \n",
       "2  https://arxiv.org/pdf/1607.04315v3.pdf         NaN  \n",
       "3  https://arxiv.org/pdf/1607.04315v3.pdf         NaN  \n",
       "4  https://arxiv.org/pdf/1607.04315v3.pdf         NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adds a new column to dataframe\n",
    "data['paper_text'] = np.nan\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:24:36,812 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1703.06520v3.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1703-06520v3-pdf.\n",
      "2019-06-04 23:24:55,372 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.19/tika-server-1.19.jar to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/tika-server.jar.\n",
      "2019-06-04 23:25:10,953 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.19/tika-server-1.19.jar.md5 to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/tika-server.jar.md5.\n",
      "2019-06-04 23:25:11,439 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n",
      "2019-06-04 23:25:29,537 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1703.06520v3.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1703-06520v3-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1703.06520v3.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:25:48,666 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1607.04315v3.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1607-04315v3-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1703.06520v3.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:25:50,797 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1607.04315v3.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1607-04315v3-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1607.04315v3.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:25:52,820 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1607.04315v3.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1607-04315v3-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1607.04315v3.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:25:54,771 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1607.04315v3.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1607-04315v3-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1607.04315v3.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:25:56,786 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1607.04315v3.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1607-04315v3-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1607.04315v3.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:25:58,852 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1607.04315v3.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1607-04315v3-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1607.04315v3.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:26:00,751 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1607.04315v3.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1607-04315v3-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1607.04315v3.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:26:02,919 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1607.04315v3.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1607-04315v3-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1607.04315v3.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:26:04,937 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1607.04315v3.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1607-04315v3-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1607.04315v3.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:26:06,907 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1607.04315v3.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1607-04315v3-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1607.04315v3.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:26:08,862 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1807.09956v2.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1807-09956v2-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1607.04315v3.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:26:10,603 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1809.11096v2.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1809-11096v2-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1807.09956v2.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:26:32,705 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1809.11096v2.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1809-11096v2-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1809.11096v2.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:26:56,007 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1809.11096v2.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1809-11096v2-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1809.11096v2.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:27:17,846 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1809.11096v2.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1809-11096v2-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1809.11096v2.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:27:41,195 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1811.01136v1.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1811-01136v1-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1809.11096v2.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:27:43,393 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1811.01136v1.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1811-01136v1-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1811.01136v1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:27:45,392 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1806.02817v1.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1806-02817v1-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1811.01136v1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:27:51,579 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1804.09530v1.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1804-09530v1-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1806.02817v1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:27:53,876 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1804.09530v1.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1804-09530v1-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1804.09530v1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:27:56,249 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1804.09530v1.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1804-09530v1-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1804.09530v1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:27:58,330 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1804.09530v1.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1804-09530v1-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1804.09530v1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:28:00,566 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1804.09530v1.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1804-09530v1-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1804.09530v1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:28:02,788 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1809.00219v2.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1809-00219v2-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1804.09530v1.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:28:14,646 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1809.00219v2.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1809-00219v2-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1809.00219v2.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:28:28,832 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1809.00219v2.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1809-00219v2-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1809.00219v2.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:28:44,448 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1809.00219v2.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1809-00219v2-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1809.00219v2.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-04 23:28:57,056 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1809.00219v2.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1809-00219v2-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1809.00219v2.pdf\n",
      "https://arxiv.org/pdf/1809.00219v2.pdf\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "paper_text_list = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    \n",
    "    content = parser.from_file(row['paper_url'])\n",
    "    raw_text = content['content']\n",
    "    \n",
    "    text = str(raw_text)\n",
    "    text = re.sub(r'\\[[0-9]*\\]', ' ', text)  \n",
    "    text = re.sub(r'\\s+', ' ', text)  \n",
    "    \n",
    "#     safe_text = raw.replace('\\\\', '\\\\\\\\').replace('\"', '\\\\\"')\n",
    "    \n",
    "    paper_text_list.append(text)\n",
    "    \n",
    "    \n",
    "    print(row['paper_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "data['paper_text'] = paper_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>extradata</th>\n",
       "      <th>global_rank</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>metric_value</th>\n",
       "      <th>model</th>\n",
       "      <th>remove</th>\n",
       "      <th>task</th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_path</th>\n",
       "      <th>paper_url</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IC15</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 10</td>\n",
       "      <td>F-Measure</td>\n",
       "      <td>75.61%</td>\n",
       "      <td>SegLink</td>\n",
       "      <td>-</td>\n",
       "      <td>Scene Text Detection</td>\n",
       "      <td>Detecting Oriented Text in Natural Images by L...</td>\n",
       "      <td>/paper/detecting-oriented-text-in-natural-imag...</td>\n",
       "      <td>https://arxiv.org/pdf/1703.06520v3.pdf</td>\n",
       "      <td>Detecting Oriented Text in Natural Images by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SCUT-CTW1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 5</td>\n",
       "      <td>F-Measure</td>\n",
       "      <td>40.8%</td>\n",
       "      <td>SegLink</td>\n",
       "      <td>-</td>\n",
       "      <td>Curved Text Detection</td>\n",
       "      <td>Detecting Oriented Text in Natural Images by L...</td>\n",
       "      <td>/paper/detecting-oriented-text-in-natural-imag...</td>\n",
       "      <td>https://arxiv.org/pdf/1703.06520v3.pdf</td>\n",
       "      <td>Detecting Oriented Text in Natural Images by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 36</td>\n",
       "      <td>% Test Accuracy</td>\n",
       "      <td>84.6</td>\n",
       "      <td>300D NSE encoders</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "      <td>Neural Semantic Encoders Tsendsuren Munkhdala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 44</td>\n",
       "      <td>% Train Accuracy</td>\n",
       "      <td>86.2</td>\n",
       "      <td>300D NSE encoders</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "      <td>Neural Semantic Encoders Tsendsuren Munkhdala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 1</td>\n",
       "      <td>Parameters</td>\n",
       "      <td>3.0m</td>\n",
       "      <td>300D NSE encoders</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "      <td>Neural Semantic Encoders Tsendsuren Munkhdala...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dataset  extradata global_rank       metric_name metric_value  \\\n",
       "0          IC15        NaN        # 10         F-Measure       75.61%   \n",
       "1  SCUT-CTW1500        NaN         # 5         F-Measure        40.8%   \n",
       "2          SNLI        NaN        # 36   % Test Accuracy         84.6   \n",
       "3          SNLI        NaN        # 44  % Train Accuracy         86.2   \n",
       "4          SNLI        NaN         # 1        Parameters         3.0m   \n",
       "\n",
       "               model remove                        task  \\\n",
       "0            SegLink      -        Scene Text Detection   \n",
       "1            SegLink      -       Curved Text Detection   \n",
       "2  300D NSE encoders      -  Natural Language Inference   \n",
       "3  300D NSE encoders      -  Natural Language Inference   \n",
       "4  300D NSE encoders      -  Natural Language Inference   \n",
       "\n",
       "                                         paper_title  \\\n",
       "0  Detecting Oriented Text in Natural Images by L...   \n",
       "1  Detecting Oriented Text in Natural Images by L...   \n",
       "2                           Neural Semantic Encoders   \n",
       "3                           Neural Semantic Encoders   \n",
       "4                           Neural Semantic Encoders   \n",
       "\n",
       "                                          paper_path  \\\n",
       "0  /paper/detecting-oriented-text-in-natural-imag...   \n",
       "1  /paper/detecting-oriented-text-in-natural-imag...   \n",
       "2                    /paper/neural-semantic-encoders   \n",
       "3                    /paper/neural-semantic-encoders   \n",
       "4                    /paper/neural-semantic-encoders   \n",
       "\n",
       "                                paper_url  \\\n",
       "0  https://arxiv.org/pdf/1703.06520v3.pdf   \n",
       "1  https://arxiv.org/pdf/1703.06520v3.pdf   \n",
       "2  https://arxiv.org/pdf/1607.04315v3.pdf   \n",
       "3  https://arxiv.org/pdf/1607.04315v3.pdf   \n",
       "4  https://arxiv.org/pdf/1607.04315v3.pdf   \n",
       "\n",
       "                                          paper_text  \n",
       "0   Detecting Oriented Text in Natural Images by ...  \n",
       "1   Detecting Oriented Text in Natural Images by ...  \n",
       "2   Neural Semantic Encoders Tsendsuren Munkhdala...  \n",
       "3   Neural Semantic Encoders Tsendsuren Munkhdala...  \n",
       "4   Neural Semantic Encoders Tsendsuren Munkhdala...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_json('../data/temp_model_data.json', orient='records', lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pwcode_scraping",
   "language": "python",
   "name": "pwcode_scraping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
