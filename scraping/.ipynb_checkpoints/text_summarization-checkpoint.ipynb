{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "import heapq\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "from tika import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-02 17:43:54,348 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1903.10318v1.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1903-10318v1-pdf.\n",
      "2019-06-02 17:44:02,075 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.19/tika-server-1.19.jar.md5 to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/tika-server.jar.md5.\n",
      "2019-06-02 17:44:02,633 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.19/tika-server-1.19.jar to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/tika-server.jar.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d450c89f5504>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# bert paper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://arxiv.org/pdf/1903.10318v1.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mraw_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/pwcode_scraping/venv/lib/python3.6/site-packages/tika/parser.py\u001b[0m in \u001b[0;36mfrom_file\u001b[0;34m(filename, serverEndpoint, xmlContent, headers, config_path)\u001b[0m\n\u001b[1;32m     34\u001b[0m     '''\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mxmlContent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mjsonOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserverEndpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         jsonOutput = parse1('all', filename, serverEndpoint, services={'meta': '/meta', 'text': '/tika', 'all': '/rmeta/xml'},\n",
      "\u001b[0;32m~/Documents/pwcode_scraping/venv/lib/python3.6/site-packages/tika/tika.py\u001b[0m in \u001b[0;36mparse1\u001b[0;34m(option, urlOrPath, serverEndpoint, verbose, tikaServerJar, responseMimeType, services, rawResponse, headers, config_path)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mservice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'/tika'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresponseMimeType\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'text/plain'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     status, response = callServer('put', serverEndpoint, service, open(path, 'rb'),\n\u001b[0;32m--> 328\u001b[0;31m                                   headers, verbose, tikaServerJar, config_path=config_path, rawResponse=rawResponse)\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'remote'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/pwcode_scraping/venv/lib/python3.6/site-packages/tika/tika.py\u001b[0m in \u001b[0;36mcallServer\u001b[0;34m(verb, serverEndpoint, service, data, headers, verbose, tikaServerJar, httpVerbs, classpath, rawResponse, config_path)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mTikaClientOnly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTikaClientOnly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m         \u001b[0mserverEndpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckTikaServer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserverHost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtikaServerJar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasspath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0mserviceUrl\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mserverEndpoint\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mservice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/pwcode_scraping/venv/lib/python3.6/site-packages/tika/tika.py\u001b[0m in \u001b[0;36mcheckTikaServer\u001b[0;34m(scheme, serverHost, port, tikaServerJar, classpath, config_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheckJarSig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtikaServerJar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjarPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjarPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                 \u001b[0mtikaServerJar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetRemoteJar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtikaServerJar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjarPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstartServer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjarPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTikaJava\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserverHost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasspath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/pwcode_scraping/venv/lib/python3.6/site-packages/tika/tika.py\u001b[0m in \u001b[0;36mgetRemoteJar\u001b[0;34m(urlOrPath, destPath)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Retrieving %s to %s.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0murlOrPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlOrPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# monkey patch fix for SSL/Windows per Tika-Python #54\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# bert paper\n",
    "content = parser.from_file('https://arxiv.org/pdf/1903.10318v1.pdf')\n",
    "\n",
    "raw_text = content['content']\n",
    "\n",
    "text = str(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# text = \"The way that information propagates in neural networks is of great importance. In this paper, we propose Path Aggregation Network (PANet) aiming at boosting information flow in proposal-based instance segmentation framework. Specifically, we enhance the entire feature hierarchy with accurate localization signals in lower layers by bottom-up path augmentation, which shortens the information path between lower layers and topmost feature. We present adaptive feature pooling, which links feature grid and all feature levels to make useful information in each feature level propagate directly to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction. These improvements are simple to implement, with subtle extra computational overhead. Our PANet reaches the 1st place in the COCO 2017 Challenge Instance Segmentation task and the 2nd place in Object Detection task without large-batch training. It is also state-of-the-art on MVD and Cityscapes. Code is available at https://github.com/ShuLiu1993/PANet\"\n",
    "\n",
    "# text = \"Detecting Oriented Text in Natural Images by Linking Segments  Baoguang Shi1 Xiang Bai1\\u2217 Serge Belongie2 1School of EIC, Huazhong University of Science and Technology  2Department of Computer Science, Cornell Tech shibaoguang@gmail.com xbai@hust.edu.cn sjb344@cornell.edu  Abstract  Most state-of-the-art text detection methods are specific to horizontal Latin text and are not fast enough for real-time applications. We introduce Segment Linking (SegLink), an oriented text detection method. The main idea is to decom- pose text into two locally detectable elements, namely seg- ments and links. A segment is an oriented box covering a part of a word or text line; A link connects two adjacent segments, indicating that they belong to the same word or text line. Both elements are detected densely at multiple scales by an end-to-end trained, fully-convolutional neural network. Final detections are produced by combining seg- ments connected by links. Compared with previous meth- ods, SegLink improves along the dimensions of accuracy, speed, and ease of training. It achieves an f-measure of 75.0% on the standard ICDAR 2015 Incidental (Challenge 4) benchmark, outperforming the previous best by a large margin. It runs at over 20 FPS on 512\\u00d7512 images. More- over, without modification, SegLink is able to detect long lines of non-Latin text, such as Chinese.  1. Introduction  Reading text in natural images is a challenging task un- der active research. It is driven by many real-world appli- cations, such as Photo OCR [2], geo-location, and image retrieval [9]. In a text reading system, text detection, i.e. localizing text with bounding boxes of words or text lines, is usually the first step of great significance. In a sense, text detection can be seen as object detection applied to text, where words\\/characters\\/text lines are taken as the detection targets. Owing to this, a new trend has emerged recently that state-of-the-art text detection methods [9, 6, 22, 30] are heavily based on the advanced general object detection or segmentation techniques, e.g. [4, 5, 15].  Despite the great success of the previous work, we ar- gue that the general detection methods are not well suited  \\u2217Corresponding author.  (a) (b) (c)  (d) (e) (f)  Figure 1. SegLink Overview. The upper row shows an image with two words of different scales and orientations. (a) Segments (yellow boxes) are detected on the image. (b) Links (green lines) are detected between pairs of adjacent segments. (c) Segments connected by links are combined into whole words. (d-f) SegLink is able to detect long lines of Latin and non-Latin text, such as Chinese.  for text detection, for two main reasons. First, word\\/text line bounding boxes have much larger aspect ratios than those of general objects. An (fast\\/faster) R-CNN [5, 4, 19]- or SSD [14]-style detector may suffer from the difficulty of producing such boxes, owing to its proposal or anchor box design. In addition, some non-Latin text does not have blank spaces between words, hence the even larger bound- ing box aspect ratios, which make the problem worse. Sec- ond, unlike general objects, text usually has a clear defini- tion of orientation [25]. It is important for a text detector to produce oriented boxes. However, most general object de- tection methods are not designed to produce oriented boxes.  To overcome the above challenges, we tackle the text de- tection problem in a new perspective. We propose to de- compose long text into two smaller and locally-detectable elements, namely segment and link. As illustrated in Fig. 1, a segment is an oriented box that covers a part of a word  ar X  iv :1  70 3.  06 52  0v 3    [ cs  .C V  ]   1  3  A  pr  2  01 7    64  512  VGG16  through  pool5  conv 4_3  Input Image (512x512)  32  1024  conv7 (fc7)  16  1024  conv 8_2  8  512  conv 9_2 conv  10_24 conv11  Combining  Segments  Detections  256  2  256  \\ud835\\udc59 = 1 \\ud835\\udc59 = 6\\ud835\\udc59 = 2 \\ud835\\udc59 = 3 \\ud835\\udc59 = 4 \\ud835\\udc59 = 5  3x3 conv  predictors  1024,k3s1 1024,k1s1  256,k1s1 512,k3s2  128,k1s1 256,k3s2  128,k1s1 256,k3s2  256,k3s2  Figure 2. Network Architecture. The network consists of convolutional feature layers (shown as gray blocks) and convolutional predictors (thin gray arrows). Convolutional filters are specified in the format of \\u201c(#filters),k(kernel size)s(stride)\\u201d. A multi-line filter specification means a hidden layer between. Segments (yellow boxes) and links (not displayed) are detected by convolutional predictors on multiple feature layers (indexed by l = 1 . . . 6) and combined into whole words by a combining algorithm.  (for clarity we use \\u201cword\\u201d here and later on, but segments also work seamlessly on text lines that comprise multiple words); A link connects a pair of adjacent segments, indi- cating that they belong to the same word. Under the above definitions, a word is located by a number of segments with links between them. During detection, segments and links are densely detected on an input image by a convo- lutional neural network. Then, the segments are combined into whole words according to the links.  The key advantage of this approach is that long and ori- ented text is now detected locally since both basic elements are locally-detectable: Detecting a segment does not require the whole word to be observed. And neither does a link since the connection of two segments can be inferred from a local context. Thereafter, we can detect text of any length and orientation with great flexibility and efficiency.  Concretely, we propose a convolutional neural network (CNN) model to detect both segments and links simultane- ously, in a fully-convolutional manner. The network uses VGG-16 [21] as its backbone. A few extra feature lay- ers are added onto it. Convolutional predictors are added to 6 of the feature layers to detect segments and links at different scales. To deal with redundant detections, we in- troduce two types of links, namely within-layer links and cross-layer links. A within-layer link connects a segment to its neighbors on the same layer. A cross-layer link, on the other hand, connects a segment to its neighbors on the lower layer. In this way, we connect segments of adjacent locations as well as scales. Finally, we find connected seg- ments with a depth-first search (DFS) algorithm and com- bine them into whole words.  Our main contribution is the novel segment-linking de-  tection method. Through experiments, we show that the proposed method possesses several distinctive advantages over the other state-of-the-art methods: 1) Robustness: SegLink models the structure of oriented text in a simple and elegant way, with robustness against complex back- grounds. Our method achieves highly competitive results on standard datasets. In particular, it outperforms the previ- ous best by a large margin in terms of f-measure (75.0% vs 64.8%) on the ICDAR 2015 Incidental (Challenge 4) bench- mark [12]; 2) Efficiency: SegLink is highly efficient due to its single-pass, fully-convolutional design. It processes more than 20 images of 512x512 size per second; 3) Gener- ality: Without modification, SegLink is able to detect long lines of non-Latin text, such as Chinese. We demonstrate this capability on a multi-lingual dataset.  2. Related Work  Text Detection Over the past few years, much research effort has been devoted to the text detection problem [24, 23, 17, 17, 25, 7, 8, 30, 29, 2, 9, 6, 22, 26]. Based on the ba- sic detection targets, the previous methods can be roughly divided into three categories: character-based, word-based and line-based. Character-based methods [17, 23, 24, 10, 7, 8] detect individual characters and group them into words. These methods find characters by classifying candidate re- gions extracted by region extraction algorithms or by classi- fying sliding windows. Such methods often involve a post- processing step of grouping characters into words. Word- based methods [9, 6] directly detect word bounding boxes. They often have a similar pipeline to the recent CNN-based general object detection networks. Though achieving ex- cellent detection accuracies, these methods may suffer from    performance drop when applied to some non-Latin text such as Chinese, as we mentioned earlier. Line-based meth- ods [29, 30, 26] find text regions using some image seg- mentation algorithms. They also require a sophisticated post-processing step of word partitioning and\\/or false pos- itive removal. Compared with the previous approaches, our method predicts segments and links jointly in a single forward network pass. The pipeline is much simpler and cleaner. Moreover, the network is end-to-end trainable.  Our method is similar in spirit to a recent work [22], which detects text lines by finding and grouping a sequence of fine-scale text proposals through a CNN coupled with recurrent neural layers. In contrast, we detect oriented seg- ments only using convolutional layers, yielding better flexi- bility and faster speed. Also, we detect links explicitly using the same strong CNN features for segments, improving the robustness.  Object Detection Text detection can be seen as a partic- ular instance of general object detection, which is a funda- mental problem in computer vision. Most state-of-the-art detection systems either classify some class-agnostic ob- ject proposals with CNN [5, 4, 19] or directly regress ob- ject bounding boxes from a set of preset boxes (e.g. anchor boxes) [18, 14].  The architecture of our network inherits that of SSD [14], a recent object detection model. SSD proposed the idea of detecting objects on multiple feature layers with convolu- tional predictors. Our model also detects segments and links in a very similar way. Despite the model similarity, our de- tection strategy is drastically different: SSD directly out- puts object bounding boxes. We, on the other hand, adopt a bottom-up approach by detecting the two comprising ele- ments of a word or text line and combine them together.  3. Segment Linking Our method detects text with a feed-forward CNN  model. Given an input image I of size wI \\u00d7 hI , the model outputs a fixed number of segments and links, which are then filtered by their confidence scores and combined into whole word bounding boxes. A bounding box is a ro- tated rectangle denoted by b = (xb, yb, wb, hb, \\u03b8b), where xb, yb are the coordinates of the center, wb, hb the width and height, and \\u03b8b the rotation angle.  3.1. CNN Model  Fig. 2 shows the network architecture. Our network uses a pretrained VGG-16 network [21] as its backbone (conv1 through pool5). Following [14], the fully-connected layers of VGG-16 are converted into convolutional layers (fc6 to conv6; fc7 to conv7). They are followed by a few extra con- volutional layers (conv8 1 to conv11), which extract even  deeper features with larger receptive fields. Their configu- rations are specified in Fig. 2.  Segments and links are detected on 6 of the feature lay- ers, which are conv4 3, conv7, conv8 2, conv9 2, conv10 2, and conv11. These feature layers provide high-quality deep features of different granularity (conv4 3 the finest and conv11 the coarsest). A convolutional predictor with 3 \\u00d7 3 kernels is added to each of the 6 layers to detect segments and links. We index the feature layers and the predictors by l = 1, . . . , 6.  Segment Detection Segments are also oriented boxes, de- noted by s = (xs, ys, ws, hs, \\u03b8s). We detect segments by estimating the confidence scores and geometric offsets to a set of default boxes [14] on the input image. Each default box is associated with a feature map location, and its score and offsets are predicted from the features at that location. For simplicity, we only associate one default box with a fea- ture map location.  Consider the l-th feature layer whose feature map size is wl \\u00d7 hl. A location (x, y) on this map corresponds to a default box centered at (xa, ya) on the image, where  xa = wI wl  (x+ 0.5); ya = hI hl  (y + 0.5) (1)  The width and height of the default box are both set to a constant al.  The convolutional predictor produces 7 channels for segment detection. Among them, 2 channels are further softmax-normalized to get the segment score in (0, 1). The rest 5 are the geometric offsets. Considering a location (x, y) on the map, we denote the vector at this location along the depth by (\\u2206xs, \\u2206ys, \\u2206ws, \\u2206hs, \\u2206\\u03b8s). Then, the segment at this location is calculated by:  xs = al\\u2206xs + xa (2) ys = al\\u2206ys + ya (3) ws = al exp(\\u2206ws) (4) hs = al exp(\\u2206hs) (5) \\u03b8s = \\u2206\\u03b8s (6)  Here, the constant al controls the scale of the output seg- ments. It should be chosen with regard to the receptive field size of the l-th layer. We use an empirical equation for choosing this size: al = \\u03b3 wIwl , where \\u03b3 = 1.5.  Within-Layer Link Detection A link connects a pair of adjacent segments, indicating that they belong to the same word. Here, adjacent segments are those detected at adja- cent feature map locations. Links are not only necessary for combining segments into whole words but also helpful for separating two nearby words \\u2013 between two nearby words, the links should be predicted as negative.    (a) Within- Layer Links  (b) Cross- Layer Links  2x  size  conv8_2  conv8_2  conv9_2  16  8  16  Figure 3. Within-Layer and Cross-Layer Links. (a) A location on conv8 2 (yellow block) and its 8-connected neighbors (blue blocks with and without fill). The detected within-layer links (green lines) connect a segment (yellow box) and its two neigh- boring segments (blue boxes) on the same layer. (b) The cross- layer links connect a segment on conv9 2 (yellow box) and two segments on conv8 2 (blue boxes).  We explicitly detect links between segments using the same features for detecting segments. Since we detect only one segment at a feature map location, segments can be indexed by their map locations (x, y) and layer indexes l, denoted by s(x,y,l). As illustrated in Fig. 3.a, we define the within-layer neighbors of a segment as its 8-connected neighbors on the same feature layer:  Nws(x,y,l) = {s (x\\u2032,y\\u2032,l)}x\\u22121\\u2264x\\u2032\\u2264x+1,y\\u22121\\u2264y\\u2032\\u2264y+1 \\\\ s(x,y,l)  (7)  As segments are detected locally, a pair of neighboring seg- ments are also adjacent on input image. Links are also de- tected by the convolutional predictors. A predictor outputs 16 channels for the links to the 8-connected neighboring segments. Every 2 channels are softmax-normalized to get the score of a link.  Cross-Layer Link Detection In our network, segments are detected at different scales on different feature layers. Each layer handles a range of scales. We make these ranges overlap in order not to miss scales at their edges. But as a result, segments of the same word could be detected on multiple layers at the same time, producing redundancies.  To address this problem, we further propose another type of links, called cross-layer links. A cross-layer link connects segments on two feature layers with adjacent in- dexes. For example, cross-layer links are detected between conv4 3 and conv7, because their indexes are l = 1 and l = 2 respectively.  An important property of such a pair is that the first layer always has twice the size as the second one, because of the  down-sampling layer (max-pooling or stride-2 convolution) between them. Note that this property only holds when all feature layers have even-numbered sizes. In practice, we ensured this property by having the width and height of the input image both dividable by 128. For example, an 1000\\u00d7 800 image is resized to 1024 \\u00d7 768, which is the nearest valid size.  As illustrated in Fig. 3.b, we define the cross-layer neighbors of a segment as  (8)N cs(x,y,l) = {s (x\\u2032,y\\u2032,l\\u22121)}2x\\u2264x\\u2032\\u22642x+1,2y\\u2264y\\u2032\\u22642y+1,  which are the segments on the preceeding layer. Every seg- ment has 4 cross-layer neighbors. The correspondence is ensured by the double-size relationship between the two layers.  Again, cross-layer links are detected by the convolu- tional predictor. The predictor outputs 8 channels for cross- layer links. Every 2 channels are softmax-normalized to produce the score of a cross-layer link. Cross-layer links are detected on feature layer l = 2 . . . 6, but not on l = 1 (conv4 3) since it has no preceeding feature layer.  With cross-layer links, segments of different scales can be connected and later combined. Compared with the tradi- tional non-maximum suppression, cross-layer linking pro- vides a trainable way of joining redundancies. Besides, it fits seamlessly into our linking strategy and is easy to im- plement under our framework.  segment  scores  segment  offsets  within-layer  link scores  cross-layer  link scores  2 5 16 8 \\ud835\\udc64\\\"  \\u210e\\\"  Figure 4. Output channels of a convolutional predictor. The block shows a wl \\u00d7 hl map of depth 31. The predictor of l = 1 does not output the channels for corss-layer links.  Outputs of a Convolutional Predictor Putting things to- gether, Fig. 4 shows the output channels of a convolutional predictor. A predictor is implemented by a convolutional layer followed by some softmax layers that normalize the segment and link scores respectively. Thereafter, all lay- ers in our network are convolutional layers. Our network is fully-convolutional.  3.2. Combining Segments with Links  After feed-forwarding, the network produces a number of segments and links (the number depends on the image size). Before combination, the output segments and links are filtered by their confidence scores. We set different fil- tering thresholds for segment and link, respectively \\u03b1 and \\u03b2.    Empirically, the performance of our model is not very sensi- tive to these thresholds. A 0.1 deviation on either thresholds from their optimal values results in less than 1% f-measure drop.  Taking the filtered segments as nodes and the filtered links as edges, we construct a graph over them. Then, a depth-first search (DFS) is performed over the graph to find its connected components. Each component contains a set of segments that are connected by links. Denoting a con- nected component by B, segments within this component are combined following the procedures in Alg. 1.  Algorithm 1 Combining Segments  1: Input: B = {s(i)}|B|i=1 is a set of segments connected by links, where s(i) = (x(i)s , y  (i) s , w  (i) s , h  (i) s , \\u03b8  (i) s ).  2: Find the average angle \\u03b8b := 1|B| \\u2211 B \\u03b8  (i) s .  3: For a straight line (tan \\u03b8b)x + b, find the b that min- imizes the sum of distances to all segment centers (x  (i) s , y  (i) s ).  4: Find the perpendicular projections of all segment cen- ters onto the straight line.  5: From the projected points, find the two with the longest distance. Denote them by (xp, yp) and (xq, yq).  6: xb := 1 2 (xp + xq)  7: yb := 1 2 (yp + yq)  8: wb := \\u221a  (xp \\u2212 xq)2 + (yp \\u2212 yq)2 + 12 (wp + wq) 9: hb :=  1 |B|  \\u2211 B h  (i) s  10: b := (xb, yb, wb, hb, \\u03b8b)  11: Output: b is the combined bounding box.  4. Training 4.1. Groundtruths of Segments and Links  The network is trained by the direct supervision of groundtruth segments and links. The groundtruths include the labels of all default boxes (i.e. the label of their corre- sponding segments), their offsets to the default boxes, and the labels of all within- and cross-layer links. We calculate them from the groundtruth word bounding boxes.  First, we assume that there is only one groundtruth word on the input image. A default box is labeled as positive iff 1) the center of the box is inside the word bounding box; 2) the ratio between the box size al and the word height h satisfies:  max( al h , h  al ) \\u2264 1.5 (9)  Otherwise, the default box is labeled as negative. Next, we consider the case of multiple words. A default  box is labeled as negative if it does not meet the above- mentioned criteria for any word. Otherwise, it is labeled as  positive and matched to the word that has the closest size, i.e. the one with the minimal value at the left-hand side of Eq. 9.  word  bounding box  default box  box center  \\ud835\\udf03  (1) Default box, word bounding box, and  the center of the default box (blue dot)  (2) Rotate word clockwise by \\ud835\\udf03 along  the center of the default box  (3) Crop word bounding box to remove  the parts to the left and right of the  default box  (4) Rotate the cropped box  anticlockwise by \\ud835\\udf03 along the center of  the default box  \\ud835\\udc4e  \\u210e  \\ud835\\udc64% \\t groundtruth   segment  \\u210e%\\t \\ud835\\udc65%, \\ud835\\udc66%  Figure 5. The steps of calculating a groundtruth segment given a default box and a word bounding box.  Offsets are calculated on positive default boxes. First, we calculate the groundtruth segments following the steps illustrated in Fig. 5. Then, we solve Eq. 2 to Eq. 6 to get the groundtruth offsets.  A link (either within-layer or cross-layer) is labeled as positive iff 1) both of the default boxes connected to it are labeled as positive; 2) the two default boxes are matched to the same word.  4.2. Optimization  Objective Our network model is trained by simultane- ously minimizing the losses on segment classification, off- sets regression, and link classification. Overall, the loss function is a weighted sum of the three losses:  L(ys, cs,yl, cl, s\\u0302, s) = 1  Ns Lconf(ys, cs)+\\u03bb1  1  Ns Lloc(s\\u0302, s)  + \\u03bb2 1  Nl Lconf(yl, cl)  (10)  Here, ys is the labels of all segments. y (i) s = 1 if the i-th  default box is labeled as positive, and 0 otherwise. Like- wise, yl is the labels of the links. Lconf is the softmax loss over the predicted segment and link scores, respectively cs and cl. Lloc is the Smooth L1 regression loss [4] over the predicted segment geometries s\\u0302 and the groundtruth s. The losses on segment classification and regression are normal- ized by Ns, which is the number of positive default boxes. The loss on link classification is normalized by the number of positive links Nl. The weight constants \\u03bb1 and \\u03bb2 are both set to 1 in practice.    Online Hard Negative Mining For both segments and links, negatives take up most of the training samples. There- fore, hard negative mining is necessary for balancing the positive and negative samples. We follow the online hard negative mining strategy proposed in [20] to keep the ra- tio between the negatives and positives 3:1 at most. Hard negative mining is performed separately for segments and links.  Data Augmentation We adopt an online augmentation pipeline that is similar to that of SSD [14] and YOLO [18]. Training images are randomly cropped to a patch that has a minimum Jaccard overlap of o with any groundtruth word Crops are resized to the same size before loaded into a batch. For oriented text, the augmentation is performed on the axis-aligned bounding boxes of the words. The overlap o is randomly chosen from 0 (no constraint), 0.1, 0.3, 0.5, 0.7, and 0.9 for every sample. The crop size is randomly chosen from [0.1, 1] of the original image size. Training images are not horizontally flipped.  5. Experiments  In this section, we evaluate the proposed method on three public datasets, namely ICDAR 2015 Incidental Text (Chal- lenge 4), MSRA-TD500, and ICDAR 2013, using the stan- dard evaluation protocol of each.  5.1. Datasets  SynthText in the Wild (SynthText) [6] contains 800,000 synthetic training images. They are created by blending nat- ural images with text rendered with random fonts, size, ori- entation, and color. Text is rendered and aligned to care- fully chosen image regions in order have a realistic look. The dataset provides very detailed annotations for charac- ters, words, and text lines. We only use the dataset for pre- training our network.  ICDAR 2015 Incidental Text (IC15) [12] is the Chal- lenge 4 of the ICDAR 2015 Robust Reading Competition. This challenge features incidental scene text images taken by Google Glasses without taking care of positioning, im- age quality, and viewpoint. Consequently, the dataset ex- hibits large variations in text orientation, scale, and res- olution, making it much more difficult than previous IC- DAR challenges. The dataset contains 1000 training images and 500 testing images. Annotations are provided as word quadrilaterals.  MSRA-TD500 (TD500) [25] is the first standard dataset that focuses on oriented text. The dataset is also multi- lingual, including both Chinese and English text. The  dataset consists of 300 training images and 200 testing im- ages. Different from IC15, TD500 is annotated at the level of text lines.  ICDAR 2013 (IC13) [13] contains mostly horizontal text, with some text slightly oriented. The dataset has been widely adopted for evaluating text detection methods pre- viously. It consists of 229 training images and 233 testing images.  5.2. Implementation Details  Our network is pre-trained on SynthText and finetuned on real datasets (specified later). It is optimized by the stan- dard SGD algorithm with a momentum of 0.9. For both pre- training and finetuning, images are resized to 384\\u00d7384 after random cropping. Since our model is fully-convolutional, we can train it on a certain size and apply it to other sizes during testing. Batch size is set to 32. In pretraining, the learning is set to 10\\u22123 for the first 60k iterations, then de- cayed to 10\\u22124 for the rest 30k iterations. During finetuning, the learning rate is fixed to 10\\u22124 for 5-10k iterations. The number of finetuning iterations depends on the size of the dataset.  Due to the precision-recall tradeoff and the difference between evaluation protocols across datasets, we choose the best thresholds \\u03b1 and \\u03b2 to optimize f-measure. Except for IC15, the thresholds are chosen separately on different datasets via a grid search with 0.1 step on a hold-out vali- dation set. IC15 does not offer an offline evaluation script, so the only way for us is to submit multiple results to the evaluation server.  Our method is implemented using TensorFlow [1] r0.11. All the experiments are carried out on a workstation with an Intel Xeon 8-core CPU (2.8 GHz), 4 Titan X Graphics Cards, and 64GB RAM. Running on 4 GPUs in parallel, training a batch takes about 0.5s. The whole training pro- cess takes less than a day.  5.3. Detecting Oriented English Text  First, we evaluate SegLink on IC15. The pretrained model is finetuned for 10k iterations on the training dataset of IC15. Testing images are resized to 1280 \\u00d7 768. We set the thresholds on segments and links to 0.9 and 0.7, respec- tively. Performance is evaluated by the official central sub- mission server (http:\\/\\/rrc.cvc.uab.es\\/?ch=4). In order to meet the requirements on submission format, the output oriented rectangles are converted into quadrilaterals.  Table 1 lists and compares the results of the proposed method and other state-of-the-art methods. Some results are obtained from the online leaderboard. SegLink outperforms the others by a large margin. In terms of f-measure, it out- performs the second best by 10.2%. Considering that some methods have close or even higher precision than SegLink,  http:\\/\\/rrc.cvc.uab.es\\/?ch=4   Recall=1.0    Precision=0.86    F-Score=0.92 Recall=1.0    Precision=1.0    F-Score=1.0  Recall=1.0    Precision=1.0    F-Score=1.0 Recall=0.88    Precision=0.88    F-Score=0.88  Recall=1.0    Precision=0.88    F-Score=0.93  Recall=1.0    Precision=1.0    F-Score=1.0  Figure 6. Example Results on IC15. Green regions are correctly detected text regions. Red ones are either false positive or false negative. Gray ones are detected but neglected by the evaluation algorithm. Visualizations are generated by the central submission system. Yellow frames contain zoom-in image regions.  Table 1. Results on ICDAR 2015 Incidental Text  Method Precision Recall F-measure HUST MCLAB 47.5 34.8 40.2 NJU Text 72.7 35.8 48.0 StradVision-2 77.5 36.7 49.8 MCLAB FCN [30] 70.8 43.0 53.6 CTPN [22] 51.6 74.2 60.9 Megvii-Image++ 72.4 57.0 63.8 Yao et al. [26] 72.3 58.7 64.8 SegLink 73.1 76.8 75.0  the improvement mainly comes from the recall. As shown in Fig. 6, our method is able to distinguish text from very cluttered backgrounds. In addition, owing to its explicit link prediction, SegLink correctly separates words that are very close to each other.  5.4. Detecting Multi-Lingual Text in Long Lines  We further demonstrate the ability of SegLink to detect long text in non-Latin scripts. TD500 is taken as the dataset for this experiment, as it consists of oriented and multi- lingual text. The training set of TD500 only has 300 im- ages, which are not enough for finetuning our model. We mix the training set of TD500 with the training set of IC15, in the way that every batch has half of its images coming from each dataset. The pretrained model is finetuned for 8k iterations. The testing images are resized to 768\\u00d7768. The thresholds \\u03b1 and \\u03b2 are set to 0.9 and 0.5 respectively. Per- formance scores are calculated by the official development toolkit.  According to Table 2, SegLink achieves the highest scores in terms of precision and f-measure. Benefiting from its fully-convolutional design, SegLink runs at 8.9 FPS, a  Table 2. Results on MSRA-TD500  Method Precision Recall F-measure FPS Kang et al. [11] 71 62 66 - Yao et al. [25] 63 63 60 0.14 Yin et al. [27] 81 63 74 0.71 Yin et al. [28] 71 61 65 1.25 Zhang et al. [30] 83 67 74 0.48 Yao et al. [26] 77 75 76 \\u223c1.61 SegLink 86 70 77 8.9  much faster speed than the others. SegLink also enjoys sim- plicity. The inference process of SegLink is a single forward pass in the detection network, while the previous methods [25, 28, 30] involve sophisticated rule-based grouping or fil- tering steps.  TD500 contains many long lines of text in mixed lan- guages (English and Chinese). Fig. 7 shows how SegLink handles such text. As can be seen, segments and links are densely detected along text lines. They result in long bound- ing boxes that are hard to obtain from a conventional ob- ject detector. Despite the large difference in appearance be- tween English and Chinese text, SegLink is able to handle them simultaneously without any modifications in its struc- ture.  5.5. Detecting Horizontal Text  Lastly, we evaluate the performance of SegLink on horizontal-text datasets. The pretrained model is finetuned for 5k iterations on the combined training sets of IC13 and IC15. Since the most text in IC13 has relatively larger sizes, the testing images are resized to 512\\u00d7 512. The thresholds \\u03b1 and \\u03b2 are set to 0.6 and 0.3, respectively. To match the submission format, we convert the detected oriented boxes    Se gm  en ts   an d  Lin ks  Co m  bi ne  d  Figure 7. Example Results on TD500. The first row shows the detected segments and links. The within-layer and cross-layer links are visualized as red and green lines, respectively. Segments are shown as rectangles in different colors, denoting different connected components. The second row shows the combined boxes.  into their axis-aligned bounding boxes. Table 3 compares SegLink with other state-of-the-art  methods. The scores are calculated by the central sub- mission system using the \\u201cDeteval\\u201d evaluation protocol. SegLink achieves very competitive results in terms of f- measure. Only one approach [22] outperforms SegLink in terms of f-measure. However, [22] is mainly designed for detecting horizontal text and is not well-suited for oriented text. In terms of speed, SegLink runs at over 20 FPS on 512\\u00d7 512 images, much faster than the other methods.  Table 3. Results on IC13. P, R, F stand for precision, recall and f-measure respectively. *These methods are only evaluated under the \\u201cICDAR 2013\\u201d evaluation protocol, the rest under \\u201cDeteval\\u201d. The two protocols usually yield very close scores.  Method P R F FPS Neumann et al. [16]\\u2217 81.8 72.4 77.1 3 Neumann et al. [17]\\u2217 82.1 71.3 76.3 3 Busta et al. [3]\\u2217 84.0 69.3 76.8 6 Zhang et al. [29] 88 74 80 <0.1 Zhang et al. [30] 88 78 83 <1 Jaderberg et al. [9] 88.5 67.8 76.8 <1 Gupta et al. [6] 92.0 75.5 83.0 15 Tian et al. [22] 93.0 83.0 87.7 7.1 SegLink 87.7 83.0 85.3 20.6  5.6. Limitations  A major limitation of SegLink is that two thresholds, \\u03b1 and \\u03b2, need to be set manually. In practice, the optimal values of the thresholds are found by a grid search. Sim- plifying the parameters would be part of our future work. Another weakness is that SegLink fails to detect text that has very large character spacing. Fig. 8.a,b show two such cases. The detected links connect adjacent segments but fail to link distant segments.  (a) (b) (c)  Figure 8. Failure cases on TD500. Red boxes are false posi- tives. (a)(b) SegLink fails to link the characters with large char- acter spacing. (c) SegLink fails to detect curved text.  Fig. 8.c shows that SegLink fails to detect text of curved shape. However, we believe that this is not a limitation of the segment linking strategy, but the segment combination algorithm, which can only produce rectangles currently.  6. Conclusion  We have presented SegLink, a novel text detection strat- egy implemented by a simple and highly-efficient CNN model. The superior performance on horizontal, ori- ented, and multi-lingual text datasets well demonstrate that SegLink is accurate, fast, and flexible. In the future, we will further explore its potentials on detecting deformed text such as curved text. Also, we are interested in extending SegLink into a end-to-end recognition system.  Acknowledgment  This work was supported in part by National Natural Science Foundation of China (61222308 and 61573160), a Google Focused Research Award, AWS Cloud Credits for Research, a Microsoft Research Award and a Facebook equipment donation. The authors also thank China Scholar- ship Council (CSC) for supporting this work.    References [1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,  C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghe- mawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mane\\u0301, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Vie\\u0301gas, O. Vinyals, P. War- den, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. Tensor- Flow: Large-scale machine learning on heterogeneous sys- tems, 2015. Software available from tensorflow.org. 6  [2] A. Bissacco, M. Cummins, Y. Netzer, and H. Neven. Pho- toocr: Reading text in uncontrolled conditions. In ICCV, 2013. 1, 2  [3] M. Busta, L. Neumann, and J. Matas. Fastext: Efficient un- constrained scene text detector. In ICCV, 2015. 8  [4] R. B. Girshick. Fast R-CNN. In ICCV, 2015. 1, 3, 5 [5] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich  feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014. 1, 3  [6] A. Gupta, A. Vedaldi, and A. Zisserman. Synthetic data for text localisation in natural images. In CVPR, 2016. 1, 2, 6, 8  [7] W. Huang, Z. Lin, J. Yang, and J. Wang. Text localization in natural images using stroke feature transform and text co- variance descriptors. In ICCV, 2013. 2  [8] W. Huang, Y. Qiao, and X. Tang. Robust scene text detection with convolution neural network induced MSER trees. In ECCV, 2014. 2  [9] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman. Reading text in the wild with convolutional neural networks. IJCV, 116(1):1\\u201320, 2016. 1, 2, 8  [10] M. Jaderberg, A. Vedaldi, and A. Zisserman. Deep features for text spotting. In ECCV, 2014. 2  [11] L. Kang, Y. Li, and D. S. Doermann. Orientation robust text line detection in natural images. In CVPR, 2014. 7  [12] D. Karatzas, L. Gomez-Bigorda, A. Nicolaou, S. K. Ghosh, A. D. Bagdanov, M. Iwamura, J. Matas, L. Neumann, V. R. Chandrasekhar, S. Lu, F. Shafait, S. Uchida, and E. Val- veny. ICDAR 2015 competition on robust reading. In ICDAR 2015, 2015. 2, 6  [13] D. Karatzas, F. Shafait, S. Uchida, M. Iwamura, L. G. i Big- orda, S. R. Mestre, J. Mas, D. F. Mota, J. Almaza\\u0301n, and L. de las Heras. ICDAR 2013 robust reading competition. In ICDAR 2013, 2013. 6  [14] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. E. Reed, C. Fu, and A. C. Berg. SSD: single shot multibox detector. In ECCV, pages 21\\u201337, 2016. 1, 3, 6  [15] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015. 1  [16] L. Neumann and J. Matas. Efficient scene text localization and recognition with local character refinement. In ICDAR, 2015. 8  [17] L. Neumann and J. Matas. Real-time lexicon-free scene text localization and recognition. PAMI, 38(9):1872\\u20131885, 2016. 2, 8  [18] J. Redmon, S. K. Divvala, R. B. Girshick, and A. Farhadi. You only look once: Unified, real-time object detection. CoRR, abs\\/1506.02640, 2015. 3, 6  [19] S. Ren, K. He, R. B. Girshick, and J. Sun. Faster R-CNN: towards real-time object detection with region proposal net- works. In NIPS, 2015. 1, 3  [20] A. Shrivastava, A. Gupta, and R. B. Girshick. Training region-based object detectors with online hard example min- ing. In CVPR, 2016. 6  [21] K. Simonyan and A. Zisserman. Very deep convolu- tional networks for large-scale image recognition. CoRR, abs\\/1409.1556, 2014. 2, 3  [22] Z. Tian, W. Huang, T. He, P. He, and Y. Qiao. Detecting text in natural image with connectionist text proposal network. In ECCV, 2016. 1, 2, 3, 7, 8  [23] K. Wang and S. J. Belongie. Word spotting in the wild. In ECCV, 2010. 2  [24] T. Wang, D. J. Wu, A. Coates, and A. Y. Ng. End-to-end text recognition with convolutional neural networks. In ICPR, 2012. 2  [25] C. Yao, X. Bai, W. Liu, Y. Ma, and Z. Tu. Detecting texts of arbitrary orientations in natural images. In CVPR, 2012. 1, 2, 6, 7  [26] C. Yao, X. Bai, N. Sang, X. Zhou, S. Zhou, and Z. Cao. Scene text detection via holistic, multi-channel prediction. CoRR, abs\\/1606.09002, 2016. 2, 3, 7  [27] X. Yin, W. Pei, J. Zhang, and H. Hao. Multi-orientation scene text detection with adaptive clustering. PAMI, 37(9):1930\\u20131937, 2015. 7  [28] X. Yin, X. Yin, K. Huang, and H. Hao. Robust text detection in natural scene images. PAMI, 36(5):970\\u2013983, 2014. 7  [29] Z. Zhang, W. Shen, C. Yao, and X. Bai. Symmetry-based text line detection in natural scenes. In CVPR, 2015. 2, 3, 8  [30] Z. Zhang, C. Zhang, W. Shen, C. Yao, W. Liu, and X. Bai. Multi-oriented text detection with fully convolutional net- works. In CVPR, 2016. 1, 2, 3, 7, 8   \"\n",
    "\n",
    "# text = \"\"\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Detecting Oriented Text in Natural Images by Linking Segments Baoguang Shi1 Xiang Bai1∗ Serge Belongie2 1School of EIC, Huazhong University of Science and Technology 2Department of Computer Science, Cornell Tech shibaoguang@gmail.com xbai@hust.edu.cn sjb344@cornell.edu Abstract Most state-of-the-art text detection methods are specific to horizontal Latin text and are not fast enough for real-time applications. We introduce Segment Linking (SegLink), an oriented text detection method. The main idea is to decom- pose text into two locally detectable elements, namely seg- ments and links. A segment is an oriented box covering a part of a word or text line; A link connects two adjacent segments, indicating that they belong to the same word or text line. Both elements are detected densely at multiple scales by an end-to-end trained, fully-convolutional neural network. Final detections are produced by combining seg- ments connected by links. Compared with previous meth- ods, SegLink improves along the dimensions of accuracy, speed, and ease of training. It achieves an f-measure of 75.0% on the standard ICDAR 2015 Incidental (Challenge 4) benchmark, outperforming the previous best by a large margin. It runs at over 20 FPS on 512×512 images. More- over, without modification, SegLink is able to detect long lines of non-Latin text, such as Chinese. 1. Introduction Reading text in natural images is a challenging task un- der active research. It is driven by many real-world appli- cations, such as Photo OCR , geo-location, and image retrieval . In a text reading system, text detection, i.e. localizing text with bounding boxes of words or text lines, is usually the first step of great significance. In a sense, text detection can be seen as object detection applied to text, where words\\\\/characters\\\\/text lines are taken as the detection targets. Owing to this, a new trend has emerged recently that state-of-the-art text detection methods [9, 6, 22, 30] are heavily based on the advanced general object detection or segmentation techniques, e.g. [4, 5, 15]. Despite the great success of the previous work, we ar- gue that the general detection methods are not well suited ∗Corresponding author. (a) (b) (c) (d) (e) (f) Figure 1. SegLink Overview. The upper row shows an image with two words of different scales and orientations. (a) Segments (yellow boxes) are detected on the image. (b) Links (green lines) are detected between pairs of adjacent segments. (c) Segments connected by links are combined into whole words. (d-f) SegLink is able to detect long lines of Latin and non-Latin text, such as Chinese. for text detection, for two main reasons. First, word\\\\/text line bounding boxes have much larger aspect ratios than those of general objects. An (fast\\\\/faster) R-CNN [5, 4, 19]- or SSD -style detector may suffer from the difficulty of producing such boxes, owing to its proposal or anchor box design. In addition, some non-Latin text does not have blank spaces between words, hence the even larger bound- ing box aspect ratios, which make the problem worse. Sec- ond, unlike general objects, text usually has a clear defini- tion of orientation . It is important for a text detector to produce oriented boxes. However, most general object de- tection methods are not designed to produce oriented boxes. To overcome the above challenges, we tackle the text de- tection problem in a new perspective. We propose to de- compose long text into two smaller and locally-detectable elements, namely segment and link. As illustrated in Fig. 1, a segment is an oriented box that covers a part of a word ar X iv :1 70 3. 06 52 0v 3 [ cs .C V ] 1 3 A pr 2 01 7 64 512 VGG16 through pool5 conv 4_3 Input Image (512x512) 32 1024 conv7 (fc7) 16 1024 conv 8_2 8 512 conv 9_2 conv 10_24 conv11 Combining Segments Detections 256 2 256 \\ud835\\udc59 = 1 \\ud835\\udc59 = 6\\ud835\\udc59 = 2 \\ud835\\udc59 = 3 \\ud835\\udc59 = 4 \\ud835\\udc59 = 5 3x3 conv predictors 1024,k3s1 1024,k1s1 256,k1s1 512,k3s2 128,k1s1 256,k3s2 128,k1s1 256,k3s2 256,k3s2 Figure 2. Network Architecture. The network consists of convolutional feature layers (shown as gray blocks) and convolutional predictors (thin gray arrows). Convolutional filters are specified in the format of “(#filters),k(kernel size)s(stride)”. A multi-line filter specification means a hidden layer between. Segments (yellow boxes) and links (not displayed) are detected by convolutional predictors on multiple feature layers (indexed by l = 1 . . . 6) and combined into whole words by a combining algorithm. (for clarity we use “word” here and later on, but segments also work seamlessly on text lines that comprise multiple words); A link connects a pair of adjacent segments, indi- cating that they belong to the same word. Under the above definitions, a word is located by a number of segments with links between them. During detection, segments and links are densely detected on an input image by a convo- lutional neural network. Then, the segments are combined into whole words according to the links. The key advantage of this approach is that long and ori- ented text is now detected locally since both basic elements are locally-detectable: Detecting a segment does not require the whole word to be observed. And neither does a link since the connection of two segments can be inferred from a local context. Thereafter, we can detect text of any length and orientation with great flexibility and efficiency. Concretely, we propose a convolutional neural network (CNN) model to detect both segments and links simultane- ously, in a fully-convolutional manner. The network uses VGG-16 as its backbone. A few extra feature lay- ers are added onto it. Convolutional predictors are added to 6 of the feature layers to detect segments and links at different scales. To deal with redundant detections, we in- troduce two types of links, namely within-layer links and cross-layer links. A within-layer link connects a segment to its neighbors on the same layer. A cross-layer link, on the other hand, connects a segment to its neighbors on the lower layer. In this way, we connect segments of adjacent locations as well as scales. Finally, we find connected seg- ments with a depth-first search (DFS) algorithm and com- bine them into whole words. Our main contribution is the novel segment-linking de- tection method. Through experiments, we show that the proposed method possesses several distinctive advantages over the other state-of-the-art methods: 1) Robustness: SegLink models the structure of oriented text in a simple and elegant way, with robustness against complex back- grounds. Our method achieves highly competitive results on standard datasets. In particular, it outperforms the previ- ous best by a large margin in terms of f-measure (75.0% vs 64.8%) on the ICDAR 2015 Incidental (Challenge 4) bench- mark ; 2) Efficiency: SegLink is highly efficient due to its single-pass, fully-convolutional design. It processes more than 20 images of 512x512 size per second; 3) Gener- ality: Without modification, SegLink is able to detect long lines of non-Latin text, such as Chinese. We demonstrate this capability on a multi-lingual dataset. 2. Related Work Text Detection Over the past few years, much research effort has been devoted to the text detection problem [24, 23, 17, 17, 25, 7, 8, 30, 29, 2, 9, 6, 22, 26]. Based on the ba- sic detection targets, the previous methods can be roughly divided into three categories: character-based, word-based and line-based. Character-based methods [17, 23, 24, 10, 7, 8] detect individual characters and group them into words. These methods find characters by classifying candidate re- gions extracted by region extraction algorithms or by classi- fying sliding windows. Such methods often involve a post- processing step of grouping characters into words. Word- based methods [9, 6] directly detect word bounding boxes. They often have a similar pipeline to the recent CNN-based general object detection networks. Though achieving ex- cellent detection accuracies, these methods may suffer from performance drop when applied to some non-Latin text such as Chinese, as we mentioned earlier. Line-based meth- ods [29, 30, 26] find text regions using some image seg- mentation algorithms. They also require a sophisticated post-processing step of word partitioning and\\\\/or false pos- itive removal. Compared with the previous approaches, our method predicts segments and links jointly in a single forward network pass. The pipeline is much simpler and cleaner. Moreover, the network is end-to-end trainable. Our method is similar in spirit to a recent work , which detects text lines by finding and grouping a sequence of fine-scale text proposals through a CNN coupled with recurrent neural layers. In contrast, we detect oriented seg- ments only using convolutional layers, yielding better flexi- bility and faster speed. Also, we detect links explicitly using the same strong CNN features for segments, improving the robustness. Object Detection Text detection can be seen as a partic- ular instance of general object detection, which is a funda- mental problem in computer vision. Most state-of-the-art detection systems either classify some class-agnostic ob- ject proposals with CNN [5, 4, 19] or directly regress ob- ject bounding boxes from a set of preset boxes (e.g. anchor boxes) [18, 14]. The architecture of our network inherits that of SSD , a recent object detection model. SSD proposed the idea of detecting objects on multiple feature layers with convolu- tional predictors. Our model also detects segments and links in a very similar way. Despite the model similarity, our de- tection strategy is drastically different: SSD directly out- puts object bounding boxes. We, on the other hand, adopt a bottom-up approach by detecting the two comprising ele- ments of a word or text line and combine them together. 3. Segment Linking Our method detects text with a feed-forward CNN model. Given an input image I of size wI × hI , the model outputs a fixed number of segments and links, which are then filtered by their confidence scores and combined into whole word bounding boxes. A bounding box is a ro- tated rectangle denoted by b = (xb, yb, wb, hb, θb), where xb, yb are the coordinates of the center, wb, hb the width and height, and θb the rotation angle. 3.1. CNN Model Fig. 2 shows the network architecture. Our network uses a pretrained VGG-16 network as its backbone (conv1 through pool5). Following , the fully-connected layers of VGG-16 are converted into convolutional layers (fc6 to conv6; fc7 to conv7). They are followed by a few extra con- volutional layers (conv8 1 to conv11), which extract even deeper features with larger receptive fields. Their configu- rations are specified in Fig. 2. Segments and links are detected on 6 of the feature lay- ers, which are conv4 3, conv7, conv8 2, conv9 2, conv10 2, and conv11. These feature layers provide high-quality deep features of different granularity (conv4 3 the finest and conv11 the coarsest). A convolutional predictor with 3 × 3 kernels is added to each of the 6 layers to detect segments and links. We index the feature layers and the predictors by l = 1, . . . , 6. Segment Detection Segments are also oriented boxes, de- noted by s = (xs, ys, ws, hs, θs). We detect segments by estimating the confidence scores and geometric offsets to a set of default boxes on the input image. Each default box is associated with a feature map location, and its score and offsets are predicted from the features at that location. For simplicity, we only associate one default box with a fea- ture map location. Consider the l-th feature layer whose feature map size is wl × hl. A location (x, y) on this map corresponds to a default box centered at (xa, ya) on the image, where xa = wI wl (x+ 0.5); ya = hI hl (y + 0.5) (1) The width and height of the default box are both set to a constant al. The convolutional predictor produces 7 channels for segment detection. Among them, 2 channels are further softmax-normalized to get the segment score in (0, 1). The rest 5 are the geometric offsets. Considering a location (x, y) on the map, we denote the vector at this location along the depth by (∆xs, ∆ys, ∆ws, ∆hs, ∆θs). Then, the segment at this location is calculated by: xs = al∆xs + xa (2) ys = al∆ys + ya (3) ws = al exp(∆ws) (4) hs = al exp(∆hs) (5) θs = ∆θs (6) Here, the constant al controls the scale of the output seg- ments. It should be chosen with regard to the receptive field size of the l-th layer. We use an empirical equation for choosing this size: al = γ wIwl , where γ = 1.5. Within-Layer Link Detection A link connects a pair of adjacent segments, indicating that they belong to the same word. Here, adjacent segments are those detected at adja- cent feature map locations. Links are not only necessary for combining segments into whole words but also helpful for separating two nearby words – between two nearby words, the links should be predicted as negative. (a) Within- Layer Links (b) Cross- Layer Links 2x size conv8_2 conv8_2 conv9_2 16 8 16 Figure 3. Within-Layer and Cross-Layer Links. (a) A location on conv8 2 (yellow block) and its 8-connected neighbors (blue blocks with and without fill). The detected within-layer links (green lines) connect a segment (yellow box) and its two neigh- boring segments (blue boxes) on the same layer. (b) The cross- layer links connect a segment on conv9 2 (yellow box) and two segments on conv8 2 (blue boxes). We explicitly detect links between segments using the same features for detecting segments. Since we detect only one segment at a feature map location, segments can be indexed by their map locations (x, y) and layer indexes l, denoted by s(x,y,l). As illustrated in Fig. 3.a, we define the within-layer neighbors of a segment as its 8-connected neighbors on the same feature layer: Nws(x,y,l) = {s (x′,y′,l)}x−1≤x′≤x+1,y−1≤y′≤y+1 \\\\ s(x,y,l) (7) As segments are detected locally, a pair of neighboring seg- ments are also adjacent on input image. Links are also de- tected by the convolutional predictors. A predictor outputs 16 channels for the links to the 8-connected neighboring segments. Every 2 channels are softmax-normalized to get the score of a link. Cross-Layer Link Detection In our network, segments are detected at different scales on different feature layers. Each layer handles a range of scales. We make these ranges overlap in order not to miss scales at their edges. But as a result, segments of the same word could be detected on multiple layers at the same time, producing redundancies. To address this problem, we further propose another type of links, called cross-layer links. A cross-layer link connects segments on two feature layers with adjacent in- dexes. For example, cross-layer links are detected between conv4 3 and conv7, because their indexes are l = 1 and l = 2 respectively. An important property of such a pair is that the first layer always has twice the size as the second one, because of the down-sampling layer (max-pooling or stride-2 convolution) between them. Note that this property only holds when all feature layers have even-numbered sizes. In practice, we ensured this property by having the width and height of the input image both dividable by 128. For example, an 1000× 800 image is resized to 1024 × 768, which is the nearest valid size. As illustrated in Fig. 3.b, we define the cross-layer neighbors of a segment as (8)N cs(x,y,l) = {s (x′,y′,l−1)}2x≤x′≤2x+1,2y≤y′≤2y+1, which are the segments on the preceeding layer. Every seg- ment has 4 cross-layer neighbors. The correspondence is ensured by the double-size relationship between the two layers. Again, cross-layer links are detected by the convolu- tional predictor. The predictor outputs 8 channels for cross- layer links. Every 2 channels are softmax-normalized to produce the score of a cross-layer link. Cross-layer links are detected on feature layer l = 2 . . . 6, but not on l = 1 (conv4 3) since it has no preceeding feature layer. With cross-layer links, segments of different scales can be connected and later combined. Compared with the tradi- tional non-maximum suppression, cross-layer linking pro- vides a trainable way of joining redundancies. Besides, it fits seamlessly into our linking strategy and is easy to im- plement under our framework. segment scores segment offsets within-layer link scores cross-layer link scores 2 5 16 8 \\ud835\\udc64\" ℎ\" Figure 4. Output channels of a convolutional predictor. The block shows a wl × hl map of depth 31. The predictor of l = 1 does not output the channels for corss-layer links. Outputs of a Convolutional Predictor Putting things to- gether, Fig. 4 shows the output channels of a convolutional predictor. A predictor is implemented by a convolutional layer followed by some softmax layers that normalize the segment and link scores respectively. Thereafter, all lay- ers in our network are convolutional layers. Our network is fully-convolutional. 3.2. Combining Segments with Links After feed-forwarding, the network produces a number of segments and links (the number depends on the image size). Before combination, the output segments and links are filtered by their confidence scores. We set different fil- tering thresholds for segment and link, respectively α and β. Empirically, the performance of our model is not very sensi- tive to these thresholds. A 0.1 deviation on either thresholds from their optimal values results in less than 1% f-measure drop. Taking the filtered segments as nodes and the filtered links as edges, we construct a graph over them. Then, a depth-first search (DFS) is performed over the graph to find its connected components. Each component contains a set of segments that are connected by links. Denoting a con- nected component by B, segments within this component are combined following the procedures in Alg. 1. Algorithm 1 Combining Segments 1: Input: B = {s(i)}|B|i=1 is a set of segments connected by links, where s(i) = (x(i)s , y (i) s , w (i) s , h (i) s , θ (i) s ). 2: Find the average angle θb := 1|B| ∑ B θ (i) s . 3: For a straight line (tan θb)x + b, find the b that min- imizes the sum of distances to all segment centers (x (i) s , y (i) s ). 4: Find the perpendicular projections of all segment cen- ters onto the straight line. 5: From the projected points, find the two with the longest distance. Denote them by (xp, yp) and (xq, yq). 6: xb := 1 2 (xp + xq) 7: yb := 1 2 (yp + yq) 8: wb := √ (xp − xq)2 + (yp − yq)2 + 12 (wp + wq) 9: hb := 1 |B| ∑ B h (i) s 10: b := (xb, yb, wb, hb, θb) 11: Output: b is the combined bounding box. 4. Training 4.1. Groundtruths of Segments and Links The network is trained by the direct supervision of groundtruth segments and links. The groundtruths include the labels of all default boxes (i.e. the label of their corre- sponding segments), their offsets to the default boxes, and the labels of all within- and cross-layer links. We calculate them from the groundtruth word bounding boxes. First, we assume that there is only one groundtruth word on the input image. A default box is labeled as positive iff 1) the center of the box is inside the word bounding box; 2) the ratio between the box size al and the word height h satisfies: max( al h , h al ) ≤ 1.5 (9) Otherwise, the default box is labeled as negative. Next, we consider the case of multiple words. A default box is labeled as negative if it does not meet the above- mentioned criteria for any word. Otherwise, it is labeled as positive and matched to the word that has the closest size, i.e. the one with the minimal value at the left-hand side of Eq. 9. word bounding box default box box center \\ud835\\udf03 (1) Default box, word bounding box, and the center of the default box (blue dot) (2) Rotate word clockwise by \\ud835\\udf03 along the center of the default box (3) Crop word bounding box to remove the parts to the left and right of the default box (4) Rotate the cropped box anticlockwise by \\ud835\\udf03 along the center of the default box \\ud835\\udc4e ℎ \\ud835\\udc64% groundtruth segment ℎ% \\ud835\\udc65%, \\ud835\\udc66% Figure 5. The steps of calculating a groundtruth segment given a default box and a word bounding box. Offsets are calculated on positive default boxes. First, we calculate the groundtruth segments following the steps illustrated in Fig. 5. Then, we solve Eq. 2 to Eq. 6 to get the groundtruth offsets. A link (either within-layer or cross-layer) is labeled as positive iff 1) both of the default boxes connected to it are labeled as positive; 2) the two default boxes are matched to the same word. 4.2. Optimization Objective Our network model is trained by simultane- ously minimizing the losses on segment classification, off- sets regression, and link classification. Overall, the loss function is a weighted sum of the three losses: L(ys, cs,yl, cl, ŝ, s) = 1 Ns Lconf(ys, cs)+λ1 1 Ns Lloc(ŝ, s) + λ2 1 Nl Lconf(yl, cl) (10) Here, ys is the labels of all segments. y (i) s = 1 if the i-th default box is labeled as positive, and 0 otherwise. Like- wise, yl is the labels of the links. Lconf is the softmax loss over the predicted segment and link scores, respectively cs and cl. Lloc is the Smooth L1 regression loss over the predicted segment geometries ŝ and the groundtruth s. The losses on segment classification and regression are normal- ized by Ns, which is the number of positive default boxes. The loss on link classification is normalized by the number of positive links Nl. The weight constants λ1 and λ2 are both set to 1 in practice. Online Hard Negative Mining For both segments and links, negatives take up most of the training samples. There- fore, hard negative mining is necessary for balancing the positive and negative samples. We follow the online hard negative mining strategy proposed in to keep the ra- tio between the negatives and positives 3:1 at most. Hard negative mining is performed separately for segments and links. Data Augmentation We adopt an online augmentation pipeline that is similar to that of SSD and YOLO . Training images are randomly cropped to a patch that has a minimum Jaccard overlap of o with any groundtruth word Crops are resized to the same size before loaded into a batch. For oriented text, the augmentation is performed on the axis-aligned bounding boxes of the words. The overlap o is randomly chosen from 0 (no constraint), 0.1, 0.3, 0.5, 0.7, and 0.9 for every sample. The crop size is randomly chosen from [0.1, 1] of the original image size. Training images are not horizontally flipped. 5. Experiments In this section, we evaluate the proposed method on three public datasets, namely ICDAR 2015 Incidental Text (Chal- lenge 4), MSRA-TD500, and ICDAR 2013, using the stan- dard evaluation protocol of each. 5.1. Datasets SynthText in the Wild (SynthText) contains 800,000 synthetic training images. They are created by blending nat- ural images with text rendered with random fonts, size, ori- entation, and color. Text is rendered and aligned to care- fully chosen image regions in order have a realistic look. The dataset provides very detailed annotations for charac- ters, words, and text lines. We only use the dataset for pre- training our network. ICDAR 2015 Incidental Text (IC15) is the Chal- lenge 4 of the ICDAR 2015 Robust Reading Competition. This challenge features incidental scene text images taken by Google Glasses without taking care of positioning, im- age quality, and viewpoint. Consequently, the dataset ex- hibits large variations in text orientation, scale, and res- olution, making it much more difficult than previous IC- DAR challenges. The dataset contains 1000 training images and 500 testing images. Annotations are provided as word quadrilaterals. MSRA-TD500 (TD500) is the first standard dataset that focuses on oriented text. The dataset is also multi- lingual, including both Chinese and English text. The dataset consists of 300 training images and 200 testing im- ages. Different from IC15, TD500 is annotated at the level of text lines. ICDAR 2013 (IC13) contains mostly horizontal text, with some text slightly oriented. The dataset has been widely adopted for evaluating text detection methods pre- viously. It consists of 229 training images and 233 testing images. 5.2. Implementation Details Our network is pre-trained on SynthText and finetuned on real datasets (specified later). It is optimized by the stan- dard SGD algorithm with a momentum of 0.9. For both pre- training and finetuning, images are resized to 384×384 after random cropping. Since our model is fully-convolutional, we can train it on a certain size and apply it to other sizes during testing. Batch size is set to 32. In pretraining, the learning is set to 10−3 for the first 60k iterations, then de- cayed to 10−4 for the rest 30k iterations. During finetuning, the learning rate is fixed to 10−4 for 5-10k iterations. The number of finetuning iterations depends on the size of the dataset. Due to the precision-recall tradeoff and the difference between evaluation protocols across datasets, we choose the best thresholds α and β to optimize f-measure. Except for IC15, the thresholds are chosen separately on different datasets via a grid search with 0.1 step on a hold-out vali- dation set. IC15 does not offer an offline evaluation script, so the only way for us is to submit multiple results to the evaluation server. Our method is implemented using TensorFlow r0.11. All the experiments are carried out on a workstation with an Intel Xeon 8-core CPU (2.8 GHz), 4 Titan X Graphics Cards, and 64GB RAM. Running on 4 GPUs in parallel, training a batch takes about 0.5s. The whole training pro- cess takes less than a day. 5.3. Detecting Oriented English Text First, we evaluate SegLink on IC15. The pretrained model is finetuned for 10k iterations on the training dataset of IC15. Testing images are resized to 1280 × 768. We set the thresholds on segments and links to 0.9 and 0.7, respec- tively. Performance is evaluated by the official central sub- mission server (http:\\\\/\\\\/rrc.cvc.uab.es\\\\/?ch=4). In order to meet the requirements on submission format, the output oriented rectangles are converted into quadrilaterals. Table 1 lists and compares the results of the proposed method and other state-of-the-art methods. Some results are obtained from the online leaderboard. SegLink outperforms the others by a large margin. In terms of f-measure, it out- performs the second best by 10.2%. Considering that some methods have close or even higher precision than SegLink, http:\\\\/\\\\/rrc.cvc.uab.es\\\\/?ch=4 Recall=1.0 Precision=0.86 F-Score=0.92 Recall=1.0 Precision=1.0 F-Score=1.0 Recall=1.0 Precision=1.0 F-Score=1.0 Recall=0.88 Precision=0.88 F-Score=0.88 Recall=1.0 Precision=0.88 F-Score=0.93 Recall=1.0 Precision=1.0 F-Score=1.0 Figure 6. Example Results on IC15. Green regions are correctly detected text regions. Red ones are either false positive or false negative. Gray ones are detected but neglected by the evaluation algorithm. Visualizations are generated by the central submission system. Yellow frames contain zoom-in image regions. Table 1. Results on ICDAR 2015 Incidental Text Method Precision Recall F-measure HUST MCLAB 47.5 34.8 40.2 NJU Text 72.7 35.8 48.0 StradVision-2 77.5 36.7 49.8 MCLAB FCN 70.8 43.0 53.6 CTPN 51.6 74.2 60.9 Megvii-Image++ 72.4 57.0 63.8 Yao et al. 72.3 58.7 64.8 SegLink 73.1 76.8 75.0 the improvement mainly comes from the recall. As shown in Fig. 6, our method is able to distinguish text from very cluttered backgrounds. In addition, owing to its explicit link prediction, SegLink correctly separates words that are very close to each other. 5.4. Detecting Multi-Lingual Text in Long Lines We further demonstrate the ability of SegLink to detect long text in non-Latin scripts. TD500 is taken as the dataset for this experiment, as it consists of oriented and multi- lingual text. The training set of TD500 only has 300 im- ages, which are not enough for finetuning our model. We mix the training set of TD500 with the training set of IC15, in the way that every batch has half of its images coming from each dataset. The pretrained model is finetuned for 8k iterations. The testing images are resized to 768×768. The thresholds α and β are set to 0.9 and 0.5 respectively. Per- formance scores are calculated by the official development toolkit. According to Table 2, SegLink achieves the highest scores in terms of precision and f-measure. Benefiting from its fully-convolutional design, SegLink runs at 8.9 FPS, a Table 2. Results on MSRA-TD500 Method Precision Recall F-measure FPS Kang et al. 71 62 66 - Yao et al. 63 63 60 0.14 Yin et al. 81 63 74 0.71 Yin et al. 71 61 65 1.25 Zhang et al. 83 67 74 0.48 Yao et al. 77 75 76 ∼1.61 SegLink 86 70 77 8.9 much faster speed than the others. SegLink also enjoys sim- plicity. The inference process of SegLink is a single forward pass in the detection network, while the previous methods [25, 28, 30] involve sophisticated rule-based grouping or fil- tering steps. TD500 contains many long lines of text in mixed lan- guages (English and Chinese). Fig. 7 shows how SegLink handles such text. As can be seen, segments and links are densely detected along text lines. They result in long bound- ing boxes that are hard to obtain from a conventional ob- ject detector. Despite the large difference in appearance be- tween English and Chinese text, SegLink is able to handle them simultaneously without any modifications in its struc- ture. 5.5. Detecting Horizontal Text Lastly, we evaluate the performance of SegLink on horizontal-text datasets. The pretrained model is finetuned for 5k iterations on the combined training sets of IC13 and IC15. Since the most text in IC13 has relatively larger sizes, the testing images are resized to 512× 512. The thresholds α and β are set to 0.6 and 0.3, respectively. To match the submission format, we convert the detected oriented boxes Se gm en ts an d Lin ks Co m bi ne d Figure 7. Example Results on TD500. The first row shows the detected segments and links. The within-layer and cross-layer links are visualized as red and green lines, respectively. Segments are shown as rectangles in different colors, denoting different connected components. The second row shows the combined boxes. into their axis-aligned bounding boxes. Table 3 compares SegLink with other state-of-the-art methods. The scores are calculated by the central sub- mission system using the “Deteval” evaluation protocol. SegLink achieves very competitive results in terms of f- measure. Only one approach outperforms SegLink in terms of f-measure. However, is mainly designed for detecting horizontal text and is not well-suited for oriented text. In terms of speed, SegLink runs at over 20 FPS on 512× 512 images, much faster than the other methods. Table 3. Results on IC13. P, R, F stand for precision, recall and f-measure respectively. *These methods are only evaluated under the “ICDAR 2013” evaluation protocol, the rest under “Deteval”. The two protocols usually yield very close scores. Method P R F FPS Neumann et al. ∗ 81.8 72.4 77.1 3 Neumann et al. ∗ 82.1 71.3 76.3 3 Busta et al. ∗ 84.0 69.3 76.8 6 Zhang et al. 88 74 80 <0.1 Zhang et al. 88 78 83 <1 Jaderberg et al. 88.5 67.8 76.8 <1 Gupta et al. 92.0 75.5 83.0 15 Tian et al. 93.0 83.0 87.7 7.1 SegLink 87.7 83.0 85.3 20.6 5.6. Limitations A major limitation of SegLink is that two thresholds, α and β, need to be set manually. In practice, the optimal values of the thresholds are found by a grid search. Sim- plifying the parameters would be part of our future work. Another weakness is that SegLink fails to detect text that has very large character spacing. Fig. 8.a,b show two such cases. The detected links connect adjacent segments but fail to link distant segments. (a) (b) (c) Figure 8. Failure cases on TD500. Red boxes are false posi- tives. (a)(b) SegLink fails to link the characters with large char- acter spacing. (c) SegLink fails to detect curved text. Fig. 8.c shows that SegLink fails to detect text of curved shape. However, we believe that this is not a limitation of the segment linking strategy, but the segment combination algorithm, which can only produce rectangles currently. 6. Conclusion We have presented SegLink, a novel text detection strat- egy implemented by a simple and highly-efficient CNN model. The superior performance on horizontal, ori- ented, and multi-lingual text datasets well demonstrate that SegLink is accurate, fast, and flexible. In the future, we will further explore its potentials on detecting deformed text such as curved text. Also, we are interested in extending SegLink into a end-to-end recognition system. Acknowledgment This work was supported in part by National Natural Science Foundation of China (61222308 and 61573160), a Google Focused Research Award, AWS Cloud Credits for Research, a Microsoft Research Award and a Facebook equipment donation. The authors also thank China Scholar- ship Council (CSC) for supporting this work. References M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghe- mawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mané, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viégas, O. Vinyals, P. War- den, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. Tensor- Flow: Large-scale machine learning on heterogeneous sys- tems, 2015. Software available from tensorflow.org. 6 A. Bissacco, M. Cummins, Y. Netzer, and H. Neven. Pho- toocr: Reading text in uncontrolled conditions. In ICCV, 2013. 1, 2 M. Busta, L. Neumann, and J. Matas. Fastext: Efficient un- constrained scene text detector. In ICCV, 2015. 8 R. B. Girshick. Fast R-CNN. In ICCV, 2015. 1, 3, 5 R. B. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014. 1, 3 A. Gupta, A. Vedaldi, and A. Zisserman. Synthetic data for text localisation in natural images. In CVPR, 2016. 1, 2, 6, 8 W. Huang, Z. Lin, J. Yang, and J. Wang. Text localization in natural images using stroke feature transform and text co- variance descriptors. In ICCV, 2013. 2 W. Huang, Y. Qiao, and X. Tang. Robust scene text detection with convolution neural network induced MSER trees. In ECCV, 2014. 2 M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman. Reading text in the wild with convolutional neural networks. IJCV, 116(1):1–20, 2016. 1, 2, 8 M. Jaderberg, A. Vedaldi, and A. Zisserman. Deep features for text spotting. In ECCV, 2014. 2 L. Kang, Y. Li, and D. S. Doermann. Orientation robust text line detection in natural images. In CVPR, 2014. 7 D. Karatzas, L. Gomez-Bigorda, A. Nicolaou, S. K. Ghosh, A. D. Bagdanov, M. Iwamura, J. Matas, L. Neumann, V. R. Chandrasekhar, S. Lu, F. Shafait, S. Uchida, and E. Val- veny. ICDAR 2015 competition on robust reading. In ICDAR 2015, 2015. 2, 6 D. Karatzas, F. Shafait, S. Uchida, M. Iwamura, L. G. i Big- orda, S. R. Mestre, J. Mas, D. F. Mota, J. Almazán, and L. de las Heras. ICDAR 2013 robust reading competition. In ICDAR 2013, 2013. 6 W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. E. Reed, C. Fu, and A. C. Berg. SSD: single shot multibox detector. In ECCV, pages 21–37, 2016. 1, 3, 6 J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015. 1 L. Neumann and J. Matas. Efficient scene text localization and recognition with local character refinement. In ICDAR, 2015. 8 L. Neumann and J. Matas. Real-time lexicon-free scene text localization and recognition. PAMI, 38(9):1872–1885, 2016. 2, 8 J. Redmon, S. K. Divvala, R. B. Girshick, and A. Farhadi. You only look once: Unified, real-time object detection. CoRR, abs\\\\/1506.02640, 2015. 3, 6 S. Ren, K. He, R. B. Girshick, and J. Sun. Faster R-CNN: towards real-time object detection with region proposal net- works. In NIPS, 2015. 1, 3 A. Shrivastava, A. Gupta, and R. B. Girshick. Training region-based object detectors with online hard example min- ing. In CVPR, 2016. 6 K. Simonyan and A. Zisserman. Very deep convolu- tional networks for large-scale image recognition. CoRR, abs\\\\/1409.1556, 2014. 2, 3 Z. Tian, W. Huang, T. He, P. He, and Y. Qiao. Detecting text in natural image with connectionist text proposal network. In ECCV, 2016. 1, 2, 3, 7, 8 K. Wang and S. J. Belongie. Word spotting in the wild. In ECCV, 2010. 2 T. Wang, D. J. Wu, A. Coates, and A. Y. Ng. End-to-end text recognition with convolutional neural networks. In ICPR, 2012. 2 C. Yao, X. Bai, W. Liu, Y. Ma, and Z. Tu. Detecting texts of arbitrary orientations in natural images. In CVPR, 2012. 1, 2, 6, 7 C. Yao, X. Bai, N. Sang, X. Zhou, S. Zhou, and Z. Cao. Scene text detection via holistic, multi-channel prediction. CoRR, abs\\\\/1606.09002, 2016. 2, 3, 7 X. Yin, W. Pei, J. Zhang, and H. Hao. Multi-orientation scene text detection with adaptive clustering. PAMI, 37(9):1930–1937, 2015. 7 X. Yin, X. Yin, K. Huang, and H. Hao. Robust text detection in natural scene images. PAMI, 36(5):970–983, 2014. 7 Z. Zhang, W. Shen, C. Yao, and X. Bai. Symmetry-based text line detection in natural scenes. In CVPR, 2015. 2, 3, 8 Z. Zhang, C. Zhang, W. Shen, C. Yao, W. Liu, and X. Bai. Multi-oriented text detection with fully convolutional net- works. In CVPR, 2016. 1, 2, 3, 7, 8 '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removes square brackets and extra spaces\n",
    "text = re.sub(r'\\[[0-9]*\\]', ' ', text)  \n",
    "text = re.sub(r'\\s+', ' ', text)  \n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Detecting Oriented Text in Natural Images by Linking Segments Baoguang Shi Xiang Bai Serge Belongie School of EIC Huazhong University of Science and Technology Department of Computer Science Cornell Tech shibaoguang gmail com xbai hust edu cn sjb cornell edu Abstract Most state of the art text detection methods are specific to horizontal Latin text and are not fast enough for real time applications We introduce Segment Linking SegLink an oriented text detection method The main idea is to decom pose text into two locally detectable elements namely seg ments and links A segment is an oriented box covering a part of a word or text line A link connects two adjacent segments indicating that they belong to the same word or text line Both elements are detected densely at multiple scales by an end to end trained fully convolutional neural network Final detections are produced by combining seg ments connected by links Compared with previous meth ods SegLink improves along the dimensions of accuracy speed and ease of training It achieves an f measure of on the standard ICDAR Incidental Challenge benchmark outperforming the previous best by a large margin It runs at over FPS on images More over without modification SegLink is able to detect long lines of non Latin text such as Chinese Introduction Reading text in natural images is a challenging task un der active research It is driven by many real world appli cations such as Photo OCR geo location and image retrieval In a text reading system text detection i e localizing text with bounding boxes of words or text lines is usually the first step of great significance In a sense text detection can be seen as object detection applied to text where words characters text lines are taken as the detection targets Owing to this a new trend has emerged recently that state of the art text detection methods are heavily based on the advanced general object detection or segmentation techniques e g Despite the great success of the previous work we ar gue that the general detection methods are not well suited Corresponding author a b c d e f Figure SegLink Overview The upper row shows an image with two words of different scales and orientations a Segments yellow boxes are detected on the image b Links green lines are detected between pairs of adjacent segments c Segments connected by links are combined into whole words d f SegLink is able to detect long lines of Latin and non Latin text such as Chinese for text detection for two main reasons First word text line bounding boxes have much larger aspect ratios than those of general objects An fast faster R CNN or SSD style detector may suffer from the difficulty of producing such boxes owing to its proposal or anchor box design In addition some non Latin text does not have blank spaces between words hence the even larger bound ing box aspect ratios which make the problem worse Sec ond unlike general objects text usually has a clear defini tion of orientation It is important for a text detector to produce oriented boxes However most general object de tection methods are not designed to produce oriented boxes To overcome the above challenges we tackle the text de tection problem in a new perspective We propose to de compose long text into two smaller and locally detectable elements namely segment and link As illustrated in Fig a segment is an oriented box that covers a part of a word ar X iv v cs C V A pr VGG through pool conv Input Image x conv fc conv conv conv conv Combining Segments Detections x conv predictors k s k s k s k s k s k s k s k s k s Figure Network Architecture The network consists of convolutional feature layers shown as gray blocks and convolutional predictors thin gray arrows Convolutional filters are specified in the format of filters k kernel size s stride A multi line filter specification means a hidden layer between Segments yellow boxes and links not displayed are detected by convolutional predictors on multiple feature layers indexed by l and combined into whole words by a combining algorithm for clarity we use word here and later on but segments also work seamlessly on text lines that comprise multiple words A link connects a pair of adjacent segments indi cating that they belong to the same word Under the above definitions a word is located by a number of segments with links between them During detection segments and links are densely detected on an input image by a convo lutional neural network Then the segments are combined into whole words according to the links The key advantage of this approach is that long and ori ented text is now detected locally since both basic elements are locally detectable Detecting a segment does not require the whole word to be observed And neither does a link since the connection of two segments can be inferred from a local context Thereafter we can detect text of any length and orientation with great flexibility and efficiency Concretely we propose a convolutional neural network CNN model to detect both segments and links simultane ously in a fully convolutional manner The network uses VGG as its backbone A few extra feature lay ers are added onto it Convolutional predictors are added to of the feature layers to detect segments and links at different scales To deal with redundant detections we in troduce two types of links namely within layer links and cross layer links A within layer link connects a segment to its neighbors on the same layer A cross layer link on the other hand connects a segment to its neighbors on the lower layer In this way we connect segments of adjacent locations as well as scales Finally we find connected seg ments with a depth first search DFS algorithm and com bine them into whole words Our main contribution is the novel segment linking de tection method Through experiments we show that the proposed method possesses several distinctive advantages over the other state of the art methods Robustness SegLink models the structure of oriented text in a simple and elegant way with robustness against complex back grounds Our method achieves highly competitive results on standard datasets In particular it outperforms the previ ous best by a large margin in terms of f measure vs on the ICDAR Incidental Challenge bench mark Efficiency SegLink is highly efficient due to its single pass fully convolutional design It processes more than images of x size per second Gener ality Without modification SegLink is able to detect long lines of non Latin text such as Chinese We demonstrate this capability on a multi lingual dataset Related Work Text Detection Over the past few years much research effort has been devoted to the text detection problem Based on the ba sic detection targets the previous methods can be roughly divided into three categories character based word based and line based Character based methods detect individual characters and group them into words These methods find characters by classifying candidate re gions extracted by region extraction algorithms or by classi fying sliding windows Such methods often involve a post processing step of grouping characters into words Word based methods directly detect word bounding boxes They often have a similar pipeline to the recent CNN based general object detection networks Though achieving ex cellent detection accuracies these methods may suffer from performance drop when applied to some non Latin text such as Chinese as we mentioned earlier Line based meth ods find text regions using some image seg mentation algorithms They also require a sophisticated post processing step of word partitioning and or false pos itive removal Compared with the previous approaches our method predicts segments and links jointly in a single forward network pass The pipeline is much simpler and cleaner Moreover the network is end to end trainable Our method is similar in spirit to a recent work which detects text lines by finding and grouping a sequence of fine scale text proposals through a CNN coupled with recurrent neural layers In contrast we detect oriented seg ments only using convolutional layers yielding better flexi bility and faster speed Also we detect links explicitly using the same strong CNN features for segments improving the robustness Object Detection Text detection can be seen as a partic ular instance of general object detection which is a funda mental problem in computer vision Most state of the art detection systems either classify some class agnostic ob ject proposals with CNN or directly regress ob ject bounding boxes from a set of preset boxes e g anchor boxes The architecture of our network inherits that of SSD a recent object detection model SSD proposed the idea of detecting objects on multiple feature layers with convolu tional predictors Our model also detects segments and links in a very similar way Despite the model similarity our de tection strategy is drastically different SSD directly out puts object bounding boxes We on the other hand adopt a bottom up approach by detecting the two comprising ele ments of a word or text line and combine them together Segment Linking Our method detects text with a feed forward CNN model Given an input image I of size wI hI the model outputs a fixed number of segments and links which are then filtered by their confidence scores and combined into whole word bounding boxes A bounding box is a ro tated rectangle denoted by b xb yb wb hb b where xb yb are the coordinates of the center wb hb the width and height and b the rotation angle CNN Model Fig shows the network architecture Our network uses a pretrained VGG network as its backbone conv through pool Following the fully connected layers of VGG are converted into convolutional layers fc to conv fc to conv They are followed by a few extra con volutional layers conv to conv which extract even deeper features with larger receptive fields Their configu rations are specified in Fig Segments and links are detected on of the feature lay ers which are conv conv conv conv conv and conv These feature layers provide high quality deep features of different granularity conv the finest and conv the coarsest A convolutional predictor with kernels is added to each of the layers to detect segments and links We index the feature layers and the predictors by l Segment Detection Segments are also oriented boxes de noted by s xs ys ws hs s We detect segments by estimating the confidence scores and geometric offsets to a set of default boxes on the input image Each default box is associated with a feature map location and its score and offsets are predicted from the features at that location For simplicity we only associate one default box with a fea ture map location Consider the l th feature layer whose feature map size is wl hl A location x y on this map corresponds to a default box centered at xa ya on the image where xa wI wl x ya hI hl y The width and height of the default box are both set to a constant al The convolutional predictor produces channels for segment detection Among them channels are further softmax normalized to get the segment score in The rest are the geometric offsets Considering a location x y on the map we denote the vector at this location along the depth by xs ys ws hs s Then the segment at this location is calculated by xs al xs xa ys al ys ya ws al exp ws hs al exp hs s s Here the constant al controls the scale of the output seg ments It should be chosen with regard to the receptive field size of the l th layer We use an empirical equation for choosing this size al wIwl where Within Layer Link Detection A link connects a pair of adjacent segments indicating that they belong to the same word Here adjacent segments are those detected at adja cent feature map locations Links are not only necessary for combining segments into whole words but also helpful for separating two nearby words between two nearby words the links should be predicted as negative a Within Layer Links b Cross Layer Links x size conv conv conv Figure Within Layer and Cross Layer Links a A location on conv yellow block and its connected neighbors blue blocks with and without fill The detected within layer links green lines connect a segment yellow box and its two neigh boring segments blue boxes on the same layer b The cross layer links connect a segment on conv yellow box and two segments on conv blue boxes We explicitly detect links between segments using the same features for detecting segments Since we detect only one segment at a feature map location segments can be indexed by their map locations x y and layer indexes l denoted by s x y l As illustrated in Fig a we define the within layer neighbors of a segment as its connected neighbors on the same feature layer Nws x y l s x y l x x x y y y s x y l As segments are detected locally a pair of neighboring seg ments are also adjacent on input image Links are also de tected by the convolutional predictors A predictor outputs channels for the links to the connected neighboring segments Every channels are softmax normalized to get the score of a link Cross Layer Link Detection In our network segments are detected at different scales on different feature layers Each layer handles a range of scales We make these ranges overlap in order not to miss scales at their edges But as a result segments of the same word could be detected on multiple layers at the same time producing redundancies To address this problem we further propose another type of links called cross layer links A cross layer link connects segments on two feature layers with adjacent in dexes For example cross layer links are detected between conv and conv because their indexes are l and l respectively An important property of such a pair is that the first layer always has twice the size as the second one because of the down sampling layer max pooling or stride convolution between them Note that this property only holds when all feature layers have even numbered sizes In practice we ensured this property by having the width and height of the input image both dividable by For example an image is resized to which is the nearest valid size As illustrated in Fig b we define the cross layer neighbors of a segment as N cs x y l s x y l x x x y y y which are the segments on the preceeding layer Every seg ment has cross layer neighbors The correspondence is ensured by the double size relationship between the two layers Again cross layer links are detected by the convolu tional predictor The predictor outputs channels for cross layer links Every channels are softmax normalized to produce the score of a cross layer link Cross layer links are detected on feature layer l but not on l conv since it has no preceeding feature layer With cross layer links segments of different scales can be connected and later combined Compared with the tradi tional non maximum suppression cross layer linking pro vides a trainable way of joining redundancies Besides it fits seamlessly into our linking strategy and is easy to im plement under our framework segment scores segment offsets within layer link scores cross layer link scores Figure Output channels of a convolutional predictor The block shows a wl hl map of depth The predictor of l does not output the channels for corss layer links Outputs of a Convolutional Predictor Putting things to gether Fig shows the output channels of a convolutional predictor A predictor is implemented by a convolutional layer followed by some softmax layers that normalize the segment and link scores respectively Thereafter all lay ers in our network are convolutional layers Our network is fully convolutional Combining Segments with Links After feed forwarding the network produces a number of segments and links the number depends on the image size Before combination the output segments and links are filtered by their confidence scores We set different fil tering thresholds for segment and link respectively and Empirically the performance of our model is not very sensi tive to these thresholds A deviation on either thresholds from their optimal values results in less than f measure drop Taking the filtered segments as nodes and the filtered links as edges we construct a graph over them Then a depth first search DFS is performed over the graph to find its connected components Each component contains a set of segments that are connected by links Denoting a con nected component by B segments within this component are combined following the procedures in Alg Algorithm Combining Segments Input B s i B i is a set of segments connected by links where s i x i s y i s w i s h i s i s Find the average angle b B B i s For a straight line tan b x b find the b that min imizes the sum of distances to all segment centers x i s y i s Find the perpendicular projections of all segment cen ters onto the straight line From the projected points find the two with the longest distance Denote them by xp yp and xq yq xb xp xq yb yp yq wb xp xq yp yq wp wq hb B B h i s b xb yb wb hb b Output b is the combined bounding box Training Groundtruths of Segments and Links The network is trained by the direct supervision of groundtruth segments and links The groundtruths include the labels of all default boxes i e the label of their corre sponding segments their offsets to the default boxes and the labels of all within and cross layer links We calculate them from the groundtruth word bounding boxes First we assume that there is only one groundtruth word on the input image A default box is labeled as positive iff the center of the box is inside the word bounding box the ratio between the box size al and the word height h satisfies max al h h al Otherwise the default box is labeled as negative Next we consider the case of multiple words A default box is labeled as negative if it does not meet the above mentioned criteria for any word Otherwise it is labeled as positive and matched to the word that has the closest size i e the one with the minimal value at the left hand side of Eq word bounding box default box box center Default box word bounding box and the center of the default box blue dot Rotate word clockwise by along the center of the default box Crop word bounding box to remove the parts to the left and right of the default box Rotate the cropped box anticlockwise by along the center of the default box groundtruth segment Figure The steps of calculating a groundtruth segment given a default box and a word bounding box Offsets are calculated on positive default boxes First we calculate the groundtruth segments following the steps illustrated in Fig Then we solve Eq to Eq to get the groundtruth offsets A link either within layer or cross layer is labeled as positive iff both of the default boxes connected to it are labeled as positive the two default boxes are matched to the same word Optimization Objective Our network model is trained by simultane ously minimizing the losses on segment classification off sets regression and link classification Overall the loss function is a weighted sum of the three losses L ys cs yl cl s s Ns Lconf ys cs Ns Lloc s s Nl Lconf yl cl Here ys is the labels of all segments y i s if the i th default box is labeled as positive and otherwise Like wise yl is the labels of the links Lconf is the softmax loss over the predicted segment and link scores respectively cs and cl Lloc is the Smooth L regression loss over the predicted segment geometries s and the groundtruth s The losses on segment classification and regression are normal ized by Ns which is the number of positive default boxes The loss on link classification is normalized by the number of positive links Nl The weight constants and are both set to in practice Online Hard Negative Mining For both segments and links negatives take up most of the training samples There fore hard negative mining is necessary for balancing the positive and negative samples We follow the online hard negative mining strategy proposed in to keep the ra tio between the negatives and positives at most Hard negative mining is performed separately for segments and links Data Augmentation We adopt an online augmentation pipeline that is similar to that of SSD and YOLO Training images are randomly cropped to a patch that has a minimum Jaccard overlap of o with any groundtruth word Crops are resized to the same size before loaded into a batch For oriented text the augmentation is performed on the axis aligned bounding boxes of the words The overlap o is randomly chosen from no constraint and for every sample The crop size is randomly chosen from of the original image size Training images are not horizontally flipped Experiments In this section we evaluate the proposed method on three public datasets namely ICDAR Incidental Text Chal lenge MSRA TD and ICDAR using the stan dard evaluation protocol of each Datasets SynthText in the Wild SynthText contains synthetic training images They are created by blending nat ural images with text rendered with random fonts size ori entation and color Text is rendered and aligned to care fully chosen image regions in order have a realistic look The dataset provides very detailed annotations for charac ters words and text lines We only use the dataset for pre training our network ICDAR Incidental Text IC is the Chal lenge of the ICDAR Robust Reading Competition This challenge features incidental scene text images taken by Google Glasses without taking care of positioning im age quality and viewpoint Consequently the dataset ex hibits large variations in text orientation scale and res olution making it much more difficult than previous IC DAR challenges The dataset contains training images and testing images Annotations are provided as word quadrilaterals MSRA TD TD is the first standard dataset that focuses on oriented text The dataset is also multi lingual including both Chinese and English text The dataset consists of training images and testing im ages Different from IC TD is annotated at the level of text lines ICDAR IC contains mostly horizontal text with some text slightly oriented The dataset has been widely adopted for evaluating text detection methods pre viously It consists of training images and testing images Implementation Details Our network is pre trained on SynthText and finetuned on real datasets specified later It is optimized by the stan dard SGD algorithm with a momentum of For both pre training and finetuning images are resized to after random cropping Since our model is fully convolutional we can train it on a certain size and apply it to other sizes during testing Batch size is set to In pretraining the learning is set to for the first k iterations then de cayed to for the rest k iterations During finetuning the learning rate is fixed to for k iterations The number of finetuning iterations depends on the size of the dataset Due to the precision recall tradeoff and the difference between evaluation protocols across datasets we choose the best thresholds and to optimize f measure Except for IC the thresholds are chosen separately on different datasets via a grid search with step on a hold out vali dation set IC does not offer an offline evaluation script so the only way for us is to submit multiple results to the evaluation server Our method is implemented using TensorFlow r All the experiments are carried out on a workstation with an Intel Xeon core CPU GHz Titan X Graphics Cards and GB RAM Running on GPUs in parallel training a batch takes about s The whole training pro cess takes less than a day Detecting Oriented English Text First we evaluate SegLink on IC The pretrained model is finetuned for k iterations on the training dataset of IC Testing images are resized to We set the thresholds on segments and links to and respec tively Performance is evaluated by the official central sub mission server http rrc cvc uab es ch In order to meet the requirements on submission format the output oriented rectangles are converted into quadrilaterals Table lists and compares the results of the proposed method and other state of the art methods Some results are obtained from the online leaderboard SegLink outperforms the others by a large margin In terms of f measure it out performs the second best by Considering that some methods have close or even higher precision than SegLink http rrc cvc uab es ch Recall Precision F Score Recall Precision F Score Recall Precision F Score Recall Precision F Score Recall Precision F Score Recall Precision F Score Figure Example Results on IC Green regions are correctly detected text regions Red ones are either false positive or false negative Gray ones are detected but neglected by the evaluation algorithm Visualizations are generated by the central submission system Yellow frames contain zoom in image regions Table Results on ICDAR Incidental Text Method Precision Recall F measure HUST MCLAB NJU Text StradVision MCLAB FCN CTPN Megvii Image Yao et al SegLink the improvement mainly comes from the recall As shown in Fig our method is able to distinguish text from very cluttered backgrounds In addition owing to its explicit link prediction SegLink correctly separates words that are very close to each other Detecting Multi Lingual Text in Long Lines We further demonstrate the ability of SegLink to detect long text in non Latin scripts TD is taken as the dataset for this experiment as it consists of oriented and multi lingual text The training set of TD only has im ages which are not enough for finetuning our model We mix the training set of TD with the training set of IC in the way that every batch has half of its images coming from each dataset The pretrained model is finetuned for k iterations The testing images are resized to The thresholds and are set to and respectively Per formance scores are calculated by the official development toolkit According to Table SegLink achieves the highest scores in terms of precision and f measure Benefiting from its fully convolutional design SegLink runs at FPS a Table Results on MSRA TD Method Precision Recall F measure FPS Kang et al Yao et al Yin et al Yin et al Zhang et al Yao et al SegLink much faster speed than the others SegLink also enjoys sim plicity The inference process of SegLink is a single forward pass in the detection network while the previous methods involve sophisticated rule based grouping or fil tering steps TD contains many long lines of text in mixed lan guages English and Chinese Fig shows how SegLink handles such text As can be seen segments and links are densely detected along text lines They result in long bound ing boxes that are hard to obtain from a conventional ob ject detector Despite the large difference in appearance be tween English and Chinese text SegLink is able to handle them simultaneously without any modifications in its struc ture Detecting Horizontal Text Lastly we evaluate the performance of SegLink on horizontal text datasets The pretrained model is finetuned for k iterations on the combined training sets of IC and IC Since the most text in IC has relatively larger sizes the testing images are resized to The thresholds and are set to and respectively To match the submission format we convert the detected oriented boxes Se gm en ts an d Lin ks Co m bi ne d Figure Example Results on TD The first row shows the detected segments and links The within layer and cross layer links are visualized as red and green lines respectively Segments are shown as rectangles in different colors denoting different connected components The second row shows the combined boxes into their axis aligned bounding boxes Table compares SegLink with other state of the art methods The scores are calculated by the central sub mission system using the Deteval evaluation protocol SegLink achieves very competitive results in terms of f measure Only one approach outperforms SegLink in terms of f measure However is mainly designed for detecting horizontal text and is not well suited for oriented text In terms of speed SegLink runs at over FPS on images much faster than the other methods Table Results on IC P R F stand for precision recall and f measure respectively These methods are only evaluated under the ICDAR evaluation protocol the rest under Deteval The two protocols usually yield very close scores Method P R F FPS Neumann et al Neumann et al Busta et al Zhang et al Zhang et al Jaderberg et al Gupta et al Tian et al SegLink Limitations A major limitation of SegLink is that two thresholds and need to be set manually In practice the optimal values of the thresholds are found by a grid search Sim plifying the parameters would be part of our future work Another weakness is that SegLink fails to detect text that has very large character spacing Fig a b show two such cases The detected links connect adjacent segments but fail to link distant segments a b c Figure Failure cases on TD Red boxes are false posi tives a b SegLink fails to link the characters with large char acter spacing c SegLink fails to detect curved text Fig c shows that SegLink fails to detect text of curved shape However we believe that this is not a limitation of the segment linking strategy but the segment combination algorithm which can only produce rectangles currently Conclusion We have presented SegLink a novel text detection strat egy implemented by a simple and highly efficient CNN model The superior performance on horizontal ori ented and multi lingual text datasets well demonstrate that SegLink is accurate fast and flexible In the future we will further explore its potentials on detecting deformed text such as curved text Also we are interested in extending SegLink into a end to end recognition system Acknowledgment This work was supported in part by National Natural Science Foundation of China and a Google Focused Research Award AWS Cloud Credits for Research a Microsoft Research Award and a Facebook equipment donation The authors also thank China Scholar ship Council CSC for supporting this work References M Abadi A Agarwal P Barham E Brevdo Z Chen C Citro G S Corrado A Davis J Dean M Devin S Ghe mawat I Goodfellow A Harp G Irving M Isard Y Jia R Jozefowicz L Kaiser M Kudlur J Levenberg D Mane R Monga S Moore D Murray C Olah M Schuster J Shlens B Steiner I Sutskever K Talwar P Tucker V Vanhoucke V Vasudevan F Vie gas O Vinyals P War den M Wattenberg M Wicke Y Yu and X Zheng Tensor Flow Large scale machine learning on heterogeneous sys tems Software available from tensorflow org A Bissacco M Cummins Y Netzer and H Neven Pho toocr Reading text in uncontrolled conditions In ICCV M Busta L Neumann and J Matas Fastext Efficient un constrained scene text detector In ICCV R B Girshick Fast R CNN In ICCV R B Girshick J Donahue T Darrell and J Malik Rich feature hierarchies for accurate object detection and semantic segmentation In CVPR A Gupta A Vedaldi and A Zisserman Synthetic data for text localisation in natural images In CVPR W Huang Z Lin J Yang and J Wang Text localization in natural images using stroke feature transform and text co variance descriptors In ICCV W Huang Y Qiao and X Tang Robust scene text detection with convolution neural network induced MSER trees In ECCV M Jaderberg K Simonyan A Vedaldi and A Zisserman Reading text in the wild with convolutional neural networks IJCV M Jaderberg A Vedaldi and A Zisserman Deep features for text spotting In ECCV L Kang Y Li and D S Doermann Orientation robust text line detection in natural images In CVPR D Karatzas L Gomez Bigorda A Nicolaou S K Ghosh A D Bagdanov M Iwamura J Matas L Neumann V R Chandrasekhar S Lu F Shafait S Uchida and E Val veny ICDAR competition on robust reading In ICDAR D Karatzas F Shafait S Uchida M Iwamura L G i Big orda S R Mestre J Mas D F Mota J Almaza n and L de las Heras ICDAR robust reading competition In ICDAR W Liu D Anguelov D Erhan C Szegedy S E Reed C Fu and A C Berg SSD single shot multibox detector In ECCV pages J Long E Shelhamer and T Darrell Fully convolutional networks for semantic segmentation In CVPR L Neumann and J Matas Efficient scene text localization and recognition with local character refinement In ICDAR L Neumann and J Matas Real time lexicon free scene text localization and recognition PAMI J Redmon S K Divvala R B Girshick and A Farhadi You only look once Unified real time object detection CoRR abs S Ren K He R B Girshick and J Sun Faster R CNN towards real time object detection with region proposal net works In NIPS A Shrivastava A Gupta and R B Girshick Training region based object detectors with online hard example min ing In CVPR K Simonyan and A Zisserman Very deep convolu tional networks for large scale image recognition CoRR abs Z Tian W Huang T He P He and Y Qiao Detecting text in natural image with connectionist text proposal network In ECCV K Wang and S J Belongie Word spotting in the wild In ECCV T Wang D J Wu A Coates and A Y Ng End to end text recognition with convolutional neural networks In ICPR C Yao X Bai W Liu Y Ma and Z Tu Detecting texts of arbitrary orientations in natural images In CVPR C Yao X Bai N Sang X Zhou S Zhou and Z Cao Scene text detection via holistic multi channel prediction CoRR abs X Yin W Pei J Zhang and H Hao Multi orientation scene text detection with adaptive clustering PAMI X Yin X Yin K Huang and H Hao Robust text detection in natural scene images PAMI Z Zhang W Shen C Yao and X Bai Symmetry based text line detection in natural scenes In CVPR Z Zhang C Zhang W Shen C Yao W Liu and X Bai Multi oriented text detection with fully convolutional net works In CVPR '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing special characters and digits\n",
    "formatted_text = re.sub('[^a-zA-Z]', ' ', text)  \n",
    "formatted_text = re.sub(r'\\s+', ' ', formatted_text)\n",
    "\n",
    "formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Detecting Oriented Text in Natural Images by Linking Segments Baoguang Shi1 Xiang Bai1∗ Serge Belongie2 1School of EIC, Huazhong University of Science and Technology 2Department of Computer Science, Cornell Tech shibaoguang@gmail.com xbai@hust.edu.cn sjb344@cornell.edu Abstract Most state-of-the-art text detection methods are specific to horizontal Latin text and are not fast enough for real-time applications.',\n",
       " 'We introduce Segment Linking (SegLink), an oriented text detection method.',\n",
       " 'The main idea is to decom- pose text into two locally detectable elements, namely seg- ments and links.',\n",
       " 'A segment is an oriented box covering a part of a word or text line; A link connects two adjacent segments, indicating that they belong to the same word or text line.',\n",
       " 'Both elements are detected densely at multiple scales by an end-to-end trained, fully-convolutional neural network.',\n",
       " 'Final detections are produced by combining seg- ments connected by links.',\n",
       " 'Compared with previous meth- ods, SegLink improves along the dimensions of accuracy, speed, and ease of training.',\n",
       " 'It achieves an f-measure of 75.0% on the standard ICDAR 2015 Incidental (Challenge 4) benchmark, outperforming the previous best by a large margin.',\n",
       " 'It runs at over 20 FPS on 512×512 images.',\n",
       " 'More- over, without modification, SegLink is able to detect long lines of non-Latin text, such as Chinese.',\n",
       " '1.',\n",
       " 'Introduction Reading text in natural images is a challenging task un- der active research.',\n",
       " 'It is driven by many real-world appli- cations, such as Photo OCR , geo-location, and image retrieval .',\n",
       " 'In a text reading system, text detection, i.e.',\n",
       " 'localizing text with bounding boxes of words or text lines, is usually the first step of great significance.',\n",
       " 'In a sense, text detection can be seen as object detection applied to text, where words\\\\/characters\\\\/text lines are taken as the detection targets.',\n",
       " 'Owing to this, a new trend has emerged recently that state-of-the-art text detection methods [9, 6, 22, 30] are heavily based on the advanced general object detection or segmentation techniques, e.g.',\n",
       " '[4, 5, 15].',\n",
       " 'Despite the great success of the previous work, we ar- gue that the general detection methods are not well suited ∗Corresponding author.',\n",
       " '(a) (b) (c) (d) (e) (f) Figure 1.',\n",
       " 'SegLink Overview.',\n",
       " 'The upper row shows an image with two words of different scales and orientations.',\n",
       " '(a) Segments (yellow boxes) are detected on the image.',\n",
       " '(b) Links (green lines) are detected between pairs of adjacent segments.',\n",
       " '(c) Segments connected by links are combined into whole words.',\n",
       " '(d-f) SegLink is able to detect long lines of Latin and non-Latin text, such as Chinese.',\n",
       " 'for text detection, for two main reasons.',\n",
       " 'First, word\\\\/text line bounding boxes have much larger aspect ratios than those of general objects.',\n",
       " 'An (fast\\\\/faster) R-CNN [5, 4, 19]- or SSD -style detector may suffer from the difficulty of producing such boxes, owing to its proposal or anchor box design.',\n",
       " 'In addition, some non-Latin text does not have blank spaces between words, hence the even larger bound- ing box aspect ratios, which make the problem worse.',\n",
       " 'Sec- ond, unlike general objects, text usually has a clear defini- tion of orientation .',\n",
       " 'It is important for a text detector to produce oriented boxes.',\n",
       " 'However, most general object de- tection methods are not designed to produce oriented boxes.',\n",
       " 'To overcome the above challenges, we tackle the text de- tection problem in a new perspective.',\n",
       " 'We propose to de- compose long text into two smaller and locally-detectable elements, namely segment and link.',\n",
       " 'As illustrated in Fig.',\n",
       " '1, a segment is an oriented box that covers a part of a word ar X iv :1 70 3.',\n",
       " '06 52 0v 3 [ cs .C V ] 1 3 A pr 2 01 7 64 512 VGG16 through pool5 conv 4_3 Input Image (512x512) 32 1024 conv7 (fc7) 16 1024 conv 8_2 8 512 conv 9_2 conv 10_24 conv11 Combining Segments Detections 256 2 256 \\ud835\\udc59 = 1 \\ud835\\udc59 = 6\\ud835\\udc59 = 2 \\ud835\\udc59 = 3 \\ud835\\udc59 = 4 \\ud835\\udc59 = 5 3x3 conv predictors 1024,k3s1 1024,k1s1 256,k1s1 512,k3s2 128,k1s1 256,k3s2 128,k1s1 256,k3s2 256,k3s2 Figure 2.',\n",
       " 'Network Architecture.',\n",
       " 'The network consists of convolutional feature layers (shown as gray blocks) and convolutional predictors (thin gray arrows).',\n",
       " 'Convolutional filters are specified in the format of “(#filters),k(kernel size)s(stride)”.',\n",
       " 'A multi-line filter specification means a hidden layer between.',\n",
       " 'Segments (yellow boxes) and links (not displayed) are detected by convolutional predictors on multiple feature layers (indexed by l = 1 .',\n",
       " '.',\n",
       " '.',\n",
       " '6) and combined into whole words by a combining algorithm.',\n",
       " '(for clarity we use “word” here and later on, but segments also work seamlessly on text lines that comprise multiple words); A link connects a pair of adjacent segments, indi- cating that they belong to the same word.',\n",
       " 'Under the above definitions, a word is located by a number of segments with links between them.',\n",
       " 'During detection, segments and links are densely detected on an input image by a convo- lutional neural network.',\n",
       " 'Then, the segments are combined into whole words according to the links.',\n",
       " 'The key advantage of this approach is that long and ori- ented text is now detected locally since both basic elements are locally-detectable: Detecting a segment does not require the whole word to be observed.',\n",
       " 'And neither does a link since the connection of two segments can be inferred from a local context.',\n",
       " 'Thereafter, we can detect text of any length and orientation with great flexibility and efficiency.',\n",
       " 'Concretely, we propose a convolutional neural network (CNN) model to detect both segments and links simultane- ously, in a fully-convolutional manner.',\n",
       " 'The network uses VGG-16 as its backbone.',\n",
       " 'A few extra feature lay- ers are added onto it.',\n",
       " 'Convolutional predictors are added to 6 of the feature layers to detect segments and links at different scales.',\n",
       " 'To deal with redundant detections, we in- troduce two types of links, namely within-layer links and cross-layer links.',\n",
       " 'A within-layer link connects a segment to its neighbors on the same layer.',\n",
       " 'A cross-layer link, on the other hand, connects a segment to its neighbors on the lower layer.',\n",
       " 'In this way, we connect segments of adjacent locations as well as scales.',\n",
       " 'Finally, we find connected seg- ments with a depth-first search (DFS) algorithm and com- bine them into whole words.',\n",
       " 'Our main contribution is the novel segment-linking de- tection method.',\n",
       " 'Through experiments, we show that the proposed method possesses several distinctive advantages over the other state-of-the-art methods: 1) Robustness: SegLink models the structure of oriented text in a simple and elegant way, with robustness against complex back- grounds.',\n",
       " 'Our method achieves highly competitive results on standard datasets.',\n",
       " 'In particular, it outperforms the previ- ous best by a large margin in terms of f-measure (75.0% vs 64.8%) on the ICDAR 2015 Incidental (Challenge 4) bench- mark ; 2) Efficiency: SegLink is highly efficient due to its single-pass, fully-convolutional design.',\n",
       " 'It processes more than 20 images of 512x512 size per second; 3) Gener- ality: Without modification, SegLink is able to detect long lines of non-Latin text, such as Chinese.',\n",
       " 'We demonstrate this capability on a multi-lingual dataset.',\n",
       " '2.',\n",
       " 'Related Work Text Detection Over the past few years, much research effort has been devoted to the text detection problem [24, 23, 17, 17, 25, 7, 8, 30, 29, 2, 9, 6, 22, 26].',\n",
       " 'Based on the ba- sic detection targets, the previous methods can be roughly divided into three categories: character-based, word-based and line-based.',\n",
       " 'Character-based methods [17, 23, 24, 10, 7, 8] detect individual characters and group them into words.',\n",
       " 'These methods find characters by classifying candidate re- gions extracted by region extraction algorithms or by classi- fying sliding windows.',\n",
       " 'Such methods often involve a post- processing step of grouping characters into words.',\n",
       " 'Word- based methods [9, 6] directly detect word bounding boxes.',\n",
       " 'They often have a similar pipeline to the recent CNN-based general object detection networks.',\n",
       " 'Though achieving ex- cellent detection accuracies, these methods may suffer from performance drop when applied to some non-Latin text such as Chinese, as we mentioned earlier.',\n",
       " 'Line-based meth- ods [29, 30, 26] find text regions using some image seg- mentation algorithms.',\n",
       " 'They also require a sophisticated post-processing step of word partitioning and\\\\/or false pos- itive removal.',\n",
       " 'Compared with the previous approaches, our method predicts segments and links jointly in a single forward network pass.',\n",
       " 'The pipeline is much simpler and cleaner.',\n",
       " 'Moreover, the network is end-to-end trainable.',\n",
       " 'Our method is similar in spirit to a recent work , which detects text lines by finding and grouping a sequence of fine-scale text proposals through a CNN coupled with recurrent neural layers.',\n",
       " 'In contrast, we detect oriented seg- ments only using convolutional layers, yielding better flexi- bility and faster speed.',\n",
       " 'Also, we detect links explicitly using the same strong CNN features for segments, improving the robustness.',\n",
       " 'Object Detection Text detection can be seen as a partic- ular instance of general object detection, which is a funda- mental problem in computer vision.',\n",
       " 'Most state-of-the-art detection systems either classify some class-agnostic ob- ject proposals with CNN [5, 4, 19] or directly regress ob- ject bounding boxes from a set of preset boxes (e.g.',\n",
       " 'anchor boxes) [18, 14].',\n",
       " 'The architecture of our network inherits that of SSD , a recent object detection model.',\n",
       " 'SSD proposed the idea of detecting objects on multiple feature layers with convolu- tional predictors.',\n",
       " 'Our model also detects segments and links in a very similar way.',\n",
       " 'Despite the model similarity, our de- tection strategy is drastically different: SSD directly out- puts object bounding boxes.',\n",
       " 'We, on the other hand, adopt a bottom-up approach by detecting the two comprising ele- ments of a word or text line and combine them together.',\n",
       " '3.',\n",
       " 'Segment Linking Our method detects text with a feed-forward CNN model.',\n",
       " 'Given an input image I of size wI × hI , the model outputs a fixed number of segments and links, which are then filtered by their confidence scores and combined into whole word bounding boxes.',\n",
       " 'A bounding box is a ro- tated rectangle denoted by b = (xb, yb, wb, hb, θb), where xb, yb are the coordinates of the center, wb, hb the width and height, and θb the rotation angle.',\n",
       " '3.1.',\n",
       " 'CNN Model Fig.',\n",
       " '2 shows the network architecture.',\n",
       " 'Our network uses a pretrained VGG-16 network as its backbone (conv1 through pool5).',\n",
       " 'Following , the fully-connected layers of VGG-16 are converted into convolutional layers (fc6 to conv6; fc7 to conv7).',\n",
       " 'They are followed by a few extra con- volutional layers (conv8 1 to conv11), which extract even deeper features with larger receptive fields.',\n",
       " 'Their configu- rations are specified in Fig.',\n",
       " '2.',\n",
       " 'Segments and links are detected on 6 of the feature lay- ers, which are conv4 3, conv7, conv8 2, conv9 2, conv10 2, and conv11.',\n",
       " 'These feature layers provide high-quality deep features of different granularity (conv4 3 the finest and conv11 the coarsest).',\n",
       " 'A convolutional predictor with 3 × 3 kernels is added to each of the 6 layers to detect segments and links.',\n",
       " 'We index the feature layers and the predictors by l = 1, .',\n",
       " '.',\n",
       " '.',\n",
       " ', 6.',\n",
       " 'Segment Detection Segments are also oriented boxes, de- noted by s = (xs, ys, ws, hs, θs).',\n",
       " 'We detect segments by estimating the confidence scores and geometric offsets to a set of default boxes on the input image.',\n",
       " 'Each default box is associated with a feature map location, and its score and offsets are predicted from the features at that location.',\n",
       " 'For simplicity, we only associate one default box with a fea- ture map location.',\n",
       " 'Consider the l-th feature layer whose feature map size is wl × hl.',\n",
       " 'A location (x, y) on this map corresponds to a default box centered at (xa, ya) on the image, where xa = wI wl (x+ 0.5); ya = hI hl (y + 0.5) (1) The width and height of the default box are both set to a constant al.',\n",
       " 'The convolutional predictor produces 7 channels for segment detection.',\n",
       " 'Among them, 2 channels are further softmax-normalized to get the segment score in (0, 1).',\n",
       " 'The rest 5 are the geometric offsets.',\n",
       " 'Considering a location (x, y) on the map, we denote the vector at this location along the depth by (∆xs, ∆ys, ∆ws, ∆hs, ∆θs).',\n",
       " 'Then, the segment at this location is calculated by: xs = al∆xs + xa (2) ys = al∆ys + ya (3) ws = al exp(∆ws) (4) hs = al exp(∆hs) (5) θs = ∆θs (6) Here, the constant al controls the scale of the output seg- ments.',\n",
       " 'It should be chosen with regard to the receptive field size of the l-th layer.',\n",
       " 'We use an empirical equation for choosing this size: al = γ wIwl , where γ = 1.5.',\n",
       " 'Within-Layer Link Detection A link connects a pair of adjacent segments, indicating that they belong to the same word.',\n",
       " 'Here, adjacent segments are those detected at adja- cent feature map locations.',\n",
       " 'Links are not only necessary for combining segments into whole words but also helpful for separating two nearby words – between two nearby words, the links should be predicted as negative.',\n",
       " '(a) Within- Layer Links (b) Cross- Layer Links 2x size conv8_2 conv8_2 conv9_2 16 8 16 Figure 3.',\n",
       " 'Within-Layer and Cross-Layer Links.',\n",
       " '(a) A location on conv8 2 (yellow block) and its 8-connected neighbors (blue blocks with and without fill).',\n",
       " 'The detected within-layer links (green lines) connect a segment (yellow box) and its two neigh- boring segments (blue boxes) on the same layer.',\n",
       " '(b) The cross- layer links connect a segment on conv9 2 (yellow box) and two segments on conv8 2 (blue boxes).',\n",
       " 'We explicitly detect links between segments using the same features for detecting segments.',\n",
       " 'Since we detect only one segment at a feature map location, segments can be indexed by their map locations (x, y) and layer indexes l, denoted by s(x,y,l).',\n",
       " 'As illustrated in Fig.',\n",
       " '3.a, we define the within-layer neighbors of a segment as its 8-connected neighbors on the same feature layer: Nws(x,y,l) = {s (x′,y′,l)}x−1≤x′≤x+1,y−1≤y′≤y+1 \\\\ s(x,y,l) (7) As segments are detected locally, a pair of neighboring seg- ments are also adjacent on input image.',\n",
       " 'Links are also de- tected by the convolutional predictors.',\n",
       " 'A predictor outputs 16 channels for the links to the 8-connected neighboring segments.',\n",
       " 'Every 2 channels are softmax-normalized to get the score of a link.',\n",
       " 'Cross-Layer Link Detection In our network, segments are detected at different scales on different feature layers.',\n",
       " 'Each layer handles a range of scales.',\n",
       " 'We make these ranges overlap in order not to miss scales at their edges.',\n",
       " 'But as a result, segments of the same word could be detected on multiple layers at the same time, producing redundancies.',\n",
       " 'To address this problem, we further propose another type of links, called cross-layer links.',\n",
       " 'A cross-layer link connects segments on two feature layers with adjacent in- dexes.',\n",
       " 'For example, cross-layer links are detected between conv4 3 and conv7, because their indexes are l = 1 and l = 2 respectively.',\n",
       " 'An important property of such a pair is that the first layer always has twice the size as the second one, because of the down-sampling layer (max-pooling or stride-2 convolution) between them.',\n",
       " 'Note that this property only holds when all feature layers have even-numbered sizes.',\n",
       " 'In practice, we ensured this property by having the width and height of the input image both dividable by 128.',\n",
       " 'For example, an 1000× 800 image is resized to 1024 × 768, which is the nearest valid size.',\n",
       " 'As illustrated in Fig.',\n",
       " '3.b, we define the cross-layer neighbors of a segment as (8)N cs(x,y,l) = {s (x′,y′,l−1)}2x≤x′≤2x+1,2y≤y′≤2y+1, which are the segments on the preceeding layer.',\n",
       " 'Every seg- ment has 4 cross-layer neighbors.',\n",
       " 'The correspondence is ensured by the double-size relationship between the two layers.',\n",
       " 'Again, cross-layer links are detected by the convolu- tional predictor.',\n",
       " 'The predictor outputs 8 channels for cross- layer links.',\n",
       " 'Every 2 channels are softmax-normalized to produce the score of a cross-layer link.',\n",
       " 'Cross-layer links are detected on feature layer l = 2 .',\n",
       " '.',\n",
       " '.',\n",
       " '6, but not on l = 1 (conv4 3) since it has no preceeding feature layer.',\n",
       " 'With cross-layer links, segments of different scales can be connected and later combined.',\n",
       " 'Compared with the tradi- tional non-maximum suppression, cross-layer linking pro- vides a trainable way of joining redundancies.',\n",
       " 'Besides, it fits seamlessly into our linking strategy and is easy to im- plement under our framework.',\n",
       " 'segment scores segment offsets within-layer link scores cross-layer link scores 2 5 16 8 \\ud835\\udc64\" ℎ\" Figure 4.',\n",
       " 'Output channels of a convolutional predictor.',\n",
       " 'The block shows a wl × hl map of depth 31.',\n",
       " 'The predictor of l = 1 does not output the channels for corss-layer links.',\n",
       " 'Outputs of a Convolutional Predictor Putting things to- gether, Fig.',\n",
       " '4 shows the output channels of a convolutional predictor.',\n",
       " 'A predictor is implemented by a convolutional layer followed by some softmax layers that normalize the segment and link scores respectively.',\n",
       " 'Thereafter, all lay- ers in our network are convolutional layers.',\n",
       " 'Our network is fully-convolutional.',\n",
       " '3.2.',\n",
       " 'Combining Segments with Links After feed-forwarding, the network produces a number of segments and links (the number depends on the image size).',\n",
       " 'Before combination, the output segments and links are filtered by their confidence scores.',\n",
       " 'We set different fil- tering thresholds for segment and link, respectively α and β. Empirically, the performance of our model is not very sensi- tive to these thresholds.',\n",
       " 'A 0.1 deviation on either thresholds from their optimal values results in less than 1% f-measure drop.',\n",
       " 'Taking the filtered segments as nodes and the filtered links as edges, we construct a graph over them.',\n",
       " 'Then, a depth-first search (DFS) is performed over the graph to find its connected components.',\n",
       " 'Each component contains a set of segments that are connected by links.',\n",
       " 'Denoting a con- nected component by B, segments within this component are combined following the procedures in Alg.',\n",
       " '1.',\n",
       " 'Algorithm 1 Combining Segments 1: Input: B = {s(i)}|B|i=1 is a set of segments connected by links, where s(i) = (x(i)s , y (i) s , w (i) s , h (i) s , θ (i) s ).',\n",
       " '2: Find the average angle θb := 1|B| ∑ B θ (i) s .',\n",
       " '3: For a straight line (tan θb)x + b, find the b that min- imizes the sum of distances to all segment centers (x (i) s , y (i) s ).',\n",
       " '4: Find the perpendicular projections of all segment cen- ters onto the straight line.',\n",
       " '5: From the projected points, find the two with the longest distance.',\n",
       " 'Denote them by (xp, yp) and (xq, yq).',\n",
       " '6: xb := 1 2 (xp + xq) 7: yb := 1 2 (yp + yq) 8: wb := √ (xp − xq)2 + (yp − yq)2 + 12 (wp + wq) 9: hb := 1 |B| ∑ B h (i) s 10: b := (xb, yb, wb, hb, θb) 11: Output: b is the combined bounding box.',\n",
       " '4.',\n",
       " 'Training 4.1.',\n",
       " 'Groundtruths of Segments and Links The network is trained by the direct supervision of groundtruth segments and links.',\n",
       " 'The groundtruths include the labels of all default boxes (i.e.',\n",
       " 'the label of their corre- sponding segments), their offsets to the default boxes, and the labels of all within- and cross-layer links.',\n",
       " 'We calculate them from the groundtruth word bounding boxes.',\n",
       " 'First, we assume that there is only one groundtruth word on the input image.',\n",
       " 'A default box is labeled as positive iff 1) the center of the box is inside the word bounding box; 2) the ratio between the box size al and the word height h satisfies: max( al h , h al ) ≤ 1.5 (9) Otherwise, the default box is labeled as negative.',\n",
       " 'Next, we consider the case of multiple words.',\n",
       " 'A default box is labeled as negative if it does not meet the above- mentioned criteria for any word.',\n",
       " 'Otherwise, it is labeled as positive and matched to the word that has the closest size, i.e.',\n",
       " 'the one with the minimal value at the left-hand side of Eq.',\n",
       " '9. word bounding box default box box center \\ud835\\udf03 (1) Default box, word bounding box, and the center of the default box (blue dot) (2) Rotate word clockwise by \\ud835\\udf03 along the center of the default box (3) Crop word bounding box to remove the parts to the left and right of the default box (4) Rotate the cropped box anticlockwise by \\ud835\\udf03 along the center of the default box \\ud835\\udc4e ℎ \\ud835\\udc64% groundtruth segment ℎ% \\ud835\\udc65%, \\ud835\\udc66% Figure 5.',\n",
       " 'The steps of calculating a groundtruth segment given a default box and a word bounding box.',\n",
       " 'Offsets are calculated on positive default boxes.',\n",
       " 'First, we calculate the groundtruth segments following the steps illustrated in Fig.',\n",
       " '5.',\n",
       " 'Then, we solve Eq.',\n",
       " '2 to Eq.',\n",
       " '6 to get the groundtruth offsets.',\n",
       " 'A link (either within-layer or cross-layer) is labeled as positive iff 1) both of the default boxes connected to it are labeled as positive; 2) the two default boxes are matched to the same word.',\n",
       " '4.2.',\n",
       " 'Optimization Objective Our network model is trained by simultane- ously minimizing the losses on segment classification, off- sets regression, and link classification.',\n",
       " 'Overall, the loss function is a weighted sum of the three losses: L(ys, cs,yl, cl, ŝ, s) = 1 Ns Lconf(ys, cs)+λ1 1 Ns Lloc(ŝ, s) + λ2 1 Nl Lconf(yl, cl) (10) Here, ys is the labels of all segments.',\n",
       " 'y (i) s = 1 if the i-th default box is labeled as positive, and 0 otherwise.',\n",
       " 'Like- wise, yl is the labels of the links.',\n",
       " 'Lconf is the softmax loss over the predicted segment and link scores, respectively cs and cl.',\n",
       " 'Lloc is the Smooth L1 regression loss over the predicted segment geometries ŝ and the groundtruth s. The losses on segment classification and regression are normal- ized by Ns, which is the number of positive default boxes.',\n",
       " 'The loss on link classification is normalized by the number of positive links Nl.',\n",
       " 'The weight constants λ1 and λ2 are both set to 1 in practice.',\n",
       " 'Online Hard Negative Mining For both segments and links, negatives take up most of the training samples.',\n",
       " 'There- fore, hard negative mining is necessary for balancing the positive and negative samples.',\n",
       " 'We follow the online hard negative mining strategy proposed in to keep the ra- tio between the negatives and positives 3:1 at most.',\n",
       " 'Hard negative mining is performed separately for segments and links.',\n",
       " 'Data Augmentation We adopt an online augmentation pipeline that is similar to that of SSD and YOLO .',\n",
       " 'Training images are randomly cropped to a patch that has a minimum Jaccard overlap of o with any groundtruth word Crops are resized to the same size before loaded into a batch.',\n",
       " 'For oriented text, the augmentation is performed on the axis-aligned bounding boxes of the words.',\n",
       " 'The overlap o is randomly chosen from 0 (no constraint), 0.1, 0.3, 0.5, 0.7, and 0.9 for every sample.',\n",
       " 'The crop size is randomly chosen from [0.1, 1] of the original image size.',\n",
       " 'Training images are not horizontally flipped.',\n",
       " '5.',\n",
       " 'Experiments In this section, we evaluate the proposed method on three public datasets, namely ICDAR 2015 Incidental Text (Chal- lenge 4), MSRA-TD500, and ICDAR 2013, using the stan- dard evaluation protocol of each.',\n",
       " '5.1.',\n",
       " 'Datasets SynthText in the Wild (SynthText) contains 800,000 synthetic training images.',\n",
       " 'They are created by blending nat- ural images with text rendered with random fonts, size, ori- entation, and color.',\n",
       " 'Text is rendered and aligned to care- fully chosen image regions in order have a realistic look.',\n",
       " 'The dataset provides very detailed annotations for charac- ters, words, and text lines.',\n",
       " 'We only use the dataset for pre- training our network.',\n",
       " 'ICDAR 2015 Incidental Text (IC15) is the Chal- lenge 4 of the ICDAR 2015 Robust Reading Competition.',\n",
       " 'This challenge features incidental scene text images taken by Google Glasses without taking care of positioning, im- age quality, and viewpoint.',\n",
       " 'Consequently, the dataset ex- hibits large variations in text orientation, scale, and res- olution, making it much more difficult than previous IC- DAR challenges.',\n",
       " 'The dataset contains 1000 training images and 500 testing images.',\n",
       " 'Annotations are provided as word quadrilaterals.',\n",
       " 'MSRA-TD500 (TD500) is the first standard dataset that focuses on oriented text.',\n",
       " 'The dataset is also multi- lingual, including both Chinese and English text.',\n",
       " 'The dataset consists of 300 training images and 200 testing im- ages.',\n",
       " 'Different from IC15, TD500 is annotated at the level of text lines.',\n",
       " 'ICDAR 2013 (IC13) contains mostly horizontal text, with some text slightly oriented.',\n",
       " 'The dataset has been widely adopted for evaluating text detection methods pre- viously.',\n",
       " 'It consists of 229 training images and 233 testing images.',\n",
       " '5.2.',\n",
       " 'Implementation Details Our network is pre-trained on SynthText and finetuned on real datasets (specified later).',\n",
       " 'It is optimized by the stan- dard SGD algorithm with a momentum of 0.9.',\n",
       " 'For both pre- training and finetuning, images are resized to 384×384 after random cropping.',\n",
       " 'Since our model is fully-convolutional, we can train it on a certain size and apply it to other sizes during testing.',\n",
       " 'Batch size is set to 32.',\n",
       " 'In pretraining, the learning is set to 10−3 for the first 60k iterations, then de- cayed to 10−4 for the rest 30k iterations.',\n",
       " 'During finetuning, the learning rate is fixed to 10−4 for 5-10k iterations.',\n",
       " 'The number of finetuning iterations depends on the size of the dataset.',\n",
       " 'Due to the precision-recall tradeoff and the difference between evaluation protocols across datasets, we choose the best thresholds α and β to optimize f-measure.',\n",
       " 'Except for IC15, the thresholds are chosen separately on different datasets via a grid search with 0.1 step on a hold-out vali- dation set.',\n",
       " 'IC15 does not offer an offline evaluation script, so the only way for us is to submit multiple results to the evaluation server.',\n",
       " 'Our method is implemented using TensorFlow r0.11.',\n",
       " 'All the experiments are carried out on a workstation with an Intel Xeon 8-core CPU (2.8 GHz), 4 Titan X Graphics Cards, and 64GB RAM.',\n",
       " 'Running on 4 GPUs in parallel, training a batch takes about 0.5s.',\n",
       " 'The whole training pro- cess takes less than a day.',\n",
       " '5.3.',\n",
       " 'Detecting Oriented English Text First, we evaluate SegLink on IC15.',\n",
       " 'The pretrained model is finetuned for 10k iterations on the training dataset of IC15.',\n",
       " 'Testing images are resized to 1280 × 768.',\n",
       " 'We set the thresholds on segments and links to 0.9 and 0.7, respec- tively.',\n",
       " 'Performance is evaluated by the official central sub- mission server (http:\\\\/\\\\/rrc.cvc.uab.es\\\\/?ch=4).',\n",
       " 'In order to meet the requirements on submission format, the output oriented rectangles are converted into quadrilaterals.',\n",
       " 'Table 1 lists and compares the results of the proposed method and other state-of-the-art methods.',\n",
       " 'Some results are obtained from the online leaderboard.',\n",
       " 'SegLink outperforms the others by a large margin.',\n",
       " 'In terms of f-measure, it out- performs the second best by 10.2%.',\n",
       " 'Considering that some methods have close or even higher precision than SegLink, http:\\\\/\\\\/rrc.cvc.uab.es\\\\/?ch=4 Recall=1.0 Precision=0.86 F-Score=0.92 Recall=1.0 Precision=1.0 F-Score=1.0 Recall=1.0 Precision=1.0 F-Score=1.0 Recall=0.88 Precision=0.88 F-Score=0.88 Recall=1.0 Precision=0.88 F-Score=0.93 Recall=1.0 Precision=1.0 F-Score=1.0 Figure 6.',\n",
       " 'Example Results on IC15.',\n",
       " 'Green regions are correctly detected text regions.',\n",
       " 'Red ones are either false positive or false negative.',\n",
       " 'Gray ones are detected but neglected by the evaluation algorithm.',\n",
       " 'Visualizations are generated by the central submission system.',\n",
       " 'Yellow frames contain zoom-in image regions.',\n",
       " 'Table 1.',\n",
       " 'Results on ICDAR 2015 Incidental Text Method Precision Recall F-measure HUST MCLAB 47.5 34.8 40.2 NJU Text 72.7 35.8 48.0 StradVision-2 77.5 36.7 49.8 MCLAB FCN 70.8 43.0 53.6 CTPN 51.6 74.2 60.9 Megvii-Image++ 72.4 57.0 63.8 Yao et al.',\n",
       " '72.3 58.7 64.8 SegLink 73.1 76.8 75.0 the improvement mainly comes from the recall.',\n",
       " 'As shown in Fig.',\n",
       " '6, our method is able to distinguish text from very cluttered backgrounds.',\n",
       " 'In addition, owing to its explicit link prediction, SegLink correctly separates words that are very close to each other.',\n",
       " '5.4.',\n",
       " 'Detecting Multi-Lingual Text in Long Lines We further demonstrate the ability of SegLink to detect long text in non-Latin scripts.',\n",
       " 'TD500 is taken as the dataset for this experiment, as it consists of oriented and multi- lingual text.',\n",
       " 'The training set of TD500 only has 300 im- ages, which are not enough for finetuning our model.',\n",
       " 'We mix the training set of TD500 with the training set of IC15, in the way that every batch has half of its images coming from each dataset.',\n",
       " 'The pretrained model is finetuned for 8k iterations.',\n",
       " 'The testing images are resized to 768×768.',\n",
       " 'The thresholds α and β are set to 0.9 and 0.5 respectively.',\n",
       " 'Per- formance scores are calculated by the official development toolkit.',\n",
       " 'According to Table 2, SegLink achieves the highest scores in terms of precision and f-measure.',\n",
       " 'Benefiting from its fully-convolutional design, SegLink runs at 8.9 FPS, a Table 2.',\n",
       " 'Results on MSRA-TD500 Method Precision Recall F-measure FPS Kang et al.',\n",
       " '71 62 66 - Yao et al.',\n",
       " '63 63 60 0.14 Yin et al.',\n",
       " '81 63 74 0.71 Yin et al.',\n",
       " '71 61 65 1.25 Zhang et al.',\n",
       " '83 67 74 0.48 Yao et al.',\n",
       " '77 75 76 ∼1.61 SegLink 86 70 77 8.9 much faster speed than the others.',\n",
       " 'SegLink also enjoys sim- plicity.',\n",
       " 'The inference process of SegLink is a single forward pass in the detection network, while the previous methods [25, 28, 30] involve sophisticated rule-based grouping or fil- tering steps.',\n",
       " 'TD500 contains many long lines of text in mixed lan- guages (English and Chinese).',\n",
       " 'Fig.',\n",
       " '7 shows how SegLink handles such text.',\n",
       " 'As can be seen, segments and links are densely detected along text lines.',\n",
       " 'They result in long bound- ing boxes that are hard to obtain from a conventional ob- ject detector.',\n",
       " 'Despite the large difference in appearance be- tween English and Chinese text, SegLink is able to handle them simultaneously without any modifications in its struc- ture.',\n",
       " '5.5.',\n",
       " 'Detecting Horizontal Text Lastly, we evaluate the performance of SegLink on horizontal-text datasets.',\n",
       " 'The pretrained model is finetuned for 5k iterations on the combined training sets of IC13 and IC15.',\n",
       " 'Since the most text in IC13 has relatively larger sizes, the testing images are resized to 512× 512.',\n",
       " 'The thresholds α and β are set to 0.6 and 0.3, respectively.',\n",
       " 'To match the submission format, we convert the detected oriented boxes Se gm en ts an d Lin ks Co m bi ne d Figure 7.',\n",
       " 'Example Results on TD500.',\n",
       " 'The first row shows the detected segments and links.',\n",
       " 'The within-layer and cross-layer links are visualized as red and green lines, respectively.',\n",
       " 'Segments are shown as rectangles in different colors, denoting different connected components.',\n",
       " 'The second row shows the combined boxes.',\n",
       " 'into their axis-aligned bounding boxes.',\n",
       " 'Table 3 compares SegLink with other state-of-the-art methods.',\n",
       " 'The scores are calculated by the central sub- mission system using the “Deteval” evaluation protocol.',\n",
       " 'SegLink achieves very competitive results in terms of f- measure.',\n",
       " 'Only one approach outperforms SegLink in terms of f-measure.',\n",
       " 'However, is mainly designed for detecting horizontal text and is not well-suited for oriented text.',\n",
       " 'In terms of speed, SegLink runs at over 20 FPS on 512× 512 images, much faster than the other methods.',\n",
       " 'Table 3.',\n",
       " 'Results on IC13.',\n",
       " 'P, R, F stand for precision, recall and f-measure respectively.',\n",
       " '*These methods are only evaluated under the “ICDAR 2013” evaluation protocol, the rest under “Deteval”.',\n",
       " 'The two protocols usually yield very close scores.',\n",
       " 'Method P R F FPS Neumann et al.',\n",
       " '∗ 81.8 72.4 77.1 3 Neumann et al.',\n",
       " '∗ 82.1 71.3 76.3 3 Busta et al.',\n",
       " '∗ 84.0 69.3 76.8 6 Zhang et al.',\n",
       " '88 74 80 <0.1 Zhang et al.',\n",
       " '88 78 83 <1 Jaderberg et al.',\n",
       " '88.5 67.8 76.8 <1 Gupta et al.',\n",
       " '92.0 75.5 83.0 15 Tian et al.',\n",
       " '93.0 83.0 87.7 7.1 SegLink 87.7 83.0 85.3 20.6 5.6.',\n",
       " 'Limitations A major limitation of SegLink is that two thresholds, α and β, need to be set manually.',\n",
       " 'In practice, the optimal values of the thresholds are found by a grid search.',\n",
       " 'Sim- plifying the parameters would be part of our future work.',\n",
       " 'Another weakness is that SegLink fails to detect text that has very large character spacing.',\n",
       " 'Fig.',\n",
       " '8.a,b show two such cases.',\n",
       " 'The detected links connect adjacent segments but fail to link distant segments.',\n",
       " '(a) (b) (c) Figure 8.',\n",
       " 'Failure cases on TD500.',\n",
       " 'Red boxes are false posi- tives.',\n",
       " '(a)(b) SegLink fails to link the characters with large char- acter spacing.',\n",
       " '(c) SegLink fails to detect curved text.',\n",
       " 'Fig.',\n",
       " '8.c shows that SegLink fails to detect text of curved shape.',\n",
       " 'However, we believe that this is not a limitation of the segment linking strategy, but the segment combination algorithm, which can only produce rectangles currently.',\n",
       " '6.',\n",
       " 'Conclusion We have presented SegLink, a novel text detection strat- egy implemented by a simple and highly-efficient CNN model.',\n",
       " 'The superior performance on horizontal, ori- ented, and multi-lingual text datasets well demonstrate that SegLink is accurate, fast, and flexible.',\n",
       " 'In the future, we will further explore its potentials on detecting deformed text such as curved text.',\n",
       " 'Also, we are interested in extending SegLink into a end-to-end recognition system.',\n",
       " 'Acknowledgment This work was supported in part by National Natural Science Foundation of China (61222308 and 61573160), a Google Focused Research Award, AWS Cloud Credits for Research, a Microsoft Research Award and a Facebook equipment donation.',\n",
       " 'The authors also thank China Scholar- ship Council (CSC) for supporting this work.',\n",
       " 'References M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J.',\n",
       " 'Dean, M. Devin, S. Ghe- mawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mané, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viégas, O. Vinyals, P. War- den, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng.',\n",
       " 'Tensor- Flow: Large-scale machine learning on heterogeneous sys- tems, 2015.',\n",
       " 'Software available from tensorflow.org.',\n",
       " '6 A. Bissacco, M. Cummins, Y. Netzer, and H. Neven.',\n",
       " 'Pho- toocr: Reading text in uncontrolled conditions.',\n",
       " 'In ICCV, 2013.',\n",
       " '1, 2 M. Busta, L. Neumann, and J. Matas.',\n",
       " 'Fastext: Efficient un- constrained scene text detector.',\n",
       " 'In ICCV, 2015.',\n",
       " '8 R. B. Girshick.',\n",
       " 'Fast R-CNN.',\n",
       " 'In ICCV, 2015.',\n",
       " '1, 3, 5 R. B. Girshick, J. Donahue, T. Darrell, and J. Malik.',\n",
       " 'Rich feature hierarchies for accurate object detection and semantic segmentation.',\n",
       " 'In CVPR, 2014.',\n",
       " '1, 3 A. Gupta, A. Vedaldi, and A. Zisserman.',\n",
       " 'Synthetic data for text localisation in natural images.',\n",
       " 'In CVPR, 2016.',\n",
       " '1, 2, 6, 8 W. Huang, Z. Lin, J. Yang, and J. Wang.',\n",
       " 'Text localization in natural images using stroke feature transform and text co- variance descriptors.',\n",
       " 'In ICCV, 2013.',\n",
       " '2 W. Huang, Y. Qiao, and X. Tang.',\n",
       " 'Robust scene text detection with convolution neural network induced MSER trees.',\n",
       " 'In ECCV, 2014.',\n",
       " '2 M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman.',\n",
       " 'Reading text in the wild with convolutional neural networks.',\n",
       " 'IJCV, 116(1):1–20, 2016.',\n",
       " '1, 2, 8 M. Jaderberg, A. Vedaldi, and A. Zisserman.',\n",
       " 'Deep features for text spotting.',\n",
       " 'In ECCV, 2014.',\n",
       " '2 L. Kang, Y. Li, and D. S. Doermann.',\n",
       " 'Orientation robust text line detection in natural images.',\n",
       " 'In CVPR, 2014.',\n",
       " '7 D. Karatzas, L. Gomez-Bigorda, A. Nicolaou, S. K. Ghosh, A. D. Bagdanov, M. Iwamura, J. Matas, L. Neumann, V. R. Chandrasekhar, S. Lu, F. Shafait, S. Uchida, and E. Val- veny.',\n",
       " 'ICDAR 2015 competition on robust reading.',\n",
       " 'In ICDAR 2015, 2015.',\n",
       " '2, 6 D. Karatzas, F. Shafait, S. Uchida, M. Iwamura, L. G. i Big- orda, S. R. Mestre, J. Mas, D. F. Mota, J. Almazán, and L. de las Heras.',\n",
       " 'ICDAR 2013 robust reading competition.',\n",
       " 'In ICDAR 2013, 2013.',\n",
       " '6 W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. E. Reed, C. Fu, and A. C. Berg.',\n",
       " 'SSD: single shot multibox detector.',\n",
       " 'In ECCV, pages 21–37, 2016.',\n",
       " '1, 3, 6 J.',\n",
       " 'Long, E. Shelhamer, and T. Darrell.',\n",
       " 'Fully convolutional networks for semantic segmentation.',\n",
       " 'In CVPR, 2015.',\n",
       " '1 L. Neumann and J. Matas.',\n",
       " 'Efficient scene text localization and recognition with local character refinement.',\n",
       " 'In ICDAR, 2015.',\n",
       " '8 L. Neumann and J. Matas.',\n",
       " 'Real-time lexicon-free scene text localization and recognition.',\n",
       " 'PAMI, 38(9):1872–1885, 2016.',\n",
       " '2, 8 J. Redmon, S. K. Divvala, R. B. Girshick, and A. Farhadi.',\n",
       " 'You only look once: Unified, real-time object detection.',\n",
       " 'CoRR, abs\\\\/1506.02640, 2015.',\n",
       " '3, 6 S. Ren, K. He, R. B. Girshick, and J.',\n",
       " 'Sun.',\n",
       " 'Faster R-CNN: towards real-time object detection with region proposal net- works.',\n",
       " 'In NIPS, 2015.',\n",
       " '1, 3 A. Shrivastava, A. Gupta, and R. B. Girshick.',\n",
       " 'Training region-based object detectors with online hard example min- ing.',\n",
       " 'In CVPR, 2016.',\n",
       " '6 K. Simonyan and A. Zisserman.',\n",
       " 'Very deep convolu- tional networks for large-scale image recognition.',\n",
       " 'CoRR, abs\\\\/1409.1556, 2014.',\n",
       " '2, 3 Z. Tian, W. Huang, T. He, P. He, and Y. Qiao.',\n",
       " 'Detecting text in natural image with connectionist text proposal network.',\n",
       " 'In ECCV, 2016.',\n",
       " '1, 2, 3, 7, 8 K. Wang and S. J. Belongie.',\n",
       " 'Word spotting in the wild.',\n",
       " 'In ECCV, 2010.',\n",
       " '2 T. Wang, D. J. Wu, A. Coates, and A. Y. Ng.',\n",
       " 'End-to-end text recognition with convolutional neural networks.',\n",
       " 'In ICPR, 2012.',\n",
       " '2 C. Yao, X. Bai, W. Liu, Y. Ma, and Z. Tu.',\n",
       " 'Detecting texts of arbitrary orientations in natural images.',\n",
       " 'In CVPR, 2012.',\n",
       " '1, 2, 6, 7 C. Yao, X. Bai, N. Sang, X. Zhou, S. Zhou, and Z. Cao.',\n",
       " 'Scene text detection via holistic, multi-channel prediction.',\n",
       " 'CoRR, abs\\\\/1606.09002, 2016.',\n",
       " '2, 3, 7 X. Yin, W. Pei, J. Zhang, and H. Hao.',\n",
       " 'Multi-orientation scene text detection with adaptive clustering.',\n",
       " 'PAMI, 37(9):1930–1937, 2015.',\n",
       " '7 X. Yin, X. Yin, K. Huang, and H. Hao.',\n",
       " 'Robust text detection in natural scene images.',\n",
       " 'PAMI, 36(5):970–983, 2014.',\n",
       " '7 Z. Zhang, W. Shen, C. Yao, and X. Bai.',\n",
       " 'Symmetry-based text line detection in natural scenes.',\n",
       " 'In CVPR, 2015.',\n",
       " '2, 3, 8 Z. Zhang, C. Zhang, W. Shen, C. Yao, W. Liu, and X. Bai.',\n",
       " 'Multi-oriented text detection with fully convolutional net- works.',\n",
       " 'In CVPR, 2016.',\n",
       " '1, 2, 3, 7, 8']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "sentence_list = nltk.sent_tokenize(text)\n",
    "\n",
    "sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "word_frequencies = {}  \n",
    "for word in nltk.word_tokenize(formatted_text):  \n",
    "    if word not in stopwords:\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "for word in word_frequencies.keys():  \n",
    "    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_scores = {}  \n",
    "for sent in sentence_list:  \n",
    "    for word in nltk.word_tokenize(sent.lower()):\n",
    "        if word in word_frequencies.keys():\n",
    "            if len(sent.split(' ')) < 30:\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \n",
    "# summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "# summary = ' '.join(summary_sentences)  \n",
    "# print(summary) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a sense, text detection can be seen as object detection applied to text, where words\\/characters\\/text lines are taken as the detection targets. The detected within-layer links (green lines) connect a segment (yellow box) and its two neigh- boring segments (blue boxes) on the same layer. Since we detect only one segment at a feature map location, segments can be indexed by their map locations (x, y) and layer indexes l, denoted by s(x,y,l). (b) The cross- layer links connect a segment on conv9 2 (yellow box) and two segments on conv8 2 (blue boxes). Combining Segments with Links After feed-forwarding, the network produces a number of segments and links (the number depends on the image size). localizing text with bounding boxes of words or text lines, is usually the first step of great significance. Segments (yellow boxes) and links (not displayed) are detected by convolutional predictors on multiple feature layers (indexed by l = 1 . Text localization in natural images using stroke feature transform and text co- variance descriptors. Groundtruths of Segments and Links The network is trained by the direct supervision of groundtruth segments and links. Detecting Multi-Lingual Text in Long Lines We further demonstrate the ability of SegLink to detect long text in non-Latin scripts. As can be seen, segments and links are densely detected along text lines. Object Detection Text detection can be seen as a partic- ular instance of general object detection, which is a funda- mental problem in computer vision. Detecting text in natural image with connectionist text proposal network. Cross-Layer Link Detection In our network, segments are detected at different scales on different feature layers.\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "summary_sentences2 = heapq.nlargest(14, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "summary2 = ' '.join(summary_sentences2)  \n",
    "print(summary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pwcode_scraping",
   "language": "python",
   "name": "pwcode_scraping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
