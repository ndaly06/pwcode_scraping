{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tika import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>extradata</th>\n",
       "      <th>global_rank</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>metric_value</th>\n",
       "      <th>model</th>\n",
       "      <th>remove</th>\n",
       "      <th>task</th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_path</th>\n",
       "      <th>paper_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IC15</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 10</td>\n",
       "      <td>F-Measure</td>\n",
       "      <td>75.61%</td>\n",
       "      <td>SegLink</td>\n",
       "      <td>-</td>\n",
       "      <td>Scene Text Detection</td>\n",
       "      <td>Detecting Oriented Text in Natural Images by L...</td>\n",
       "      <td>/paper/detecting-oriented-text-in-natural-imag...</td>\n",
       "      <td>https://arxiv.org/pdf/1703.06520v3.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SCUT-CTW1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 5</td>\n",
       "      <td>F-Measure</td>\n",
       "      <td>40.8%</td>\n",
       "      <td>SegLink</td>\n",
       "      <td>-</td>\n",
       "      <td>Curved Text Detection</td>\n",
       "      <td>Detecting Oriented Text in Natural Images by L...</td>\n",
       "      <td>/paper/detecting-oriented-text-in-natural-imag...</td>\n",
       "      <td>https://arxiv.org/pdf/1703.06520v3.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 36</td>\n",
       "      <td>% Test Accuracy</td>\n",
       "      <td>84.6</td>\n",
       "      <td>300D NSE encoders</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 44</td>\n",
       "      <td>% Train Accuracy</td>\n",
       "      <td>86.2</td>\n",
       "      <td>300D NSE encoders</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 1</td>\n",
       "      <td>Parameters</td>\n",
       "      <td>3.0m</td>\n",
       "      <td>300D NSE encoders</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dataset  extradata global_rank       metric_name metric_value  \\\n",
       "0          IC15        NaN        # 10         F-Measure       75.61%   \n",
       "1  SCUT-CTW1500        NaN         # 5         F-Measure        40.8%   \n",
       "2          SNLI        NaN        # 36   % Test Accuracy         84.6   \n",
       "3          SNLI        NaN        # 44  % Train Accuracy         86.2   \n",
       "4          SNLI        NaN         # 1        Parameters         3.0m   \n",
       "\n",
       "               model remove                        task  \\\n",
       "0            SegLink      -        Scene Text Detection   \n",
       "1            SegLink      -       Curved Text Detection   \n",
       "2  300D NSE encoders      -  Natural Language Inference   \n",
       "3  300D NSE encoders      -  Natural Language Inference   \n",
       "4  300D NSE encoders      -  Natural Language Inference   \n",
       "\n",
       "                                         paper_title  \\\n",
       "0  Detecting Oriented Text in Natural Images by L...   \n",
       "1  Detecting Oriented Text in Natural Images by L...   \n",
       "2                           Neural Semantic Encoders   \n",
       "3                           Neural Semantic Encoders   \n",
       "4                           Neural Semantic Encoders   \n",
       "\n",
       "                                          paper_path  \\\n",
       "0  /paper/detecting-oriented-text-in-natural-imag...   \n",
       "1  /paper/detecting-oriented-text-in-natural-imag...   \n",
       "2                    /paper/neural-semantic-encoders   \n",
       "3                    /paper/neural-semantic-encoders   \n",
       "4                    /paper/neural-semantic-encoders   \n",
       "\n",
       "                                paper_url  \n",
       "0  https://arxiv.org/pdf/1703.06520v3.pdf  \n",
       "1  https://arxiv.org/pdf/1703.06520v3.pdf  \n",
       "2  https://arxiv.org/pdf/1607.04315v3.pdf  \n",
       "3  https://arxiv.org/pdf/1607.04315v3.pdf  \n",
       "4  https://arxiv.org/pdf/1607.04315v3.pdf  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "data = pd.read_csv('../data/pw_code_model_data.csv')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nialdaly/Documents/pwcode_scraping/venv/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>extradata</th>\n",
       "      <th>global_rank</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>metric_value</th>\n",
       "      <th>model</th>\n",
       "      <th>remove</th>\n",
       "      <th>task</th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_path</th>\n",
       "      <th>paper_url</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IC15</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 10</td>\n",
       "      <td>F-Measure</td>\n",
       "      <td>75.61%</td>\n",
       "      <td>SegLink</td>\n",
       "      <td>-</td>\n",
       "      <td>Scene Text Detection</td>\n",
       "      <td>Detecting Oriented Text in Natural Images by L...</td>\n",
       "      <td>/paper/detecting-oriented-text-in-natural-imag...</td>\n",
       "      <td>https://arxiv.org/pdf/1703.06520v3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SCUT-CTW1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 5</td>\n",
       "      <td>F-Measure</td>\n",
       "      <td>40.8%</td>\n",
       "      <td>SegLink</td>\n",
       "      <td>-</td>\n",
       "      <td>Curved Text Detection</td>\n",
       "      <td>Detecting Oriented Text in Natural Images by L...</td>\n",
       "      <td>/paper/detecting-oriented-text-in-natural-imag...</td>\n",
       "      <td>https://arxiv.org/pdf/1703.06520v3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 36</td>\n",
       "      <td>% Test Accuracy</td>\n",
       "      <td>84.6</td>\n",
       "      <td>300D NSE encoders</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 44</td>\n",
       "      <td>% Train Accuracy</td>\n",
       "      <td>86.2</td>\n",
       "      <td>300D NSE encoders</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 1</td>\n",
       "      <td>Parameters</td>\n",
       "      <td>3.0m</td>\n",
       "      <td>300D NSE encoders</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dataset  extradata global_rank       metric_name metric_value  \\\n",
       "0          IC15        NaN        # 10         F-Measure       75.61%   \n",
       "1  SCUT-CTW1500        NaN         # 5         F-Measure        40.8%   \n",
       "2          SNLI        NaN        # 36   % Test Accuracy         84.6   \n",
       "3          SNLI        NaN        # 44  % Train Accuracy         86.2   \n",
       "4          SNLI        NaN         # 1        Parameters         3.0m   \n",
       "\n",
       "               model remove                        task  \\\n",
       "0            SegLink      -        Scene Text Detection   \n",
       "1            SegLink      -       Curved Text Detection   \n",
       "2  300D NSE encoders      -  Natural Language Inference   \n",
       "3  300D NSE encoders      -  Natural Language Inference   \n",
       "4  300D NSE encoders      -  Natural Language Inference   \n",
       "\n",
       "                                         paper_title  \\\n",
       "0  Detecting Oriented Text in Natural Images by L...   \n",
       "1  Detecting Oriented Text in Natural Images by L...   \n",
       "2                           Neural Semantic Encoders   \n",
       "3                           Neural Semantic Encoders   \n",
       "4                           Neural Semantic Encoders   \n",
       "\n",
       "                                          paper_path  \\\n",
       "0  /paper/detecting-oriented-text-in-natural-imag...   \n",
       "1  /paper/detecting-oriented-text-in-natural-imag...   \n",
       "2                    /paper/neural-semantic-encoders   \n",
       "3                    /paper/neural-semantic-encoders   \n",
       "4                    /paper/neural-semantic-encoders   \n",
       "\n",
       "                                paper_url  paper_text  \n",
       "0  https://arxiv.org/pdf/1703.06520v3.pdf         NaN  \n",
       "1  https://arxiv.org/pdf/1703.06520v3.pdf         NaN  \n",
       "2  https://arxiv.org/pdf/1607.04315v3.pdf         NaN  \n",
       "3  https://arxiv.org/pdf/1607.04315v3.pdf         NaN  \n",
       "4  https://arxiv.org/pdf/1607.04315v3.pdf         NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adds a new column to dataframe\n",
    "test_df = data[:10]\n",
    "\n",
    "test_df['paper_text'] = np.nan\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-01 21:21:31,319 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1703.06520v3.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1703-06520v3-pdf.\n",
      "2019-06-01 21:22:09,154 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1703.06520v3.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1703-06520v3-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1703.06520v3.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-01 21:22:52,319 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1607.04315v3.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1607-04315v3-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1703.06520v3.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-01 21:22:54,844 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1607.04315v3.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1607-04315v3-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1607.04315v3.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-01 21:22:58,383 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1607.04315v3.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1607-04315v3-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1607.04315v3.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-01 21:23:03,085 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1607.04315v3.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1607-04315v3-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1607.04315v3.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-01 21:23:11,892 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1607.04315v3.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1607-04315v3-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1607.04315v3.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-01 21:23:15,899 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1607.04315v3.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1607-04315v3-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1607.04315v3.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-01 21:23:19,190 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1607.04315v3.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1607-04315v3-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1607.04315v3.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-01 21:23:27,585 [MainThread  ] [INFO ]  Retrieving https://arxiv.org/pdf/1607.04315v3.pdf to /var/folders/1x/nph4r0452v1dk6t51bknf9080000gn/T/https-arxiv-org-pdf-1607-04315v3-pdf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/1607.04315v3.pdf\n",
      "https://arxiv.org/pdf/1607.04315v3.pdf\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "paper_text_list = []\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    raw = parser.from_file(row['paper_url'])\n",
    "    \n",
    "#     safe_text = str(text_str).replace('\\\\', '\\\\\\\\').replace('\"', '\\\\\"')\n",
    "    \n",
    "    paper_text_list.append(str(raw['content']))\n",
    "    \n",
    "    \n",
    "    print(row['paper_url'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDetecting Oriented Text in Natural Images by Linking Segments\\n\\nBaoguang Shi1 Xiang Bai1∗ Serge Belongie2\\n1School of EIC, Huazhong University of Science and Technology\\n\\n2Department of Computer Science, Cornell Tech\\nshibaoguang@gmail.com xbai@hust.edu.cn sjb344@cornell.edu\\n\\nAbstract\\n\\nMost state-of-the-art text detection methods are specific\\nto horizontal Latin text and are not fast enough for real-time\\napplications. We introduce Segment Linking (SegLink), an\\noriented text detection method. The main idea is to decom-\\npose text into two locally detectable elements, namely seg-\\nments and links. A segment is an oriented box covering a\\npart of a word or text line; A link connects two adjacent\\nsegments, indicating that they belong to the same word or\\ntext line. Both elements are detected densely at multiple\\nscales by an end-to-end trained, fully-convolutional neural\\nnetwork. Final detections are produced by combining seg-\\nments connected by links. Compared with previous meth-\\nods, SegLink improves along the dimensions of accuracy,\\nspeed, and ease of training. It achieves an f-measure of\\n75.0% on the standard ICDAR 2015 Incidental (Challenge\\n4) benchmark, outperforming the previous best by a large\\nmargin. It runs at over 20 FPS on 512×512 images. More-\\nover, without modification, SegLink is able to detect long\\nlines of non-Latin text, such as Chinese.\\n\\n1. Introduction\\n\\nReading text in natural images is a challenging task un-\\nder active research. It is driven by many real-world appli-\\ncations, such as Photo OCR [2], geo-location, and image\\nretrieval [9]. In a text reading system, text detection, i.e.\\nlocalizing text with bounding boxes of words or text lines,\\nis usually the first step of great significance. In a sense, text\\ndetection can be seen as object detection applied to text,\\nwhere words/characters/text lines are taken as the detection\\ntargets. Owing to this, a new trend has emerged recently\\nthat state-of-the-art text detection methods [9, 6, 22, 30] are\\nheavily based on the advanced general object detection or\\nsegmentation techniques, e.g. [4, 5, 15].\\n\\nDespite the great success of the previous work, we ar-\\ngue that the general detection methods are not well suited\\n\\n∗Corresponding author.\\n\\n(a) (b) (c)\\n\\n(d) (e) (f)\\n\\nFigure 1. SegLink Overview. The upper row shows an image\\nwith two words of different scales and orientations. (a) Segments\\n(yellow boxes) are detected on the image. (b) Links (green lines)\\nare detected between pairs of adjacent segments. (c) Segments\\nconnected by links are combined into whole words. (d-f) SegLink\\nis able to detect long lines of Latin and non-Latin text, such as\\nChinese.\\n\\nfor text detection, for two main reasons. First, word/text\\nline bounding boxes have much larger aspect ratios than\\nthose of general objects. An (fast/faster) R-CNN [5, 4, 19]-\\nor SSD [14]-style detector may suffer from the difficulty\\nof producing such boxes, owing to its proposal or anchor\\nbox design. In addition, some non-Latin text does not have\\nblank spaces between words, hence the even larger bound-\\ning box aspect ratios, which make the problem worse. Sec-\\nond, unlike general objects, text usually has a clear defini-\\ntion of orientation [25]. It is important for a text detector to\\nproduce oriented boxes. However, most general object de-\\ntection methods are not designed to produce oriented boxes.\\n\\nTo overcome the above challenges, we tackle the text de-\\ntection problem in a new perspective. We propose to de-\\ncompose long text into two smaller and locally-detectable\\nelements, namely segment and link. As illustrated in Fig. 1,\\na segment is an oriented box that covers a part of a word\\n\\nar\\nX\\n\\niv\\n:1\\n\\n70\\n3.\\n\\n06\\n52\\n\\n0v\\n3 \\n\\n [\\ncs\\n\\n.C\\nV\\n\\n] \\n 1\\n\\n3 \\nA\\n\\npr\\n 2\\n\\n01\\n7\\n\\n\\n\\n64\\n\\n512\\n\\nVGG16 \\nthrough \\npool5\\n\\nconv\\n4_3\\n\\nInput Image (512x512)\\n\\n32\\n\\n1024\\n\\nconv7\\n(fc7)\\n\\n16\\n\\n1024\\n\\nconv\\n8_2\\n\\n8\\n\\n512\\n\\nconv\\n9_2 conv\\n\\n10_24 conv11\\n\\nCombining \\nSegments\\n\\nDetections\\n\\n256\\n\\n2\\n\\n256\\n\\n𝑙 = 1 𝑙 = 6𝑙 = 2 𝑙 = 3 𝑙 = 4 𝑙 = 5\\n\\n3x3 conv \\npredictors\\n\\n1024,k3s1\\n1024,k1s1\\n\\n256,k1s1\\n512,k3s2\\n\\n128,k1s1\\n256,k3s2\\n\\n128,k1s1\\n256,k3s2\\n\\n256,k3s2\\n\\nFigure 2. Network Architecture. The network consists of convolutional feature layers (shown as gray blocks) and convolutional predictors\\n(thin gray arrows). Convolutional filters are specified in the format of “(#filters),k(kernel size)s(stride)”. A multi-line filter specification\\nmeans a hidden layer between. Segments (yellow boxes) and links (not displayed) are detected by convolutional predictors on multiple\\nfeature layers (indexed by l = 1 . . . 6) and combined into whole words by a combining algorithm.\\n\\n(for clarity we use “word” here and later on, but segments\\nalso work seamlessly on text lines that comprise multiple\\nwords); A link connects a pair of adjacent segments, indi-\\ncating that they belong to the same word. Under the above\\ndefinitions, a word is located by a number of segments\\nwith links between them. During detection, segments and\\nlinks are densely detected on an input image by a convo-\\nlutional neural network. Then, the segments are combined\\ninto whole words according to the links.\\n\\nThe key advantage of this approach is that long and ori-\\nented text is now detected locally since both basic elements\\nare locally-detectable: Detecting a segment does not require\\nthe whole word to be observed. And neither does a link\\nsince the connection of two segments can be inferred from\\na local context. Thereafter, we can detect text of any length\\nand orientation with great flexibility and efficiency.\\n\\nConcretely, we propose a convolutional neural network\\n(CNN) model to detect both segments and links simultane-\\nously, in a fully-convolutional manner. The network uses\\nVGG-16 [21] as its backbone. A few extra feature lay-\\ners are added onto it. Convolutional predictors are added\\nto 6 of the feature layers to detect segments and links at\\ndifferent scales. To deal with redundant detections, we in-\\ntroduce two types of links, namely within-layer links and\\ncross-layer links. A within-layer link connects a segment\\nto its neighbors on the same layer. A cross-layer link, on\\nthe other hand, connects a segment to its neighbors on the\\nlower layer. In this way, we connect segments of adjacent\\nlocations as well as scales. Finally, we find connected seg-\\nments with a depth-first search (DFS) algorithm and com-\\nbine them into whole words.\\n\\nOur main contribution is the novel segment-linking de-\\n\\ntection method. Through experiments, we show that the\\nproposed method possesses several distinctive advantages\\nover the other state-of-the-art methods: 1) Robustness:\\nSegLink models the structure of oriented text in a simple\\nand elegant way, with robustness against complex back-\\ngrounds. Our method achieves highly competitive results\\non standard datasets. In particular, it outperforms the previ-\\nous best by a large margin in terms of f-measure (75.0% vs\\n64.8%) on the ICDAR 2015 Incidental (Challenge 4) bench-\\nmark [12]; 2) Efficiency: SegLink is highly efficient due\\nto its single-pass, fully-convolutional design. It processes\\nmore than 20 images of 512x512 size per second; 3) Gener-\\nality: Without modification, SegLink is able to detect long\\nlines of non-Latin text, such as Chinese. We demonstrate\\nthis capability on a multi-lingual dataset.\\n\\n2. Related Work\\n\\nText Detection Over the past few years, much research\\neffort has been devoted to the text detection problem [24,\\n23, 17, 17, 25, 7, 8, 30, 29, 2, 9, 6, 22, 26]. Based on the ba-\\nsic detection targets, the previous methods can be roughly\\ndivided into three categories: character-based, word-based\\nand line-based. Character-based methods [17, 23, 24, 10, 7,\\n8] detect individual characters and group them into words.\\nThese methods find characters by classifying candidate re-\\ngions extracted by region extraction algorithms or by classi-\\nfying sliding windows. Such methods often involve a post-\\nprocessing step of grouping characters into words. Word-\\nbased methods [9, 6] directly detect word bounding boxes.\\nThey often have a similar pipeline to the recent CNN-based\\ngeneral object detection networks. Though achieving ex-\\ncellent detection accuracies, these methods may suffer from\\n\\n\\n\\nperformance drop when applied to some non-Latin text such\\nas Chinese, as we mentioned earlier. Line-based meth-\\nods [29, 30, 26] find text regions using some image seg-\\nmentation algorithms. They also require a sophisticated\\npost-processing step of word partitioning and/or false pos-\\nitive removal. Compared with the previous approaches,\\nour method predicts segments and links jointly in a single\\nforward network pass. The pipeline is much simpler and\\ncleaner. Moreover, the network is end-to-end trainable.\\n\\nOur method is similar in spirit to a recent work [22],\\nwhich detects text lines by finding and grouping a sequence\\nof fine-scale text proposals through a CNN coupled with\\nrecurrent neural layers. In contrast, we detect oriented seg-\\nments only using convolutional layers, yielding better flexi-\\nbility and faster speed. Also, we detect links explicitly using\\nthe same strong CNN features for segments, improving the\\nrobustness.\\n\\nObject Detection Text detection can be seen as a partic-\\nular instance of general object detection, which is a funda-\\nmental problem in computer vision. Most state-of-the-art\\ndetection systems either classify some class-agnostic ob-\\nject proposals with CNN [5, 4, 19] or directly regress ob-\\nject bounding boxes from a set of preset boxes (e.g. anchor\\nboxes) [18, 14].\\n\\nThe architecture of our network inherits that of SSD [14],\\na recent object detection model. SSD proposed the idea of\\ndetecting objects on multiple feature layers with convolu-\\ntional predictors. Our model also detects segments and links\\nin a very similar way. Despite the model similarity, our de-\\ntection strategy is drastically different: SSD directly out-\\nputs object bounding boxes. We, on the other hand, adopt\\na bottom-up approach by detecting the two comprising ele-\\nments of a word or text line and combine them together.\\n\\n3. Segment Linking\\nOur method detects text with a feed-forward CNN\\n\\nmodel. Given an input image I of size wI × hI , the model\\noutputs a fixed number of segments and links, which are\\nthen filtered by their confidence scores and combined into\\nwhole word bounding boxes. A bounding box is a ro-\\ntated rectangle denoted by b = (xb, yb, wb, hb, θb), where\\nxb, yb are the coordinates of the center, wb, hb the width\\nand height, and θb the rotation angle.\\n\\n3.1. CNN Model\\n\\nFig. 2 shows the network architecture. Our network uses\\na pretrained VGG-16 network [21] as its backbone (conv1\\nthrough pool5). Following [14], the fully-connected layers\\nof VGG-16 are converted into convolutional layers (fc6 to\\nconv6; fc7 to conv7). They are followed by a few extra con-\\nvolutional layers (conv8 1 to conv11), which extract even\\n\\ndeeper features with larger receptive fields. Their configu-\\nrations are specified in Fig. 2.\\n\\nSegments and links are detected on 6 of the feature lay-\\ners, which are conv4 3, conv7, conv8 2, conv9 2, conv10 2,\\nand conv11. These feature layers provide high-quality deep\\nfeatures of different granularity (conv4 3 the finest and\\nconv11 the coarsest). A convolutional predictor with 3 × 3\\nkernels is added to each of the 6 layers to detect segments\\nand links. We index the feature layers and the predictors by\\nl = 1, . . . , 6.\\n\\nSegment Detection Segments are also oriented boxes, de-\\nnoted by s = (xs, ys, ws, hs, θs). We detect segments by\\nestimating the confidence scores and geometric offsets to a\\nset of default boxes [14] on the input image. Each default\\nbox is associated with a feature map location, and its score\\nand offsets are predicted from the features at that location.\\nFor simplicity, we only associate one default box with a fea-\\nture map location.\\n\\nConsider the l-th feature layer whose feature map size\\nis wl × hl. A location (x, y) on this map corresponds to a\\ndefault box centered at (xa, ya) on the image, where\\n\\nxa =\\nwI\\nwl\\n\\n(x+ 0.5); ya =\\nhI\\nhl\\n\\n(y + 0.5) (1)\\n\\nThe width and height of the default box are both set to a\\nconstant al.\\n\\nThe convolutional predictor produces 7 channels for\\nsegment detection. Among them, 2 channels are further\\nsoftmax-normalized to get the segment score in (0, 1). The\\nrest 5 are the geometric offsets. Considering a location\\n(x, y) on the map, we denote the vector at this location\\nalong the depth by (∆xs, ∆ys, ∆ws, ∆hs, ∆θs). Then,\\nthe segment at this location is calculated by:\\n\\nxs = al∆xs + xa (2)\\nys = al∆ys + ya (3)\\nws = al exp(∆ws) (4)\\nhs = al exp(∆hs) (5)\\nθs = ∆θs (6)\\n\\nHere, the constant al controls the scale of the output seg-\\nments. It should be chosen with regard to the receptive\\nfield size of the l-th layer. We use an empirical equation\\nfor choosing this size: al = γ wIwl , where γ = 1.5.\\n\\nWithin-Layer Link Detection A link connects a pair of\\nadjacent segments, indicating that they belong to the same\\nword. Here, adjacent segments are those detected at adja-\\ncent feature map locations. Links are not only necessary for\\ncombining segments into whole words but also helpful for\\nseparating two nearby words – between two nearby words,\\nthe links should be predicted as negative.\\n\\n\\n\\n(a) Within-\\nLayer Links\\n\\n(b) Cross-\\nLayer Links\\n\\n2x \\nsize\\n\\nconv8_2\\n\\nconv8_2\\n\\nconv9_2\\n\\n16\\n\\n8\\n\\n16\\n\\nFigure 3. Within-Layer and Cross-Layer Links. (a) A location\\non conv8 2 (yellow block) and its 8-connected neighbors (blue\\nblocks with and without fill). The detected within-layer links\\n(green lines) connect a segment (yellow box) and its two neigh-\\nboring segments (blue boxes) on the same layer. (b) The cross-\\nlayer links connect a segment on conv9 2 (yellow box) and two\\nsegments on conv8 2 (blue boxes).\\n\\nWe explicitly detect links between segments using the\\nsame features for detecting segments. Since we detect only\\none segment at a feature map location, segments can be\\nindexed by their map locations (x, y) and layer indexes l,\\ndenoted by s(x,y,l). As illustrated in Fig. 3.a, we define\\nthe within-layer neighbors of a segment as its 8-connected\\nneighbors on the same feature layer:\\n\\nNws(x,y,l) = {s\\n(x′,y′,l)}x−1≤x′≤x+1,y−1≤y′≤y+1 \\\\ s(x,y,l)\\n\\n(7)\\n\\nAs segments are detected locally, a pair of neighboring seg-\\nments are also adjacent on input image. Links are also de-\\ntected by the convolutional predictors. A predictor outputs\\n16 channels for the links to the 8-connected neighboring\\nsegments. Every 2 channels are softmax-normalized to get\\nthe score of a link.\\n\\nCross-Layer Link Detection In our network, segments\\nare detected at different scales on different feature layers.\\nEach layer handles a range of scales. We make these ranges\\noverlap in order not to miss scales at their edges. But as\\na result, segments of the same word could be detected on\\nmultiple layers at the same time, producing redundancies.\\n\\nTo address this problem, we further propose another\\ntype of links, called cross-layer links. A cross-layer link\\nconnects segments on two feature layers with adjacent in-\\ndexes. For example, cross-layer links are detected between\\nconv4 3 and conv7, because their indexes are l = 1 and\\nl = 2 respectively.\\n\\nAn important property of such a pair is that the first layer\\nalways has twice the size as the second one, because of the\\n\\ndown-sampling layer (max-pooling or stride-2 convolution)\\nbetween them. Note that this property only holds when all\\nfeature layers have even-numbered sizes. In practice, we\\nensured this property by having the width and height of the\\ninput image both dividable by 128. For example, an 1000×\\n800 image is resized to 1024 × 768, which is the nearest\\nvalid size.\\n\\nAs illustrated in Fig. 3.b, we define the cross-layer\\nneighbors of a segment as\\n\\n(8)N cs(x,y,l) = {s\\n(x′,y′,l−1)}2x≤x′≤2x+1,2y≤y′≤2y+1,\\n\\nwhich are the segments on the preceeding layer. Every seg-\\nment has 4 cross-layer neighbors. The correspondence is\\nensured by the double-size relationship between the two\\nlayers.\\n\\nAgain, cross-layer links are detected by the convolu-\\ntional predictor. The predictor outputs 8 channels for cross-\\nlayer links. Every 2 channels are softmax-normalized to\\nproduce the score of a cross-layer link. Cross-layer links\\nare detected on feature layer l = 2 . . . 6, but not on l = 1\\n(conv4 3) since it has no preceeding feature layer.\\n\\nWith cross-layer links, segments of different scales can\\nbe connected and later combined. Compared with the tradi-\\ntional non-maximum suppression, cross-layer linking pro-\\nvides a trainable way of joining redundancies. Besides, it\\nfits seamlessly into our linking strategy and is easy to im-\\nplement under our framework.\\n\\nsegment \\nscores\\n\\nsegment \\noffsets\\n\\nwithin-layer \\nlink scores\\n\\ncross-layer \\nlink scores\\n\\n2 5 16 8\\n𝑤\"\\n\\nℎ\"\\n\\nFigure 4. Output channels of a convolutional predictor. The block\\nshows a wl × hl map of depth 31. The predictor of l = 1 does not\\noutput the channels for corss-layer links.\\n\\nOutputs of a Convolutional Predictor Putting things to-\\ngether, Fig. 4 shows the output channels of a convolutional\\npredictor. A predictor is implemented by a convolutional\\nlayer followed by some softmax layers that normalize the\\nsegment and link scores respectively. Thereafter, all lay-\\ners in our network are convolutional layers. Our network is\\nfully-convolutional.\\n\\n3.2. Combining Segments with Links\\n\\nAfter feed-forwarding, the network produces a number\\nof segments and links (the number depends on the image\\nsize). Before combination, the output segments and links\\nare filtered by their confidence scores. We set different fil-\\ntering thresholds for segment and link, respectively α and β.\\n\\n\\n\\nEmpirically, the performance of our model is not very sensi-\\ntive to these thresholds. A 0.1 deviation on either thresholds\\nfrom their optimal values results in less than 1% f-measure\\ndrop.\\n\\nTaking the filtered segments as nodes and the filtered\\nlinks as edges, we construct a graph over them. Then, a\\ndepth-first search (DFS) is performed over the graph to find\\nits connected components. Each component contains a set\\nof segments that are connected by links. Denoting a con-\\nnected component by B, segments within this component\\nare combined following the procedures in Alg. 1.\\n\\nAlgorithm 1 Combining Segments\\n\\n1: Input: B = {s(i)}|B|i=1 is a set of segments connected\\nby links, where s(i) = (x(i)s , y\\n\\n(i)\\ns , w\\n\\n(i)\\ns , h\\n\\n(i)\\ns , θ\\n\\n(i)\\ns ).\\n\\n2: Find the average angle θb := 1|B|\\n∑\\nB θ\\n\\n(i)\\ns .\\n\\n3: For a straight line (tan θb)x + b, find the b that min-\\nimizes the sum of distances to all segment centers\\n(x\\n\\n(i)\\ns , y\\n\\n(i)\\ns ).\\n\\n4: Find the perpendicular projections of all segment cen-\\nters onto the straight line.\\n\\n5: From the projected points, find the two with the longest\\ndistance. Denote them by (xp, yp) and (xq, yq).\\n\\n6: xb :=\\n1\\n2 (xp + xq)\\n\\n7: yb :=\\n1\\n2 (yp + yq)\\n\\n8: wb :=\\n√\\n\\n(xp − xq)2 + (yp − yq)2 + 12 (wp + wq)\\n9: hb :=\\n\\n1\\n|B|\\n\\n∑\\nB h\\n\\n(i)\\ns\\n\\n10: b := (xb, yb, wb, hb, θb)\\n\\n11: Output: b is the combined bounding box.\\n\\n4. Training\\n4.1. Groundtruths of Segments and Links\\n\\nThe network is trained by the direct supervision of\\ngroundtruth segments and links. The groundtruths include\\nthe labels of all default boxes (i.e. the label of their corre-\\nsponding segments), their offsets to the default boxes, and\\nthe labels of all within- and cross-layer links. We calculate\\nthem from the groundtruth word bounding boxes.\\n\\nFirst, we assume that there is only one groundtruth word\\non the input image. A default box is labeled as positive iff\\n1) the center of the box is inside the word bounding box;\\n2) the ratio between the box size al and the word height h\\nsatisfies:\\n\\nmax(\\nal\\nh\\n,\\nh\\n\\nal\\n) ≤ 1.5 (9)\\n\\nOtherwise, the default box is labeled as negative.\\nNext, we consider the case of multiple words. A default\\n\\nbox is labeled as negative if it does not meet the above-\\nmentioned criteria for any word. Otherwise, it is labeled as\\n\\npositive and matched to the word that has the closest size,\\ni.e. the one with the minimal value at the left-hand side of\\nEq. 9.\\n\\nword \\nbounding box\\n\\ndefault box\\n\\nbox center\\n\\n𝜃\\n\\n(1) Default box, word bounding box, and \\nthe center of the default box (blue dot)\\n\\n(2) Rotate word clockwise by 𝜃 along \\nthe center of the default box\\n\\n(3) Crop word bounding box to remove \\nthe parts to the left and right of the \\ndefault box\\n\\n(4) Rotate the cropped box \\nanticlockwise by 𝜃 along the center of \\nthe default box\\n\\n𝑎\\n\\nℎ\\n\\n𝑤% \\t\\ngroundtruth \\n\\nsegment\\n\\nℎ%\\t\\n𝑥%, 𝑦%\\n\\nFigure 5. The steps of calculating a groundtruth segment given a\\ndefault box and a word bounding box.\\n\\nOffsets are calculated on positive default boxes. First,\\nwe calculate the groundtruth segments following the steps\\nillustrated in Fig. 5. Then, we solve Eq. 2 to Eq. 6 to get the\\ngroundtruth offsets.\\n\\nA link (either within-layer or cross-layer) is labeled as\\npositive iff 1) both of the default boxes connected to it are\\nlabeled as positive; 2) the two default boxes are matched to\\nthe same word.\\n\\n4.2. Optimization\\n\\nObjective Our network model is trained by simultane-\\nously minimizing the losses on segment classification, off-\\nsets regression, and link classification. Overall, the loss\\nfunction is a weighted sum of the three losses:\\n\\nL(ys, cs,yl, cl, ŝ, s) =\\n1\\n\\nNs\\nLconf(ys, cs)+λ1\\n\\n1\\n\\nNs\\nLloc(ŝ, s)\\n\\n+ λ2\\n1\\n\\nNl\\nLconf(yl, cl)\\n\\n(10)\\n\\nHere, ys is the labels of all segments. y\\n(i)\\ns = 1 if the i-th\\n\\ndefault box is labeled as positive, and 0 otherwise. Like-\\nwise, yl is the labels of the links. Lconf is the softmax loss\\nover the predicted segment and link scores, respectively cs\\nand cl. Lloc is the Smooth L1 regression loss [4] over the\\npredicted segment geometries ŝ and the groundtruth s. The\\nlosses on segment classification and regression are normal-\\nized by Ns, which is the number of positive default boxes.\\nThe loss on link classification is normalized by the number\\nof positive links Nl. The weight constants λ1 and λ2 are\\nboth set to 1 in practice.\\n\\n\\n\\nOnline Hard Negative Mining For both segments and\\nlinks, negatives take up most of the training samples. There-\\nfore, hard negative mining is necessary for balancing the\\npositive and negative samples. We follow the online hard\\nnegative mining strategy proposed in [20] to keep the ra-\\ntio between the negatives and positives 3:1 at most. Hard\\nnegative mining is performed separately for segments and\\nlinks.\\n\\nData Augmentation We adopt an online augmentation\\npipeline that is similar to that of SSD [14] and YOLO [18].\\nTraining images are randomly cropped to a patch that has a\\nminimum Jaccard overlap of o with any groundtruth word\\nCrops are resized to the same size before loaded into a\\nbatch. For oriented text, the augmentation is performed on\\nthe axis-aligned bounding boxes of the words. The overlap\\no is randomly chosen from 0 (no constraint), 0.1, 0.3, 0.5,\\n0.7, and 0.9 for every sample. The crop size is randomly\\nchosen from [0.1, 1] of the original image size. Training\\nimages are not horizontally flipped.\\n\\n5. Experiments\\n\\nIn this section, we evaluate the proposed method on three\\npublic datasets, namely ICDAR 2015 Incidental Text (Chal-\\nlenge 4), MSRA-TD500, and ICDAR 2013, using the stan-\\ndard evaluation protocol of each.\\n\\n5.1. Datasets\\n\\nSynthText in the Wild (SynthText) [6] contains 800,000\\nsynthetic training images. They are created by blending nat-\\nural images with text rendered with random fonts, size, ori-\\nentation, and color. Text is rendered and aligned to care-\\nfully chosen image regions in order have a realistic look.\\nThe dataset provides very detailed annotations for charac-\\nters, words, and text lines. We only use the dataset for pre-\\ntraining our network.\\n\\nICDAR 2015 Incidental Text (IC15) [12] is the Chal-\\nlenge 4 of the ICDAR 2015 Robust Reading Competition.\\nThis challenge features incidental scene text images taken\\nby Google Glasses without taking care of positioning, im-\\nage quality, and viewpoint. Consequently, the dataset ex-\\nhibits large variations in text orientation, scale, and res-\\nolution, making it much more difficult than previous IC-\\nDAR challenges. The dataset contains 1000 training images\\nand 500 testing images. Annotations are provided as word\\nquadrilaterals.\\n\\nMSRA-TD500 (TD500) [25] is the first standard dataset\\nthat focuses on oriented text. The dataset is also multi-\\nlingual, including both Chinese and English text. The\\n\\ndataset consists of 300 training images and 200 testing im-\\nages. Different from IC15, TD500 is annotated at the level\\nof text lines.\\n\\nICDAR 2013 (IC13) [13] contains mostly horizontal text,\\nwith some text slightly oriented. The dataset has been\\nwidely adopted for evaluating text detection methods pre-\\nviously. It consists of 229 training images and 233 testing\\nimages.\\n\\n5.2. Implementation Details\\n\\nOur network is pre-trained on SynthText and finetuned\\non real datasets (specified later). It is optimized by the stan-\\ndard SGD algorithm with a momentum of 0.9. For both pre-\\ntraining and finetuning, images are resized to 384×384 after\\nrandom cropping. Since our model is fully-convolutional,\\nwe can train it on a certain size and apply it to other sizes\\nduring testing. Batch size is set to 32. In pretraining, the\\nlearning is set to 10−3 for the first 60k iterations, then de-\\ncayed to 10−4 for the rest 30k iterations. During finetuning,\\nthe learning rate is fixed to 10−4 for 5-10k iterations. The\\nnumber of finetuning iterations depends on the size of the\\ndataset.\\n\\nDue to the precision-recall tradeoff and the difference\\nbetween evaluation protocols across datasets, we choose\\nthe best thresholds α and β to optimize f-measure. Except\\nfor IC15, the thresholds are chosen separately on different\\ndatasets via a grid search with 0.1 step on a hold-out vali-\\ndation set. IC15 does not offer an offline evaluation script,\\nso the only way for us is to submit multiple results to the\\nevaluation server.\\n\\nOur method is implemented using TensorFlow [1] r0.11.\\nAll the experiments are carried out on a workstation with\\nan Intel Xeon 8-core CPU (2.8 GHz), 4 Titan X Graphics\\nCards, and 64GB RAM. Running on 4 GPUs in parallel,\\ntraining a batch takes about 0.5s. The whole training pro-\\ncess takes less than a day.\\n\\n5.3. Detecting Oriented English Text\\n\\nFirst, we evaluate SegLink on IC15. The pretrained\\nmodel is finetuned for 10k iterations on the training dataset\\nof IC15. Testing images are resized to 1280 × 768. We set\\nthe thresholds on segments and links to 0.9 and 0.7, respec-\\ntively. Performance is evaluated by the official central sub-\\nmission server (http://rrc.cvc.uab.es/?ch=4).\\nIn order to meet the requirements on submission format, the\\noutput oriented rectangles are converted into quadrilaterals.\\n\\nTable 1 lists and compares the results of the proposed\\nmethod and other state-of-the-art methods. Some results are\\nobtained from the online leaderboard. SegLink outperforms\\nthe others by a large margin. In terms of f-measure, it out-\\nperforms the second best by 10.2%. Considering that some\\nmethods have close or even higher precision than SegLink,\\n\\nhttp://rrc.cvc.uab.es/?ch=4\\n\\n\\nRecall=1.0    Precision=0.86    F-Score=0.92 Recall=1.0    Precision=1.0    F-Score=1.0\\n\\nRecall=1.0    Precision=1.0    F-Score=1.0 Recall=0.88    Precision=0.88    F-Score=0.88\\n\\nRecall=1.0    Precision=0.88    F-Score=0.93\\n\\nRecall=1.0    Precision=1.0    F-Score=1.0\\n\\nFigure 6. Example Results on IC15. Green regions are correctly detected text regions. Red ones are either false positive or false negative.\\nGray ones are detected but neglected by the evaluation algorithm. Visualizations are generated by the central submission system. Yellow\\nframes contain zoom-in image regions.\\n\\nTable 1. Results on ICDAR 2015 Incidental Text\\n\\nMethod Precision Recall F-measure\\nHUST MCLAB 47.5 34.8 40.2\\nNJU Text 72.7 35.8 48.0\\nStradVision-2 77.5 36.7 49.8\\nMCLAB FCN [30] 70.8 43.0 53.6\\nCTPN [22] 51.6 74.2 60.9\\nMegvii-Image++ 72.4 57.0 63.8\\nYao et al. [26] 72.3 58.7 64.8\\nSegLink 73.1 76.8 75.0\\n\\nthe improvement mainly comes from the recall. As shown\\nin Fig. 6, our method is able to distinguish text from very\\ncluttered backgrounds. In addition, owing to its explicit link\\nprediction, SegLink correctly separates words that are very\\nclose to each other.\\n\\n5.4. Detecting Multi-Lingual Text in Long Lines\\n\\nWe further demonstrate the ability of SegLink to detect\\nlong text in non-Latin scripts. TD500 is taken as the dataset\\nfor this experiment, as it consists of oriented and multi-\\nlingual text. The training set of TD500 only has 300 im-\\nages, which are not enough for finetuning our model. We\\nmix the training set of TD500 with the training set of IC15,\\nin the way that every batch has half of its images coming\\nfrom each dataset. The pretrained model is finetuned for 8k\\niterations. The testing images are resized to 768×768. The\\nthresholds α and β are set to 0.9 and 0.5 respectively. Per-\\nformance scores are calculated by the official development\\ntoolkit.\\n\\nAccording to Table 2, SegLink achieves the highest\\nscores in terms of precision and f-measure. Benefiting from\\nits fully-convolutional design, SegLink runs at 8.9 FPS, a\\n\\nTable 2. Results on MSRA-TD500\\n\\nMethod Precision Recall F-measure FPS\\nKang et al. [11] 71 62 66 -\\nYao et al. [25] 63 63 60 0.14\\nYin et al. [27] 81 63 74 0.71\\nYin et al. [28] 71 61 65 1.25\\nZhang et al. [30] 83 67 74 0.48\\nYao et al. [26] 77 75 76 ∼1.61\\nSegLink 86 70 77 8.9\\n\\nmuch faster speed than the others. SegLink also enjoys sim-\\nplicity. The inference process of SegLink is a single forward\\npass in the detection network, while the previous methods\\n[25, 28, 30] involve sophisticated rule-based grouping or fil-\\ntering steps.\\n\\nTD500 contains many long lines of text in mixed lan-\\nguages (English and Chinese). Fig. 7 shows how SegLink\\nhandles such text. As can be seen, segments and links are\\ndensely detected along text lines. They result in long bound-\\ning boxes that are hard to obtain from a conventional ob-\\nject detector. Despite the large difference in appearance be-\\ntween English and Chinese text, SegLink is able to handle\\nthem simultaneously without any modifications in its struc-\\nture.\\n\\n5.5. Detecting Horizontal Text\\n\\nLastly, we evaluate the performance of SegLink on\\nhorizontal-text datasets. The pretrained model is finetuned\\nfor 5k iterations on the combined training sets of IC13 and\\nIC15. Since the most text in IC13 has relatively larger sizes,\\nthe testing images are resized to 512× 512. The thresholds\\nα and β are set to 0.6 and 0.3, respectively. To match the\\nsubmission format, we convert the detected oriented boxes\\n\\n\\n\\nSe\\ngm\\n\\nen\\nts\\n\\n an\\nd\\n\\nLin\\nks\\n\\nCo\\nm\\n\\nbi\\nne\\n\\nd\\n\\nFigure 7. Example Results on TD500. The first row shows the detected segments and links. The within-layer and cross-layer links\\nare visualized as red and green lines, respectively. Segments are shown as rectangles in different colors, denoting different connected\\ncomponents. The second row shows the combined boxes.\\n\\ninto their axis-aligned bounding boxes.\\nTable 3 compares SegLink with other state-of-the-art\\n\\nmethods. The scores are calculated by the central sub-\\nmission system using the “Deteval” evaluation protocol.\\nSegLink achieves very competitive results in terms of f-\\nmeasure. Only one approach [22] outperforms SegLink in\\nterms of f-measure. However, [22] is mainly designed for\\ndetecting horizontal text and is not well-suited for oriented\\ntext. In terms of speed, SegLink runs at over 20 FPS on\\n512× 512 images, much faster than the other methods.\\n\\nTable 3. Results on IC13. P, R, F stand for precision, recall and\\nf-measure respectively. *These methods are only evaluated under\\nthe “ICDAR 2013” evaluation protocol, the rest under “Deteval”.\\nThe two protocols usually yield very close scores.\\n\\nMethod P R F FPS\\nNeumann et al. [16]∗ 81.8 72.4 77.1 3\\nNeumann et al. [17]∗ 82.1 71.3 76.3 3\\nBusta et al. [3]∗ 84.0 69.3 76.8 6\\nZhang et al. [29] 88 74 80 <0.1\\nZhang et al. [30] 88 78 83 <1\\nJaderberg et al. [9] 88.5 67.8 76.8 <1\\nGupta et al. [6] 92.0 75.5 83.0 15\\nTian et al. [22] 93.0 83.0 87.7 7.1\\nSegLink 87.7 83.0 85.3 20.6\\n\\n5.6. Limitations\\n\\nA major limitation of SegLink is that two thresholds, α\\nand β, need to be set manually. In practice, the optimal\\nvalues of the thresholds are found by a grid search. Sim-\\nplifying the parameters would be part of our future work.\\nAnother weakness is that SegLink fails to detect text that\\nhas very large character spacing. Fig. 8.a,b show two such\\ncases. The detected links connect adjacent segments but fail\\nto link distant segments.\\n\\n(a) (b) (c)\\n\\nFigure 8. Failure cases on TD500. Red boxes are false posi-\\ntives. (a)(b) SegLink fails to link the characters with large char-\\nacter spacing. (c) SegLink fails to detect curved text.\\n\\nFig. 8.c shows that SegLink fails to detect text of curved\\nshape. However, we believe that this is not a limitation of\\nthe segment linking strategy, but the segment combination\\nalgorithm, which can only produce rectangles currently.\\n\\n6. Conclusion\\n\\nWe have presented SegLink, a novel text detection strat-\\negy implemented by a simple and highly-efficient CNN\\nmodel. The superior performance on horizontal, ori-\\nented, and multi-lingual text datasets well demonstrate that\\nSegLink is accurate, fast, and flexible. In the future, we\\nwill further explore its potentials on detecting deformed text\\nsuch as curved text. Also, we are interested in extending\\nSegLink into a end-to-end recognition system.\\n\\nAcknowledgment\\n\\nThis work was supported in part by National Natural\\nScience Foundation of China (61222308 and 61573160),\\na Google Focused Research Award, AWS Cloud Credits\\nfor Research, a Microsoft Research Award and a Facebook\\nequipment donation. The authors also thank China Scholar-\\nship Council (CSC) for supporting this work.\\n\\n\\n\\nReferences\\n[1] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen,\\n\\nC. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghe-\\nmawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia,\\nR. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mané,\\nR. Monga, S. Moore, D. Murray, C. Olah, M. Schuster,\\nJ. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker,\\nV. Vanhoucke, V. Vasudevan, F. Viégas, O. Vinyals, P. War-\\nden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. Tensor-\\nFlow: Large-scale machine learning on heterogeneous sys-\\ntems, 2015. Software available from tensorflow.org. 6\\n\\n[2] A. Bissacco, M. Cummins, Y. Netzer, and H. Neven. Pho-\\ntoocr: Reading text in uncontrolled conditions. In ICCV,\\n2013. 1, 2\\n\\n[3] M. Busta, L. Neumann, and J. Matas. Fastext: Efficient un-\\nconstrained scene text detector. In ICCV, 2015. 8\\n\\n[4] R. B. Girshick. Fast R-CNN. In ICCV, 2015. 1, 3, 5\\n[5] R. B. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich\\n\\nfeature hierarchies for accurate object detection and semantic\\nsegmentation. In CVPR, 2014. 1, 3\\n\\n[6] A. Gupta, A. Vedaldi, and A. Zisserman. Synthetic data for\\ntext localisation in natural images. In CVPR, 2016. 1, 2, 6, 8\\n\\n[7] W. Huang, Z. Lin, J. Yang, and J. Wang. Text localization\\nin natural images using stroke feature transform and text co-\\nvariance descriptors. In ICCV, 2013. 2\\n\\n[8] W. Huang, Y. Qiao, and X. Tang. Robust scene text detection\\nwith convolution neural network induced MSER trees. In\\nECCV, 2014. 2\\n\\n[9] M. Jaderberg, K. Simonyan, A. Vedaldi, and A. Zisserman.\\nReading text in the wild with convolutional neural networks.\\nIJCV, 116(1):1–20, 2016. 1, 2, 8\\n\\n[10] M. Jaderberg, A. Vedaldi, and A. Zisserman. Deep features\\nfor text spotting. In ECCV, 2014. 2\\n\\n[11] L. Kang, Y. Li, and D. S. Doermann. Orientation robust text\\nline detection in natural images. In CVPR, 2014. 7\\n\\n[12] D. Karatzas, L. Gomez-Bigorda, A. Nicolaou, S. K. Ghosh,\\nA. D. Bagdanov, M. Iwamura, J. Matas, L. Neumann, V. R.\\nChandrasekhar, S. Lu, F. Shafait, S. Uchida, and E. Val-\\nveny. ICDAR 2015 competition on robust reading. In ICDAR\\n2015, 2015. 2, 6\\n\\n[13] D. Karatzas, F. Shafait, S. Uchida, M. Iwamura, L. G. i Big-\\norda, S. R. Mestre, J. Mas, D. F. Mota, J. Almazán, and\\nL. de las Heras. ICDAR 2013 robust reading competition.\\nIn ICDAR 2013, 2013. 6\\n\\n[14] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. E. Reed,\\nC. Fu, and A. C. Berg. SSD: single shot multibox detector.\\nIn ECCV, pages 21–37, 2016. 1, 3, 6\\n\\n[15] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional\\nnetworks for semantic segmentation. In CVPR, 2015. 1\\n\\n[16] L. Neumann and J. Matas. Efficient scene text localization\\nand recognition with local character refinement. In ICDAR,\\n2015. 8\\n\\n[17] L. Neumann and J. Matas. Real-time lexicon-free scene text\\nlocalization and recognition. PAMI, 38(9):1872–1885, 2016.\\n2, 8\\n\\n[18] J. Redmon, S. K. Divvala, R. B. Girshick, and A. Farhadi.\\nYou only look once: Unified, real-time object detection.\\nCoRR, abs/1506.02640, 2015. 3, 6\\n\\n[19] S. Ren, K. He, R. B. Girshick, and J. Sun. Faster R-CNN:\\ntowards real-time object detection with region proposal net-\\nworks. In NIPS, 2015. 1, 3\\n\\n[20] A. Shrivastava, A. Gupta, and R. B. Girshick. Training\\nregion-based object detectors with online hard example min-\\ning. In CVPR, 2016. 6\\n\\n[21] K. Simonyan and A. Zisserman. Very deep convolu-\\ntional networks for large-scale image recognition. CoRR,\\nabs/1409.1556, 2014. 2, 3\\n\\n[22] Z. Tian, W. Huang, T. He, P. He, and Y. Qiao. Detecting text\\nin natural image with connectionist text proposal network. In\\nECCV, 2016. 1, 2, 3, 7, 8\\n\\n[23] K. Wang and S. J. Belongie. Word spotting in the wild. In\\nECCV, 2010. 2\\n\\n[24] T. Wang, D. J. Wu, A. Coates, and A. Y. Ng. End-to-end text\\nrecognition with convolutional neural networks. In ICPR,\\n2012. 2\\n\\n[25] C. Yao, X. Bai, W. Liu, Y. Ma, and Z. Tu. Detecting texts of\\narbitrary orientations in natural images. In CVPR, 2012. 1,\\n2, 6, 7\\n\\n[26] C. Yao, X. Bai, N. Sang, X. Zhou, S. Zhou, and Z. Cao.\\nScene text detection via holistic, multi-channel prediction.\\nCoRR, abs/1606.09002, 2016. 2, 3, 7\\n\\n[27] X. Yin, W. Pei, J. Zhang, and H. Hao. Multi-orientation\\nscene text detection with adaptive clustering. PAMI,\\n37(9):1930–1937, 2015. 7\\n\\n[28] X. Yin, X. Yin, K. Huang, and H. Hao. Robust text detection\\nin natural scene images. PAMI, 36(5):970–983, 2014. 7\\n\\n[29] Z. Zhang, W. Shen, C. Yao, and X. Bai. Symmetry-based\\ntext line detection in natural scenes. In CVPR, 2015. 2, 3, 8\\n\\n[30] Z. Zhang, C. Zhang, W. Shen, C. Yao, W. Liu, and X. Bai.\\nMulti-oriented text detection with fully convolutional net-\\nworks. In CVPR, 2016. 1, 2, 3, 7, 8\\n\\n\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paper_text_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nialdaly/Documents/pwcode_scraping/venv/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>extradata</th>\n",
       "      <th>global_rank</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>metric_value</th>\n",
       "      <th>model</th>\n",
       "      <th>remove</th>\n",
       "      <th>task</th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_path</th>\n",
       "      <th>paper_url</th>\n",
       "      <th>paper_text</th>\n",
       "      <th>paper_text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IC15</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 10</td>\n",
       "      <td>F-Measure</td>\n",
       "      <td>75.61%</td>\n",
       "      <td>SegLink</td>\n",
       "      <td>-</td>\n",
       "      <td>Scene Text Detection</td>\n",
       "      <td>Detecting Oriented Text in Natural Images by L...</td>\n",
       "      <td>/paper/detecting-oriented-text-in-natural-imag...</td>\n",
       "      <td>https://arxiv.org/pdf/1703.06520v3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SCUT-CTW1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 5</td>\n",
       "      <td>F-Measure</td>\n",
       "      <td>40.8%</td>\n",
       "      <td>SegLink</td>\n",
       "      <td>-</td>\n",
       "      <td>Curved Text Detection</td>\n",
       "      <td>Detecting Oriented Text in Natural Images by L...</td>\n",
       "      <td>/paper/detecting-oriented-text-in-natural-imag...</td>\n",
       "      <td>https://arxiv.org/pdf/1703.06520v3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 36</td>\n",
       "      <td>% Test Accuracy</td>\n",
       "      <td>84.6</td>\n",
       "      <td>300D NSE encoders</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 44</td>\n",
       "      <td>% Train Accuracy</td>\n",
       "      <td>86.2</td>\n",
       "      <td>300D NSE encoders</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 1</td>\n",
       "      <td>Parameters</td>\n",
       "      <td>3.0m</td>\n",
       "      <td>300D NSE encoders</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 33</td>\n",
       "      <td>% Test Accuracy</td>\n",
       "      <td>85.4</td>\n",
       "      <td>300D MMA-NSE encoders with attention</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 41</td>\n",
       "      <td>% Train Accuracy</td>\n",
       "      <td>86.9</td>\n",
       "      <td>300D MMA-NSE encoders with attention</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SNLI</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 1</td>\n",
       "      <td>Parameters</td>\n",
       "      <td>3.2m</td>\n",
       "      <td>300D MMA-NSE encoders with attention</td>\n",
       "      <td>-</td>\n",
       "      <td>Natural Language Inference</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SST-2 Binary classification</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 10</td>\n",
       "      <td>Accuracy</td>\n",
       "      <td>89.7</td>\n",
       "      <td>Neural Semantic Encoder</td>\n",
       "      <td>-</td>\n",
       "      <td>Sentiment Analysis</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WikiQA</td>\n",
       "      <td>NaN</td>\n",
       "      <td># 8</td>\n",
       "      <td>MAP</td>\n",
       "      <td>0.6811</td>\n",
       "      <td>MMA-NSE attention</td>\n",
       "      <td>-</td>\n",
       "      <td>Question Answering</td>\n",
       "      <td>Neural Semantic Encoders</td>\n",
       "      <td>/paper/neural-semantic-encoders</td>\n",
       "      <td>https://arxiv.org/pdf/1607.04315v3.pdf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       dataset  extradata global_rank       metric_name  \\\n",
       "0                         IC15        NaN        # 10         F-Measure   \n",
       "1                 SCUT-CTW1500        NaN         # 5         F-Measure   \n",
       "2                         SNLI        NaN        # 36   % Test Accuracy   \n",
       "3                         SNLI        NaN        # 44  % Train Accuracy   \n",
       "4                         SNLI        NaN         # 1        Parameters   \n",
       "5                         SNLI        NaN        # 33   % Test Accuracy   \n",
       "6                         SNLI        NaN        # 41  % Train Accuracy   \n",
       "7                         SNLI        NaN         # 1        Parameters   \n",
       "8  SST-2 Binary classification        NaN        # 10          Accuracy   \n",
       "9                       WikiQA        NaN         # 8               MAP   \n",
       "\n",
       "  metric_value                                 model remove  \\\n",
       "0       75.61%                               SegLink      -   \n",
       "1        40.8%                               SegLink      -   \n",
       "2         84.6                     300D NSE encoders      -   \n",
       "3         86.2                     300D NSE encoders      -   \n",
       "4         3.0m                     300D NSE encoders      -   \n",
       "5         85.4  300D MMA-NSE encoders with attention      -   \n",
       "6         86.9  300D MMA-NSE encoders with attention      -   \n",
       "7         3.2m  300D MMA-NSE encoders with attention      -   \n",
       "8         89.7               Neural Semantic Encoder      -   \n",
       "9       0.6811                     MMA-NSE attention      -   \n",
       "\n",
       "                         task  \\\n",
       "0        Scene Text Detection   \n",
       "1       Curved Text Detection   \n",
       "2  Natural Language Inference   \n",
       "3  Natural Language Inference   \n",
       "4  Natural Language Inference   \n",
       "5  Natural Language Inference   \n",
       "6  Natural Language Inference   \n",
       "7  Natural Language Inference   \n",
       "8          Sentiment Analysis   \n",
       "9          Question Answering   \n",
       "\n",
       "                                         paper_title  \\\n",
       "0  Detecting Oriented Text in Natural Images by L...   \n",
       "1  Detecting Oriented Text in Natural Images by L...   \n",
       "2                           Neural Semantic Encoders   \n",
       "3                           Neural Semantic Encoders   \n",
       "4                           Neural Semantic Encoders   \n",
       "5                           Neural Semantic Encoders   \n",
       "6                           Neural Semantic Encoders   \n",
       "7                           Neural Semantic Encoders   \n",
       "8                           Neural Semantic Encoders   \n",
       "9                           Neural Semantic Encoders   \n",
       "\n",
       "                                          paper_path  \\\n",
       "0  /paper/detecting-oriented-text-in-natural-imag...   \n",
       "1  /paper/detecting-oriented-text-in-natural-imag...   \n",
       "2                    /paper/neural-semantic-encoders   \n",
       "3                    /paper/neural-semantic-encoders   \n",
       "4                    /paper/neural-semantic-encoders   \n",
       "5                    /paper/neural-semantic-encoders   \n",
       "6                    /paper/neural-semantic-encoders   \n",
       "7                    /paper/neural-semantic-encoders   \n",
       "8                    /paper/neural-semantic-encoders   \n",
       "9                    /paper/neural-semantic-encoders   \n",
       "\n",
       "                                paper_url  paper_text  \\\n",
       "0  https://arxiv.org/pdf/1703.06520v3.pdf         NaN   \n",
       "1  https://arxiv.org/pdf/1703.06520v3.pdf         NaN   \n",
       "2  https://arxiv.org/pdf/1607.04315v3.pdf         NaN   \n",
       "3  https://arxiv.org/pdf/1607.04315v3.pdf         NaN   \n",
       "4  https://arxiv.org/pdf/1607.04315v3.pdf         NaN   \n",
       "5  https://arxiv.org/pdf/1607.04315v3.pdf         NaN   \n",
       "6  https://arxiv.org/pdf/1607.04315v3.pdf         NaN   \n",
       "7  https://arxiv.org/pdf/1607.04315v3.pdf         NaN   \n",
       "8  https://arxiv.org/pdf/1607.04315v3.pdf         NaN   \n",
       "9  https://arxiv.org/pdf/1607.04315v3.pdf         NaN   \n",
       "\n",
       "                                         paper_text2  \n",
       "0  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
       "1  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
       "2  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
       "3  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
       "4  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
       "5  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
       "6  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
       "7  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
       "8  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  \n",
       "9  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# paper_text_json = json.dumps(paper_text_list)\n",
    "\n",
    "test_df['paper_text2'] = paper_text_list\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/temp.json', 'w') as f:\n",
    "#     f.write(test_df.to_json(orient='records', lines=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_json('../data/temp.json', orient='index', lines=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pwcode_scraping",
   "language": "python",
   "name": "pwcode_scraping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
